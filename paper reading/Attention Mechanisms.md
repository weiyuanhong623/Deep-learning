# Attention Mechanisms in Computer Vision:   A Survey

###### Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-T ao Jiang, T ai-Jiang Mu, Song-Hai Zhang, Ralph R. Martin, Ming-Ming Cheng, Senior Member, IEEE, Shi-Min Hu, Senior Member, IEEE,

<br>

### 一、摘要：

#### 1.1 启示：

人类可以自然有效地在复杂场景中找到突出区域。

#### 1.2 注意力机制：

这种关注机制可以被视为基于输入图像的**特征的动态权重调整过程**。

#### 1.3 主要内容：

全面回顾了计算机视觉中的各种注意机制，并根据方法对它们进行了分类，如**通道注意**、**空间注意**、**时间注意**和**分支注意**

<br>

### 二、介绍：

#### 2.1 注意力机制：

将注意力**转移到**图像中**最重要的区域**并**忽略不相关部分**的方法称为注意力机制。在视觉系统中，注意力机制可以被视为**动态选择过程**，其通过根据输入的**重要性自适应加权特征**来实现。

#### 2.2 注意力机制取得的进展：

注意力机制已经**在许多视觉任务中提供了好处**，例如图像分类、对象检测、语义分割、人脸识别、人物重新识别、动作识别、少数显示学习、医学图像处理、图像生成、姿势估计、超分辨率、3D视觉和多模态任务。

#### **2.3 **对深度学习时代计算机视觉中基于注意力的模型的历史的简要总结。 

![image-20221120104902550](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120104902550.png)

Brief summary of **key developments** in **attention in computer vision**, which have loosely occurred in four phases. 

- Phase 1 **adopted RNNs to construct attention**, a representative method being **RAM** 
- Phase 2 **explicitly predicted important regions**, a representative method being **STN** 
- Phase 3 **implicitly completed the attention process**, a representative method being **SENet** . 
- Phase 4 used **self-attention methods** 

##### 2.3.1 RAM：

将**深度神经网络与注意力机制**相结合的开创性工作。它**反复预测重要区域**，并通过策略梯度以**端到端**的方式更新整个网络。这个阶段递归神经网络（RNN）是注意力机制的必要工具

##### 2.3.2 STN：

引入了一**个子网络**来**预测**用于选择输入中重要区域的**仿射变换**。**明确预测歧视性输入**特征是第二阶段的主要特征

##### 2.3.3 SENeT：

提出新的信道注意网络，能隐式地和自适应地预测潜在的关键特征。

##### 2.3.4 self-attention ：

先在NLP方向取得巨大进展，后引入CV。提出了一种新颖的非本地网络，在视频理解和对象检测方面取得了巨大成功，它们提高了速度、结果质量和泛化能力。基于注意力的模型**有可能取代卷积神经网络**，成为**计算机视觉中**更强大和通用的架构。

#### 2.4.小结：

##### 2.4.1 本文的目的是总结和分类当前计算机视觉中的注意力方法。

##### 2.4.2 关键、次要符号解释

![image-20221120143442280](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120143442280.png)

##### 2.4.3 主要注意力方法：

![image-20221120143757657](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120143757657.png)

Attention **mechanisms** can be categorised **according to data domain**. These include four fundamental categories of 

- **channel attention,**

- **spatial attention,** 

- ##### temporal attention and branch attention, 

- two **hybrid** categories, combining **channel & spatial attention** and **spatial & temporal attention**.

-  ∅ means such combinations do not (yet) exist.

##### 2.4.4 对各类方法的进一步解释：

![image-20221120144317042](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120144317042.png)

- **Channel**, **spatial** and **temporal** attention can be regarded as **operating on different domains.** 

- **C** represents the **channel domain,** **H** and **W** represent **spatial domains**, and **T** means the **temporal domain**.
- **Branch attention** is **complementary** to these.

##### 2.4.5 对注意力方法进行分类（六类）：

- 四个基本类别：通道注意力（注意什么）、空间注意力（注意哪里）、时间注意力（何时注意）和分支通道（注意什么），
- 两个混合组合类别：通道和空间注意力和空间和时间注意力。

##### 2.4.6 对各类别注意力方法和相关工作的总结：

![image-20221120150021125](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120150021125.png)

##### 2.4.7 本文的主要贡献：

- 对视觉注意力方法进行了**系统综述**，涵盖了注意力机制的统一描述、视觉注意力机制的发展以及当前的研究，
- 根据注意力方法的数据域对其进行了**分类**，使我们能够独立于其特定应用将视觉注意力方法联系起来
- 对未来视觉注意力**研究的建议**。

##### 2.4.8 本文其他主要结构

第2节考虑了相关调查，第3节是我们调查的主体。第4节给出了未来研究的建议，最后，第5节给出了结论。 

### 三、OTHER SURVEYS

#### 3.1将本文和其他调查进行比较

##### 3.1.1 调查侧重点

这些调查回顾了注意方法和视觉变换器。Chaudhari等人[140]对深度神经网络中的注意力模型进行了一项调查，重点研究了它们在**自然语言处理中的应用**，而我们的工作**侧重于计算机视觉**。

##### 3.1.2调查范围

- 三项更具体的调查[141]、[142]、[143]总结了**视觉变压器的发展**，而我们的论文**更全面地回顾了视觉中的注意机制**，而不仅仅是自我注意机制。

- Wang等人[144]对计算机视觉中的**注意力模型**进行了调查，但它**只考虑了基于RNN的注意力模型**，这只是我们调查的一部分。

##### 3.1.3本文调查的创新点：

与之前的调查不同，我们**提供了一种分类**，根据数据域而非应用领域对各种注意力方法进行分类。这样做可以让我们集中精力 在方法本身，而不是将它们作为其他任务的补充。

### 四、计算机视觉中的注意力方法

#### 4.1总述：

- 总结了基于人类视觉系统识别过程的**注意力机制的一般形式**

- 回顾了**各种类型的注意力模型**
- 更深入地介绍了**该类别的注意力策略**，考虑到其在**动机**、**形式**和**功能**方面的发展。 

#### 4.2注意力机制的一般形式

##### 4.2.1将注意力机制转化为公式：

**一般形式**：g（x）可以表示产生与关注辨别区域的过程相对应的注意。f（g（x），x）表示基于**与处理关键区域**和**获取信息一致的关注度g（x）**来处理输入x。

![image-20221120160610252](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120160610252.png)

##### 4.2.2对于模型：self-attention、squeeze-and-excitation(SE)

- self-attention

![image-20221120161219693](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120161219693.png)

g（x）：全连接层后接softmax变换

- squeeze-and-excitation(SE)

![image-20221120161236246](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120161236246.png)

g（x）：全局平均池化->多层感知机->sigmoid

#### 4.3对于各种注意力机制的介绍

##### 4.3.1Channel Attention

##### （1）⭐总述：

在深度神经网络中，**不同特征图**中的**不同通道**通常代表**不同的对象**[50]。通道注意力**自适应**地**重新校准每个通道的权重**，并且可以被视为**一个对象选择过程，从而确定要关注什么（需要注意的通道权重更高）**。

![image-20221120213322806](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120213322806.png)

##### （2）SENet：

- SENet的**核心**是一个**挤压和激励（SE）块**，用于**收集全局信息、捕获信道关系并提高表示能力**。
- **SE块分为两个部分**，**挤压模块**和**激励模块**。通过**全局平均池（即一阶统计）在挤压模块中收集全局空间信息**。**激励模块通过使用完全连接的层和非线性层（ReLU和sigmoid）来捕获通道关系并输出注意力向量。**然后，⭐**通过乘以关注向量中的对应元素来缩放输入特征的每个通道。**
- 公式化：

![image-20221120163652644](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120163652644.png)

- 优点：SE块在抑制噪声的同时起到强调重要信道的作用，低计算资源要求（可在每个剩余单元后面添加）。
- 缺点：**挤压模块**中，**全局平均池太简单**，**无法捕获复杂的全局信息**。在**激励模块**中，**完全连接的层增加了模型的复杂性**。

##### （3）GSoP-Net

- 使用**全局二阶池（GSoP）**块来建模高阶统计，同时收集全局信息，从而改进挤压模块。与SE模块一样，GSoP模块也具有挤压模块和激励模块。在**挤压模块**中，GSoP块首先使用**1x1卷积将信道数量从c减少到c0**（c0＜c），然后**计算不同信道的c0×c0协方差矩阵以获得它们的相关性。**接下来，**对协方差矩阵执行逐行归一化。**归一化协方差矩阵中的**每个（i，j）显式地将信道i与信道j相关联。**在**激励模块**中，GSoP块**执行逐行卷积以保持结构信息并输出向量。**然后应用一个**全连通层和一个S形函数**得到一个**c维注意力向量**。最后将**输入特征与关注向量相乘**

![image-20221120185030795](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120185030795.png)

Conv（·）减少了信道数量，Cov（·）计算协方差矩阵，RC（·）表示逐行卷积。

- 优点：提高了收集全局信息的能力
- 增大了计算量

##### （4）SRM

- 它的主要贡献是**样式池**，它利用输入特征的**均值和标准差**来**提高其捕获全局信息的能力**。它还采用了一个轻量级的全连接（CFC）层来代替原来的全连接层，以**减少计算需求**（也可在每个剩余单元后面添加）。
- 用**结合了全局平均池和全局标准差池的样式池**（SP（·））来**收集全局信息**。然后，使用**信道方向全连接**（CFC（·））层（即**每个信道全连接**）、**批量归一化BN**和**S形函数σ**来提供关注向量。最后，如在SE块中，输入特征乘以关注向量。

![image-20221120190914671](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120190914671.png)

##### （5）GCT

- 由于激励模块中全连接层的计算需求和参数数量，在每个卷积层之后使用SE块是不切实际的。此外，使用**完全连接的层**来建模信道关系是一个**隐式**过程。
- 门控信道变换（GCT），可以有效地收集信息，同时**显式地建模信道关系**。与以前的方法不同，GCT首先通过**计算每个信道的l2范数**来**收集全局信息**。接下来，应用可**学习向量α来缩放特征**。然后通过**渠道规范化**采用**竞争机制**来**实现渠道之间的互动**。与其他常见的归一化方法一样，应用**可学习的尺度参数γ和偏差β来重新缩放归一化**。
- 采用**tanh**激活来**控制注意力向量**
- 将输入和注意力向量相乘，同时**添加标识连接** 

![image-20221120191710307](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120191710307.png)

其中α、β和γ是可训练参数。范数（·）表示每个信道的L2范数。CN是信道归一化。GCT块比SE块具有更少的参数，并且由于它是

- 优点：轻量级，可以在**CNN的每个卷积层之后**添加。

##### （6）ECANet

- 为了**避免高模型复杂性**，SENet**减少了通道的数量**。然而，该策略**无法直接建模权重向量和输入之间的对应关系**，从而降低了结果的质量。
- 信道关注（ECA）块，该块**使用1D卷积**来**确定信道之间的交互**，**而不是降维**。ECA块使用**聚集全局空间信息**的挤压模块和用于**建模跨信道交互**的有效激励模块。与间接对应不同，ECA块**只考虑每个信道与其k近邻之间的直接交互**，以**控制模型复杂性**。

![image-20221120193121134](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120193121134.png)

Conv1D（·）表示在信道域上具有形状为k的核的1D卷积，以模拟局部跨信道交互。参数k决定交互的覆盖范围，在ECA中，内核大小k由信道维度C自适应地确定

![image-20221120193307158](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120193307158.png)

γ和b是超参数|x|odd表示x的最接近的奇函数

- 优点：比SENet更加高效，可以容易地合并到各种CNN中。

##### （7） FcaNet

- 仅在挤压模块中使用全局平均池限制了表示能力
- 证明了全局平均池是**离散余弦变换**（DCT）的一种特殊情况，并利用这一观察结果提出了一种**新的多谱信道关注**。给定输入特征图X∈  RC×H×W，多光谱通道注意力首先**将X分成多个部分xi**∈ RC0×H×W。然后**将2D DCT应用于每个零件xi**。注意，2D  DCT可以使用**预处理结果来减少计算**。在处理每个部分之后，所有结果都被连接到一个向量中。最后全连接层，使用Relu激活和sigmoid来获取注意力向量（像SE块中那样）。

![image-20221120201141284](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120201141284.png)

Group（·）表示将输入分成多个组，DCT（·）是2D离散余弦变换。

- 该模型在分类任务中取得了优异的性能。 

##### （8）EncNet

- 提出了**包含语义编码丢失**（SE丢失）的**上下文编码模块（CEM）**，以建模**场景上下文**和**对象类别概率之间的关系**，从而**利用全局场景上下文信息进行语义分割。**
- 给定输入特征图X∈  RC×H×W，CEM首先在训练阶段**学习K个聚类中心**D={d1，…，dK}和一组**平滑因子**S={s1，…，sK}。接下来，它使用**软分配权重**对输入中的**局部描述符**和**对应的聚类中心**之间的差异求和，**以获得置换不变描述符**。然后，为了计算效率，它将聚合应用于K个簇中心的描述符，而不是级联。

![image-20221120203048204](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120203048204.png)

dk∈ RC和sk∈  R是可学习的参数。φ表示ReLU激活的批量归一化。除了按通道缩放向量外，还将紧凑的上下文描述符e应用于计算SE损失以正则化训练，这改进了小对象的分割。

- 优点：CEM不仅**增强了与类别相关的特征图**，还通过**合并SE损失**，**迫使网络同等地考虑大小对象**。由于其轻量级架构，CEM可以**应用于各种主干网，计算开销很低**。

##### （9）Bilinear Attention

- 提出了一种新的**双线性关注块（双关注）**，以捕获**每个通道内的局部成对特征交互**，同时**保留空间信息**。双注意使用注意中的注意（AiA）机制来**捕获二阶统计信息**（以获取高阶信息）：外部点方向通道注意向量是根据内部通道注意的输出计算的。对于给定输入特征图X，双注意首先使用**双线性池来捕获二阶信息** 。

- 双注意块使用双线性池来建模**沿着每个通道的局部成对特征交互**，同时**保留空间信息**。使用所提出的AiA，与其他基于注意力的模型相比，该模型**更关注高阶统计信息**。双注意可以被结合到任何CNN主干中，以提高其代表能力，同时抑制噪声。 

##### 4.3.2 Spatial Attention

![image-20221122145032894](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221122145032894.png)

（1）⭐总述：

空间注意力可以被视为一种**自适应的空间区域选择机制**：关注哪里。如图4所示，RAM[31]、STN[32]、GENet[61]和Non-Local[15]是**不同类型空间注意力方法的代表**。RAM表示**基于RNN**的方法。STN表示那些**使用子网络来明确预测相关区域**的人。GENet表示那些**隐式使用子网络来预测软掩模以选择重要区域**的网络。**Non-Local**表示与self-attention相关的方法。我们首先总结了具有代表性的空间注意机制，并将过程g（x）和f（g（x，x）描述为表4中的等式1，然后根据图4讨论它们。

##### （2）RAM

- 采用**RNN**[147]和**强化学习**（RL）[148]，以**使网络了解关注的位置**。RAM开创了使用RNN进行视觉注意的先河，RAM有三个关键元素：（A）**一个窥视传感器**，（B）**一个窥探网络**和（C）**一个RNN模型**。
- 窥视传感器获取**坐标**lt−1和**图像**Xt。它输出多个分辨率补丁ρ（Xt，lt−1）  以lt为中心−1.扫视网络fg（θ（g））包括扫视传感器，并输出输入坐标lt的特征表示gt−1和图像Xt。RNN模型考虑gt和内部状态ht−并输出下一个中心坐标lt和例如softmax处的动作，从而产生图像分类任务。由于整个过程是不可区分的，它**在更新过程中应用强化学习策略**。这提供了一种简单但有效的方法来将网络聚焦于关键区域，从而**减少了网络执行的计算次数**，特别是对于大输入，同时改进了图像分类结果。 

![image-20221120214518065](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120214518065.png)

RAM中的注意过程[31]。（A） ：一瞥传感器将图像和中心坐标作为输入，并输出多个分辨率补丁。（B）  ：一瞥网络包括一瞥传感器，以图像和中心坐标作为输入并输出特征向量。（C） 整个网络周期性地使用一瞥网络，输出预测结果以及下一个中心坐标。图取自[31]。 

##### （3）Glimpse Network

- 如何**顺序执行**视觉识别
- 提出了一种深度递归网络，类似于RAM[31]，能够处理输入图像的**多分辨率裁剪**，称为一瞥，**用于多目标识别任务**。所提出的网络使用一瞥作为输入来更新其隐藏状态，然后在**每一步预测新对象以及下一个一瞥位置**。一瞥通常比整个图像小得多，这使得网络的计算效率更高。所提出的深度递归视觉注意模型由上下文网络、一瞥网络、递归网络、发射网络和分类网络组成。首先，上下文网络将下采样的整个图像作为输入，以提供递归网络的初始状态以及第一次一瞥的位置。然后，在当前时间步骤t，给定当前的一瞥xt及其位置元组lt，一瞥网络的目标是提取有用的信息
- 优点：模型的计算成本要低得多，而且它可以自然地处理不同大小的图像，因为它在每个步骤中只处理一个一瞥。重复注意机制还提高了鲁棒性，这也缓解了过度拟合的问题。该管道可并入任何最先进的CNN骨干网或RNN单元。

##### （4）Hard and soft attention

- 为了可视化图像字幕生成模型应该关注的地方和内容，提出了两种不同的注意力机制，硬注意力和软注意力。给定一组特征向量a={a1，…，aL}，ai∈  RD从输入图像中提取，该模型旨在通过在每个时间步长生成一个单词来生成字幕。因此，它们采用长短期存储器（LSTM）网络作为解码器；注意机制用于生成基于特征集a和先前隐藏状态ht的上下文向量zt−1，其中t表示时间步长。

- 注意力机制的使用通过允许用户了解模型关注的内容和位置，提高了图像字幕生成过程的可解释性。它也有助于提高网络的表现能力。 

##### （5） Attention Gate

- 以前的MR分割方法通常对**特定的感兴趣区域**（ROI）进行操作，这需要过度和浪费地使用计算资源和模型参数。
- 提出了一种简单而有效的机制，即**注意力门**（AG），以**集中于目标区域**，同时**抑制不相关区域中的特征激活**。给定输入特征图X和选通信号G∈  RC0×H×W以粗略的尺度收集并包含上下文信息，注意力**门使用附加的注意力来获得门控系数**。首先将输入X和门控信号线性映射到RF×H×W维空间，然后在信道域中压缩输出以产生空间注意力权重映射S∈  R1×H×W。

- 优点：

注意门**将模型的注意力引导到重要区域**，同时**抑制不相关区域的特征激活**。由于其轻量化设计，它大大**增强了模型的表示能力**，而不**会显著增加计算成本或模型参数的数量**。它是**通用的和模块化的**，使其易于在各种CNN模型中使用。 

##### （6）STN

- 翻转不变的特性使得神经网络适合于处理图像数据。然而，神经网络缺乏其他变换不变性，如旋转不变性、缩放不变性和扭曲不变性。
- 为了实现这些其他**变化不变性属性**，同时使神经网络**专注于重要区域**，提出了空间变换网络（STN），该网络使用**显式**程序学习**平移、缩放、旋转和其他更一般的扭曲的不变性**，使网络关注最相关的区域。STN是**第一个明确预测重要区域**并**提供具有变换不变性的深度神经网络的注意力机制**。随后的各种工作[7]、[36]取得了更大的成功。
- STN**自动聚焦于区分区域**，**并学习对某些几何变换的不变性**。

##### （7） Deformable Convolutional Networks

- 提出了可变形卷积网络（可变形卷积网）对几何变换保持不变，但他们以不同的方式关注重要区域。具体而言，可变形ConvNets不学习仿射变换。他们将卷积分为两个步骤，首先从输入特征图中对规则网格R上的特征进行采样，然后使用卷积核通过加权求和来聚集采样的特征。
- 优点：可变形卷积神经网络**自适应地选择重要区域**并**扩大**卷积神经网络的**有效感受野**；这在**对象检测**和**语义分割**任务中很重要。 

##### （8）Self-attention and variants

- 自我关注被提出，并在自然语言处理（NLP）领域取得了巨大成功。最近，它还显示出成为计算机视觉中的主导工具的潜力。

- 通常，自我注意力被用作一种**空间注意力机制**来**捕捉全局信息**。由于**卷积运算的局限性**，神经网络具有**固有的窄感受野**[154]，[155]，这**限制了神经网络在全局范围内理解场景的能力**。为了**增加感受野**，Wang等人[15]将Self-attention 引入了计算机视觉。以2D图像为例，给定特征图F∈  RC×H×W，自我关注首先**计算查询**、**键和值**Q、K、V∈ RC0×N，N=H×W。

- 大多数变体侧重于降低其计算复杂性，解纠缠的非局部方法[74]提高了自我关注的准确性和有效性

- 缺点：二次复杂性限制了它的适用性。

- CCNet[41]将自关注操作视为图卷积，并用几个稀疏连接的图代替由自关注处理的密集连接的图。为此，它提出了交叉关注，循环考虑行关注和列关注以获得全局信息。CCNet将自我关注的复杂性从O（N2）降低到O（N√N） 

- EMANet[40]从期望最大化（EM）的角度看待自我关注。提出了EM注意，它采用EM算法来获得一组紧凑的基，而不是使用所有点作为重建基。这将复杂性从O（N2）降低到O（N  K），其中K是紧基的数目。

- ANN[68]建议，使用所有位置特征作为关键字和向量是冗余的，并采用空间金字塔池[156]，[157]来获得一些代表性的关键字和值特征，以代替使用，以减少计算。

- GCNet[69]分析了用于自我关注的注意力图，发现对于同一图像中的不同查询位置，通过自我关注获得的全局上下文是相似的。因此，它首先提出预测所有查询点共享的单个关注图，然后根据该关注图从输入特征的加权和中获得全局信息。这类似于平均池，但是收集全局信息的更一般的过程。

- A2Net[70]受SENet的启发，使用两种不同的注意力将注意力分为特征收集和特征分布过程。第一种通过二阶注意力池聚合全局信息，第二种通过软选择注意力分配全局描述符

- GloRe[71]从图形学习的角度理解自我关注。它首先将N个输入特征收集到M？N个节点，然后学习节点之间的全局交互的邻接矩阵。最后，节点将全局信息分配给输入特征。在LatentGNN[72]、MLP  Mixer[158]和ResMLP[159]中可以找到类似的想法。

- OCRNet[73]提出了对象上下文表示的概念，它是同一类别中所有对象区域表示的加权聚合，例如所有汽车区域表示的平均加权。它用这种对象上下文表示代替了键和向量，从而成功地提高了速度和效率。

- disentangled non-local approach ，将自我注意解耦为成对项和一元项的核心思想。成对项侧重于建模关系，而一元项侧重于突出边界。这种分解防止了两个术语之间不必要的交互，从而大大改进了语义分割、对象检测和动作识别。 

- HamNet[42]将捕获全局关系建模为低秩完成问题，并设计了一系列白盒方法来使用矩阵分解捕获全局上下文。这不仅降低了复杂性，而且增加了自我关注的可解释性。 

- EANet 自我关注应该只考虑单个样本中的相关性，而应该忽略不同样本之间的潜在关系。为了探索不同样本之间的相关性并减少计算，它利用了采用可学习、轻量级和共享密钥和值向量的外部注意力。它进一步揭示了使用softmax对注意力图进行归一化不是最优的，并提出了双重归一化作为更好的替代方案。 

  ⭐除了作为神经网络的一种补充方法，自我关注也可以用来**代替卷积运算来聚集邻域信息**。如下（使用**局部自我关注**作为**基本神经网络块**的几个具体工作 ）：

- SASA[43]建议，使用**自我关注来收集全局信息过于计算密集**，而是采用局部自我关注来代替CNN中的所有空间卷积。作者表明，这样做可以提高速度、参数数量和结果质量。他们还探讨了位置嵌入的行为，并表明**相对位置嵌入**[160]是合适的。他们的工作还研究了如何**将局部自我关注与卷积相结合**。

- LR  Net[76]与SASA同时出现。它还研究了如何通过**使用局部自我关注来模拟局部关系**。一项全面的研究探讨了位置嵌入、内核大小、外观可组合性和对抗性攻击的影响。

- SAN 利用注意力进行局部特征聚合的两种模式，成对模式和拼接模式。它提出了一种同时在内容和通道上的新的向量注意力自适应方法 。除了在图像领域提供显著改进外，它还被证明在3D点云处理中有用[80]。 

- Vision T ransformers 视觉转换器（ViT），这是第一个用于图像处理的纯转换器架构。它能够获得与现代卷积神经网络相当的结果。ViT的主要部分是多头注意力（MHA）模块。MHA将序列作为输入。它首先将类标记与输入特征F连接起来∈ RN×C，其中N是像素数。然后得到Q，K∈  RN×C0和V∈  RN×C进行线性投影。接下来，Q、K和V在信道域中被分成H个头部，并分别应用于它们的自我关注。MHA方法如图8所示。ViT堆叠了多个具有完全连接层的MHA层、层归一化[162]和GELU[163]激活函数

ViT证明，纯粹的基于注意力的网络可以获得比卷积神经网络更好的结果，特别是对于大型数据集，如JFT-300[164]和ImageNet-21K[165]。继ViT之后，出现了许多基于变换器的架构，如PCT[27]、IPT[79]、T2T  ViT[44]、DeepViT[166]、SETR[81]、PVT[45]、CaiT[167]、TNT[82]、Swingtransformer[46]、Query2Label[83]、MoCoV3[84]、BEiT[85]、SegFormer[86]、FuseFormer[168]和MAE[169]，在许多类型的视觉任务中，包括图像分类、对象检测、，语义分割、点云处理、动作识别和自我监督学习。

![image-20221122143701891](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221122143701891.png)

视觉变换器[34]。左：建筑。视觉变换器首先将图像分割成不同的面片，并将其投影到特征空间中，变换器编码器对其进行处理以产生最终结果。右：基本视觉变换器块，带有多头注意力核心。图取自[34]。 

- GENet 通过在空间域中提供重新校准功能来捕获长距离空间上下文信息。GENet结合了零件采集和励磁操作。在第一步中，它聚集了大型社区的输入特征，并对不同空间位置之间的关系进行建模。在第二步中，首先使用插值生成与输入特征图大小相同的关注图。然后，通过乘以关注图中的对应元素来缩放输入特征图中的每个位置。

聚集激发模块重量轻，可以像SE块一样插入每个剩余单元。它在抑制噪声的同时强调重要特征

- PSANet 在成功捕获卷积神经网络中的长距离依赖性的激励下，Zhao等人[87]提出了新的PSANet框架来聚合全局信息。它将信息聚合建模为信息流，并提出一种双向信息传播机制，使信息在全局范围内流动

汇总了全球信息，同时强调了相关特征。它可以作为有效的补充添加到卷积神经网络的末尾，以大大改进语义分割。 

##### 4.3.3 Temporal Attention

![image-20221122145100485](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221122145100485.png)

（1）⭐总述：时间注意力可以被视为一种**决定何时注意**的**动态时间选择机制**，因此**通常用于视频处理**。先前的工作[171]、[172]经常强调如何捕获短期和长期跨帧特征依赖性。

（2）Self-attention and variants

- RNN和时间池（temporal  pooling）或权重学习（weight learning）已被广泛用于视频表示学习的工作中，以捕捉帧之间的交互，但这些方法在效率或时间关系建模方面都有局限性。 
- 提出了一种全局局部时间表示（GLTR），以利用视频序列中的多尺度时间线索。GLTR由用于局部时间上下文学习的扩展时间金字塔（DTP）和用于捕获全局时间交互的时间自我关注模块组成。 DTP采用扩张卷积，扩张率逐渐增加，以覆盖各种时间范围，然后连接各种输出以聚合多尺度信息。来自相邻帧的短期时间上下文信息有助于区分视觉上相似的区域，而长期时间信息用于克服遮挡和噪声。GLTR结合了两个模块的优点，增强了表示能力并抑制了噪声。它可以结合到任何最先进的CNN主干中，以学习整个视频的全局描述符。然而，自我注意机制具有二次时间复杂性，限制了其应用。

（3）TAM 

- 为了高效灵活地捕捉复杂的时间关系，Liu等人[172]提出了一种时间自适应模块（TAM）。它采用自适应内核而非自关注来捕获全局上下文信息，时间复杂度低于GLTR[171]
- TAM有两个分支机构，一个本地分支机构和一个全球分支机构。给定输入特征图X∈  RC×T×H×W，全局空间平均池GAP首先应用于特征图，以确保TAM具有低计算成本。然后，TAM中的本地分支使用几个具有ReLU的1D卷积 以产生用于增强逐帧特征的位置敏感重要度图。
- 与局部分支不同，全局分支是位置不变的，并专注于基于每个信道中的全局时间信息生成信道自适应核。对于第c个通道，内核可以写为 
- 在局部分支和全局分支的帮助下，TAM可以捕获视频中的复杂时间结构，并以低计算成本增强每帧特征。由于其灵活性和轻量化设计，TAM可以添加到任何现有的2D CNN中

##### 4.3.4 Branch Attention

![image-20221122160845605](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221122160845605.png)

（1）总述：⭐分支注意可以被视为一种**动态的分支选择机制**：与多分支结构一起使用时，需要注意哪些分支。

（2） Highway networks

- 受长期-短期记忆网络的启发，Srivastava等人[113]提出了高速公路网络，该网络采用自适应门控机制，使信息能够跨层流动，以解决训练非常深的网络的问题。
- 门控机制和跳跃连接结构使得使用简单的梯度下降方法直接训练非常深的公路网成为可能。与固定跳过连接不同，门控机制适应输入，这有助于跨层路由信息。公路网可以并入任何CNN。 

（3）SKNet

- 神经科学界的研究表明，视觉皮层神经元根据输入刺激自适应地调整其感受野（RF）的大小[174]。这启发了Li等人[114]提出了一种称为选择性核（SK）卷积的自动选择操作。SK卷积使用三种操作实现：分离、融合和选择。在分割期间，将具有不同内核大小的变换应用于特征图以获得不同大小的RF。然后通过逐元素求和将来自所有分支的信息融合在一起，以计算门向量。这用于控制来自多个分支的信息流。最后，在门向量的引导下，通过聚集所有分支的特征图来获得输出特征图。
- 每个变换Fk具有唯一的内核大小，以为每个分支提供不同的信息尺度。为了提高效率，Fk是通过分组或深度卷积实现的，然后依次进行扩展卷积、批量归一化和ReLU激活。
- SK卷积使网络能够根据输入自适应地调整神经元的RF大小，以很低的计算成本显著改善结果。SK卷积中的门机制用于融合来自多个分支的信息，由于其轻量化设计，通过替换所有大型内核卷积，SK卷积可以应用于任何CNN主干。ResNeSt[115]还采用这种关注机制以更一般的方式改进CNN主干，在ResNet[145]和ResNeXt[175]上给出了优异的结果。

（4）CondConv

- CNN中的一个基本假设是所有卷积核都是相同的。鉴于此，增强网络代表能力的典型方法是增加其深度或宽度，这会带来显著的额外计算成本。为了更有效地提高卷积神经网络的容量，Yang等人[173]提出了一种新的多分支算子，称为CondCov 
- 这个过程相当于多个专家的集合，如图10所示。CondCov充分利用了多分支结构的优势，使用分支注意力方法，计算成本很低。它提供了一种有效提高网络性能的新方法 

![image-20221122161745419](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221122161745419.png)

（a）CondCov首先组合不同的卷积核，然后使用组合的核进行卷积。（b）  混合专家首先使用多个卷积核进行卷积，然后合并结果。虽然（a）和（b）是等价的，但（a）的计算成本要低得多。图取自[173] 

（5）Dynamic Convolution

- 轻量级神经网络的极低计算成本限制了网络的深度和宽度，进一步降低了其代表性。为了解决上述问题，Chen等人[116]提出了动态卷积，这是一种新颖的算子设计，它以可忽略的额外计算成本增加了表示能力，并且不会与CondCov[173]并行地改变网络的宽度或深度
- 动态卷积使用相同大小和输入/输出维度的K个并行卷积内核，而不是每层一个内核。与SE块一样，它采用挤压和激励机制来生成不同卷积核的注意力权重。 这些卷积核通过加权求和动态聚合并应用于输入特征图X： 
- 与将卷积应用于特征图相比，挤压和激励以及加权求和的计算成本极低。因此，动态卷积提供了一种有效的操作来提高表示能力，并且可以容易地用作任何卷积的替代。

##### 4.3.5 Channel & Spatial Attention

（1）总述⭐：

- 渠道和空间注意力结合了渠道注意力和空间注意力的优点。它自适应地选择重要对象和区域[50]。剩余注意力网络[119]开创了渠道和空间注意力领域的先河，强调了信息特征在空间和渠道维度上的重要性。它采用由多个卷积组成的自下而上的结构来生成3D（高度、宽度、通道）注意力图。然而，它具有较高的计算成本和有限的感受野。

- 为了利用全局空间信息，后来的工作[6]，[117]通过引入全局平均池以及解耦信道关注度和空间信道关注度来增强特征的区分，以实现计算的效率
- 其他工作[10]，[101]将自我注意机制应用于通道和空间注意，以探索成对交互。进一步的工作[120]，[124]采用空间通道注意机制来扩大感受野。

（2）Residual Attention Network

- 受ResNet[145]的成功启发，Wang等人[119]通过将注意力机制与剩余连接相结合，提出了非常深的卷积剩余注意力网络（RAN）。堆叠在剩余注意力网络中的每个注意力模块可以分为掩码分支和主干分支。主干分支处理功能，可以通过任何最先进的结构实现，包括预激活残余单元和起始块。掩码分支使用自底向上的结构来学习相同大小的掩码，该掩码对主干分支的输出特征进行软加权。在两个1×1卷积层之后，S形层将输出归一化为[0，1]。
- 在每个注意力模块内，自下而上的前馈结构对空间和跨渠道依赖性进行建模，从而实现一致的性能改进。剩余注意力可以以端到端的培训方式融入任何深度网络结构。然而，所提出的自下而上的结构未能充分利用全球空间信息。此外，直接预测3D注意力图具有高的计算成本。 

- 其中GAP和GMP表示空间域中的全局平均池化和全局最大池化操作。空间注意力子模块对特征的空间关系进行建模，并与通道注意力互补。与通道注意力不同，它应用具有大内核的卷积层来生成注意力图 
- 将通道注意力和空间注意力顺序地结合在一起，CBAM可以利用特征的空间和跨通道关系来告诉网络应该关注什么以及应该关注哪里。更具体地说，它强调有用的渠道以及增强信息性的地方区域。由于其轻量级设计，CBAM可以无缝集成到任何CNN架构中，而无需额外的成本。然而，渠道和空间注意力机制仍有改进的空间。例如，CBAM采用卷积产生空间注意力图，因此空间子模块可能受到有限的感受野的影响。 

（3）BAM

- 与CBAM同时，Park等人[117]提出了瓶颈注意力模块（BAM），旨在有效提高网络的表征能力。它使用扩张卷积来扩大空间注意力子模块的感受野，并根据ResNet的建议构建瓶颈结构以节省计算成本。 
- 对于给定的输入特征图X，BAM推断频道关注度sc∈ RC和空间注意力ss∈  两个并行流中的RH×W，然后在将两个分支输出大小调整为RC×H×W后将两个注意力图相加。通道注意力分支与SE块一样，将全局平均池应用于特征图以聚合全局信息，然后使用具有通道降维的MLP。为了更高效的使用上下文信息。空间注意力分支结合了瓶颈结构和扩张卷积。
- BAM可以在空间和通道维度上强调或抑制特征，并改进表达能力。应用于通道和空间注意力分支的降维使其能够与任何卷积神经网络集成，而无需额外的计算成本。然而，尽管扩张卷积有效地扩大了感受野，但它仍然无法捕获长距离上下文信息以及编码跨域关系。 

（4）scSE

- 为了聚合全局空间信息，SE块将全局池应用于特征地图。然而，它忽略了像素级空间信息，这在密集预测任务中很重要。因此，Roy等人[123]提出了空间和信道SE块（scSE）。与BAM一样，使用空间SE块作为SE块的补充，以提供空间注意力权重来关注重要区域。 
- 给定输入特征图X，将空间SE和信道SE两个并行模块应用于特征图以分别编码空间和信道信息。信道SE模块是普通的SE块，而空间SE模块采用1×1卷积进行空间压缩。两个模块的输出均已熔断。
- 所提出的scSE块结合了信道和空间注意力以增强特征以及捕获像素级空间信息。因此，分割任务将大大受益。在F-CNNs中集成scSE块以可忽略的额外成本实现了语义分割的一致改进。 

##### 4.3.6 Triplet Attention

(1)总述：⭐

- 在CBAM和BAM中，通道注意力和空间注意力是**独立计算**的，忽略了这两个领域之间的关系[121]。受空间注意力的激励，Misra等人[121]提出了**三重注意力**，这是一种轻量级但有效的注意力机制，**用于捕获跨域交互**。
- 给定一个输入特征图X，三重注意力使用三个分支，每个分支在捕获H、W和C中任意两个域之间的跨域交互中发挥作用。在每个分支中，沿着不同轴的旋转操作首先应用于输入，然后Zpool层负责聚集第零维的信息。最后，核大小为k×k的标准卷积层对最后两个域之间的关系进行建模。









（3）第一阶段RNN、第二阶段STN、第三阶段SeNet、第四阶段self-attention 

（4）将现有的注意力方法分为六类，包括四个**基本类别**：**channels**注意力（关注什么[50]）、**spatial**注意力（关注哪里）、**temporal**注意力（何时关注）和**branch** **channels**（关注哪些），以及两个混合的组合类别：**channels**和**spatial**注意力以及**spatial**和**temporal**注意力。

<br>

##### 计算机视觉中的注意力机制：

##### 1.总述：

将“专注于辨别区域，并快速处理这些区域”这一过程抽象为：

![image-20221115111808471](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221115111808471.png)

- g（x）表示**产生**与关注辨别区域的过程相对应的**注意力。**

- f（g（x），x）表示基于与处理关键区域和获取信息一致的关注度g（x）来**处理输入**x

<br>

##### （1）对于self-attention:

g(x) and f(g(x), x) can be written as
Q, K, V = Linear(x) 
g(x) = Softmax(QK) 
f(g(x), x) = g(x)V

##### （2）For SE:

g(x) and f(g(x), x) can be written as
g(x) = Sigmoid(MLP(GAP(x))) 
f(g(x), x) = g(x)x

<br>

##### 2.Channel Attention

（1）在**深度神经网络**中，不同特征图中的不同通道通常表示不同的对象。**想到卷积神经网络的多通道输出，每个输出通道都有一个卷积核，每个通道提取一类图像的特征，最后将所有提取到的特征加权组合到一起。**

（2）通道注意力**自适应地重新校准每个通道的权重**，并可以被视为一个对象选择过程，**从而确定要注意什么**（需要注意的权重更高？）

<br>

##### 代表模型：

##### SeNet:

- Se块：用于收集全局信息、捕获通道关系和提高表示能力
- squeeze module

global average pooling:收集全局信息

- excitation module

fully-connected layers and non-linear layers (ReLU and sigmoid).：捕获通道关系，输出注意力向量

缺点：

squeeze module中： **global average pooling is too simple to capture complex global information.** 

excitation module中：fully-connected layers increase the **complexity** of the
model.

<br>

##### GSoP-Net

相比SeNet的优点：using a **global second-order pooling** (GSoP) block to model high-order statistics while gathering global information.

<br>

- Representative channel attention **mechanisms** ordered by **category** and **publication date**. Their **key aims are to emphasize important channels and**
  **capture global information**. 

- **Application** areas include: Cls = classification, Det = detection, SSeg = semantic segmentation, ISeg = instance
  segmentation, ST = style transfer, Action = action recognition.

- g(x) and f(g(x), x) are the **attention process described by Eq**. 1（公式1）. Ranges means the
  **ranges of attention map**. S or H means **soft or hard attention.**
- (A) channel-wise product. (I) emphasize important channels, (II) capture global
  information.

![image-20221120103004395](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221120103004395.png)

