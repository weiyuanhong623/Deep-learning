#### 认识什么是深度学习，常用方法，常用框架

---

##### 什么是深度学习

1.深度学习是指通过训练由**多层**网络结构构成的模型来对数据进行回归或分类

2.深度学习作为机器学习的一个子集，优点有：

- 传统机器学习的训练流程往往由多个独立的步骤组成，每个步骤结果的好坏会影响下一步骤的执行结果，从而会影响整个训练结果（非端到端）；而深度学习从模型输入端输入数据，从输出端直接得到结果（**端到端**）。
- 深度学习在训练过程中取代了劳动密集型的特征⼯程。机器学习中模型输入的数据是在原始数据中提取的特征（特征工程和学习模型部分分开）；深度学习通过模型的人工神经网络进行特征的提取的转换（当数据量特别大的时候，特征工程往往不稳定）

---

##### 常用方法

- 批量归一化
- 反向传播
- 随机梯度下降（minisgd）
- 损失函数（平方损失、均方损失）
- 汇聚（池化）
- Dropout
- 填充和步幅

---

##### 常用框架

| 框架名称       |
| -------------- |
| **TensorFlow** |
| **Caffe**      |
| **PyTorch**    |
| **MXNet**      |
| **Keras**      |



---

---



#### 回顾-典型隐层神经网络的结构

---

#### 卷积神经网络的基本含义以及重要概念-1（卷积原理）

---

#### 卷积神经网络的基本含义以及重要概念-2（填充、步幅）

---

#### 卷积神经网络的基本含义以及重要概念-3（池化、通道）

---

#### 卷积神经网络的基本含义以及重要概念-4（激活函数）

---

#### 卷积神经网络的基本含义以及重要概念-5（Softmax回归）

- softmax回归模型通过单个**仿射变换**将我们的输⼊直接映射到输出，然后进⾏softmax操 作。 

  <br>

- softmax回归主要用于分类，和线性回归不同是softmax回归处理的数值是离散的，线性回归的数值是连续的，同时对于有自然顺序的类别也可以转化为回归问题。  

  <br>

- 使用softmax运算将**未规范化的预测值**变换为**非负**且**总和为1**，同时让模型保持**可导**的性质。

**主要过程**：对每个预测值求幂（以e为底数）后除以它们求幂后的总和。  

<br>

- softmax运算是非线性函数，但softmax回归的输出仍然**由输⼊特征的仿射变换**决定，所以是线性模型。  

  <br>

- ##### 使用交叉熵损失函数，这里用交叉熵来衡量预测和标号的差异，并将这种差异作为损失

**主要过程**：拿出对应真实标号的预测值，取ln，乘以-1（应为经过softmax运算后都是0<预测值<=1的规范值,因此以e为底的对数值都是<=0的，所以要添上负号，损失通常非负）。  

<br>

- 特别注意，当未规范化的预测值**特别大**的时候求幂（以e为底数）后可能会大于数据类型容许的最⼤数字，发生**上溢**（overflow）。

**解决方法**：

1. 在继续softmax计算之前，先从所有**预测值**中减去**max(ok)**（最大预测值）。此时exp(oj − max(ok))后将有接近零的值，发生**下溢**（underflow）。**所以**运算的时候避免exp(oj − max(ok))，⽽直接使⽤oj − max(ok)
2. 针对1.中的问题，将softmax和交叉熵结合在⼀起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。

<br>

---

#### LeNet网络的研究与实现

![image-20221126135452291](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221126135452291.png)

1.用卷积层代替全连接层，**好处**：能够保存图像中的空间结构；能够减少参数（**参数少、模型复杂度小、更不容易出现过拟合现象**）

2、LeNet（**LeNet-5**）结构主要由两部分组成：

- 卷积编码器：由**两个**卷积层组成; 

（1）每个卷积块中的基本单元是⼀个**卷积层**、⼀个**sigmoid激活函数**和**平均汇聚层**。

（2）每个卷积层使⽤**5 × 5**卷积核和⼀个**sigmoid激活**函数。这 些层将输⼊映射到多个⼆维特征输出，通常同时增加通道的数量。第⼀卷积层有**6个输出通道**，⽽第⼆个卷 积层有**16个输出通道**。每个**2 × 2池操作（步幅2）**通过**空间下采样**将维数减少4倍（高减少2倍，宽减少两倍，整个减少2*2=4倍）。卷积的输出形状由批量⼤ ⼩、通道数、⾼度、宽度决定。

<br>

- 全连接层密集块：由**三个**全**连接隐藏**层组成。

（1）LeNet的稠密块有三个全连接层，分别有120、84和10个输出（10是在应用于分类的时候对于的类别数）。

（2）**⼩批量中展平每个样本**，以将卷积块的输出传递给全连接层（变成向量）。

<br>

---

#### AlexNet网络的研究与实现

![image-20221126135823202](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221126135823202.png)

1.成就：它**⾸次**证明了学习到的特征可以超越⼿⼯设计的特征。

2.在AlexNet⽹络的最底层，模型**学习**到了⼀些类似于**传统滤波器**的**特征抽取器**。

（1）**总述**：AlexNet的**更⾼层**建⽴在这些**底层表⽰的基础上**，以表⽰**更⼤的特征**，如眼睛、⿐⼦、草叶等等。⽽**更⾼的层 可以检测整个物体**，如⼈、⻜机、狗或⻜盘。**最终**的隐藏神经元**可以学习图像的综合表⽰**，从⽽使属于不同 类别的数据易于区分。

<br>

**AlexNet**模型由8层网络组成：

- 5个卷积层

第1层卷积层卷积核形状**（10，10）**，第2层卷积层**（5，5）**，第3、4、5层都是**（3，3）**。然后在第**1、2、5**层卷积层后加上**（3，3），步幅为2的最大池化层**。

- 2个全连接隐藏层

- 1个全连接层

- 全部使用**Relu()**作为激活函数。

- ###### 容量控制和预处理

（1）使用**Dropout**控制**全连接层的模型复杂度**（LeNet只有权重衰减）

（2）为了进⼀步扩充数 据，AlexNet在训练时**增加了⼤量的图像增强数据**，如**翻转**、**裁切**和变⾊。这使得**模型更健壮**，更⼤的样本量 有效地**减少了过拟合**。

---

#### VGG网络的研究与实现

![image-20221126150542568](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221126150542568.png)

##### 1.结构

（1）VGG网络：

- 卷积层（VGG块）
- 全连接层

（2）单个**VGG块**主要由个部分组成：

- 卷积层（k=3,padding=1**保持卷积变换后形状不变**）、**ReLU激活函数**、池化层（k=2,stride=2，**降低计算量，扩大感受野**）
- 超参数变量**conv_arch**。该 变量指定了**每个VGG块**⾥**卷积层个数**和**输出通道数**。

##### 2.具体模型：VGG-11

（1）第⼀个**模块**有64个 输出通道，每个后续模块**将输出通道数量翻倍，直到该数字达到512**。

（2）该⽹络络有5个VGG块,使⽤8个卷积层和3个全连接 层，因此它通常被称为VGG-11。

<br>

⭐**对于LeNet、AlexNet、VGG网络的小结：**

（1）**深层且窄**的卷积（即3 × 3）⽐较浅层且宽的卷积更有效。

（2）通过⼀系列的**卷积层与汇聚层**来**提取空间结构特征**；然后通过**全连接层**对**特征的表征**进⾏**处理**。AlexNet和VGG对LeNet的改进主要在于如何**扩⼤和加深这两个模块**，同时使用了Dropout控制全连接层模型复杂度。

<br>

---

#### NiN网络的研究与实现

![image-20221126150631633](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221126150631633.png)

1.使⽤全连接层，可能会完全放弃表征的空间结构。⽹络中的⽹络（NiN）提供了⼀个⾮常简单的解决⽅案：**在每个像素的通道上分别使⽤多层感知机**

2.结构

##### （1）NiN块结构：

- 卷积层
- 两个卷积核为1*1的卷积层（带有RelU()激活函数，充当逐像素的全连接层），**后一个卷积层为每个像素增加了非线性性（因为包含了RelU激活函数）**

##### （2）NiN网络结构：

- NiN块后面接**最大池化层**，最后接**全局平均池化层**

- NiN使⽤包含窗⼝形状为**11×11、5×5和3× 3**的卷积层的NiN块，输出通道数量与AlexNet中的相同。**每个NiN块后有⼀个最⼤汇聚层**，汇聚窗⼝形状为**3 × 3，步 幅为2**。最后放⼀个**全局平均汇聚层**（global average pooling layer），⽣成⼀个**对数⼏率**（logits）。 NiN设计的⼀个**优点**是，它**显著减少了模型所需参数的数量**。然⽽，在实践中，这种设计有时会**增加训练模 型的时间。**

- ##### ⭐概念：global average pooling(GAP)

  是将**每一张特征图**（每个通道）计算所有像素点的均值，输出**一个数据值**（再通过展平成为向量进行下一步处理，如sotfmax运算）

3.特点：

- NiN去除了容易造成过拟合的全连接层，将它们替换为**全局平均池化层**（即在所有位置上进⾏求和）

- **移除全连接层可减少过拟合**，同时**显著减少NiN的参数。**

- ##### 使用全局平均池化的缺点：收敛的速度变慢。

<br>

---

#### GoogleNet网络的研究与实现

![image-20221126150727339](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221126150727339.png)

![image-20221126150718450](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221126150718450.png)

⼀个重点是解决了什么样⼤⼩的卷积核最合适的问题。有时使⽤不同⼤⼩的卷积核组合是有利的

<br>

##### 1.Inception块

（1）

- 块由四条并⾏路径组成。前三条路径使⽤窗⼝⼤⼩为1 × 1、3 × 3和5 × 5的卷积层， 从不同空间⼤⼩中提取信息。中间的两条路径在输⼊上**先执⾏1 × 1卷积**，以减少通道数，从⽽降低模型的复杂 性。第四条路径使⽤3 × 3最⼤汇聚层，然后使⽤1 × 1卷积层来改变通道数。
- 四条路径都使⽤合适的填充 来使**输⼊与输出的⾼和宽⼀致**，最后我们将每条线路的输出在**通道维度上连结**，并构成Inception块的输出。

（2）GoogLeNe**t有效的原因**：滤波器（filter）的组合，它们可以⽤**各种滤 波器尺⼨**探索图像，这意味着**不同⼤⼩的滤波器**可以有效地识别不同范围的图像细节。同时对不同filter**分配不同的参数**。

<br>

##### 2.GoogLeNet模型：

- 9个**Inception块**和**全局平均池化层**的堆叠来⽣成其估计值。
- Inception块 之间的**最⼤汇聚层**可**降低维度**。
- 全局平均汇聚层避免了在最后使⽤全连接层（减少参数，降低模型复杂度）。

<br>

---

#### 批量归一化

##### 问题提出：神经网络层越来越深，浅层的梯度较大，越深的梯度越小（对权重的更新就越少），浅层的部分收敛的较快，深层收敛较慢。深层的参数进行变换的时候会导致浅层不断的跟随重新进行学习，导致整个过程的学习变慢。（线性变换）

批量归一化就是在解决这个问题。

<br>

##### 批量归一化主要思路：

利⽤⼩批量的均值和标准差，不断调整神经⽹络的中间输出，使得每层的**输入或者输出**都符合某一个分布。使整个神 经⽹络各层的**中间输出值更加稳定**。

<br>

#### ⭐批量归一化所作的工作：

- ##### 本质上是在每个批量里面加入噪音来控制模型复杂度。（先进行标准化：减去均值后除以标准差，标准化后符合标准正态分布，均值为0，方差为1，标准化后再进行偏移和缩放）

![image-20221110215412594](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221110215412594.png)

- ##### 这里的批量均值和批量标准差是在每次的⭐随机批量上计算得来的，是噪音项，γ和β分别做了 随机偏移 和 随机缩放（这两个参数是需要学习的）。

### ⭐随机批量体现在读取数据集的时候是随机读取batch_size大小的数据。每一轮迭代都是对每个批量的样本进行学习，每个批量的样本经过模型后的输出进行批量归一化。

- 没必要跟丢弃法混合使用（**都是控制模型复杂度**）

**特点：**

（1）**可持续加速深层⽹络 的收敛速度**。**结合残差块**，批量规范化使得研究⼈员能够训练100层以上的⽹络。

（2）可以将参数的**量级进⾏统⼀**（如有的权重过大，需要进行归一化使得特征的权重相同）

（3）更深层的**⽹络很复杂**，**容易过拟合**。这意味着**正则化**变得更加重要。

（4）大致过程：在每次训练迭代中，我们⾸先**规范化输 ⼊**，即通过**减去其均值并除以其标准差**，其中两者均**基于当前⼩批量处理**。接下来，我们应⽤**⽐例系数和⽐ 例偏移**。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。**注**：（使用**批量⼤⼩为1**的⼩批量应⽤批量规范化，**将⽆法学到任何东西**，减去均 值之后，每个隐藏单元将为0）。只有使⽤**⾜够⼤的⼩批量**，批量规范化这种⽅法才是**有效且稳定**的。

<br>

---

#### ResNet网络的研究与实现

（1）目标：⭐加深神经网络提高模型精度，应对更深模型的梯度消失问题。

（2）⭐残差块如何应对梯度消失（由乘法变为加法），以及如何实现对深层权重的高效更新：

##### 残差块：

![image-20221112155055255](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221112155055255.png)

**主要流程**：下个网络结构的输入f(x)=x+g(x)；当新加的的一部分模型**没有学到任何参数**（g(x)=0,⭐学习效果很好，可能会导致反向传播的时候梯度为0，对应的权重也就不变），下个网络结构的输入就等价于当前网络层的输出x（此时为f(x)为x的**恒等映射**。新加的模型和原模型将同样有效）；同时新模型**可能得出更优的解**来拟合训练数据集，因此添加层似乎更容易降低训练误差（需要拟合出**残差映射g(x)**；此时将将**权重、偏差设置为0，就会得到恒等映射**）。**整体效果**使得深层次网络结构的精度**至少**不会随着模型复杂度增加而减少。

在残差块中的体现:

```python
#定义正向传播                                 #整体是在ReLU激活函数之前进行残差映射或者恒等映射
def forward(self,X):
    Y=F.relu(self.bn1(self.conv1(X)))       #先经过第一层卷积层然后进行归一化，接着进行ReLU激活函数
    Y=self.bn2(self.conv2(Y))               #接着进入第二层卷积层然后归一化
    if self.conv3:                          #对于x进行1x1卷积层或者不进行任何操作。
        X=self.conv3(X)
    Y+=X
    return  F.relu(Y)                       #映射后进行激活
```

（3）结构：

##### ResNet（ResNet-18，卷积层数+全连接层数）主要构造：

1.残差块

- 3x3卷积，归一化层，RelU层，3x3卷积层，归一化层。
- 对于下一层的原输入x，可进行1x1卷积层，也可不经过任何变换。

2.ResNet模型

- 输出通道64，核7x7卷积层；归一化层；步幅2，3x3最大池化层
- 4个**由2个残差块组成的模块**（每个残差块将上一个模块的通道数x2，并将高、宽减半）
- 全局平均池化；全连接层

<br>

---

#### DenseNet网络的研究与实现

###### f拓展成超过两部分的信息就得到DenseNet。

![image-20221113130045956](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221113130045956.png)

##### 1.DenseNet和ResNet的区别：

（1）DenseNet输出的是**连接**。在应⽤越来越复杂的函数序列后，我们执⾏从x到其展开式的映射：

```
x → [x, f1(x), f2([x, f1(x)]), f3([x, f1(x), f2([x, f1(x)])]), . . .] . 
```

（2）DenseNet这个名字由变量之间的“稠密连接”⽽得来，最后⼀层与之前的所有层紧密 相连。

2.DenseNet的构成：

（1）稠密块（dense block）：定义**如何连接输⼊和 输出**

（2）过渡层（transition layer）：控制**通道数量，使其不会太复杂。**

<br>

- ###### 稠密块：

由批量规范化、激活和卷积的组成的卷积块组成。

- ###### 过渡层：

通过1 × 1卷积层来**减⼩通道数**，并使⽤步幅为2的**平均汇聚层减半⾼和宽**，从⽽进⼀步降低模型复杂度。

##### （3）DenseNet模型：

- 核7x7，步幅2，填充3**卷积层**，**归一化层**，**ReLU**，核3x3，步幅2，填充1**最大池化层**
- 4个稠密块（每个稠密块由4个卷积块组成）
- 归一化层，全局平均池化，全连接层

