### DAY1



##### 完成实验环境的搭建





---

---

网页学习部分





##### 什么是神经网络

人工神经网络是受到人类大脑结构的启发而创造出来的

训练深度神经网络的过程就叫做深度学习





##### 将数据输入神经网络

（1）对于图像，计算机要存储三个独立的矩阵（三个**通道**）分别对应红、绿、蓝，矩阵里面的**数值**就对应于图像的红绿蓝**强度值**

（2）一般将3个矩阵转化成1个**向量**x（向量可以理解成1 * n或n * 1的数组，前者为行向量，后者为列向量

![image-20221020183623969](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221020183623969.png)

（3）每一个输入到神经网络的数据都被叫做一个特征，由二维矩阵变成的**向量**就叫做**特征向量**，向量的长度就是该向量的**维度**。神经网络接收这个特征向量x作为输入，并进行预测，然后给出相应的结果。

```
eg:
对于一张20*20*20像素的图像 在计算机中有三个二维矩阵进行存储，转为向量后规格为：1*(20*20*3)或(20*20*3)*1，该向量维度就是1200，这张图像就有1200个特征
```





**总结**：不同应用场景所输入的数据在计算机中都有对应的数字表示形式，将其化成一个特征向量，然后将其输入到神经网络中





##### 神经网络如何进行预测



向量相乘（对应元素相乘再相加）

```
dot(w,x)
```



场景：对某一事件进行预测

输入：影响事件发展结果因素（特征），并对特征赋予不同的**权重**，设置不同输出对应的事件结果

输出：预测结果





##### 激活函数——sigmoid

![image-20221020185827218](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221020185827218.png)

z越大越接近1，越小越接近0







如用于分类： 若输出经过sigmoid的隐藏层后输出为0.8，说明有80%的概率是目标类别





---

---



##### 《drive into deeplearning》学习部分





任⼀调整参数后 的程序，我们称为**模型**（model）。

通过操作参数⽽⽣成的所有不同程序（输⼊-输出映射）的集合称为“**模 型族**”

使⽤数据集来选择参数的元程序被称为**学习算法**（learning algorithm）。





#### 机器学习的一些关键组件



1.数据

（1）数据由**样本**组成，每个样本由被称为**特征**的属性组成，机器学习**模型**会根据这些属性进⾏预测。如在**监督学习问题**中，要预测的是⼀个特殊的属 性，它被称为**标签**（label，或⽬标（target））。

（2）与传统机器学习⽅法相⽐，深度学习的⼀个主要优势是可以处理**不同⻓度的数据**。

（3）更多的数据意味着可以**减少对预先设想假设的依赖**（数据集的由⼩变⼤为现代深度学习的成功奠定基础）

（4）要**筛除无效数据**以及注意数据集的**均衡问题**



2.模型

深度学习与经典⽅法的区别主要在于：前者关注的功能强⼤的模型，这些模型由神经⽹络错综复杂的交织在⼀起，包含层层数据转换，因此被称为深度学习（deep learning）。



3.目标函数

（1）定义模型的**优劣程度的度量**，这个度量在⼤多 数情况是“**可优化**”的，我们称之为⽬标函数（objective function），有时被称为损失函数。

（2）通常，损失函数是**根据模型参数**定义的，并取决于数据集。最常⻅的损失函数是平⽅误差（squared error）

（3）总的流程是：通过**最⼩化总损失**来学习 模型**参数**的**最佳值**。这里是获取**损失**。



4.优化算法

（1）**搜索出最佳参数**，以最⼩化损失函数（⼤多流⾏的优化算法通常基于⼀种基本⽅法‒梯度下 降）。在每个步骤中，梯度下降法都会检查每个参数，看看如果你仅对该参数 进⾏少量变动，训练集损失会朝哪个⽅向移动。然后，它在可以**减少损失的⽅向**上优化参数。







#### 机器学习的类别

##### 1.监督学习

（1）在“给定输⼊特征”的情况下预测标签。每个“**特征-标签**”对都称为 ⼀个样本（example）。估计给定输⼊特征的标签。

（2）监督学习过程：从已知数据集中随级选取一个子集，给子集中的每个样本获取真实标签（有的样本已有标签；有的需要人工标记），输入的特征和标签构成训练集通过**学习算法**输出**模型**，然后使用测试集特征进行输入，模型的输出作为对输入特征的标签的预测。



###### 监督学习的细分：



##### 回归

本质由输出决定，输出的预测标签取任意数值。⽬标是⽣成⼀个模型，它的预测⾮常接近实际标签值。



##### 分类

在分类中，我们训练⼀个**分类器**，它的输出即为预测的类别。

- 模型预测样本属于哪个类别。最简单的分类问题是只有两类，我们称之为“**⼆元分类**”。

- 有两个以上的类别时，我们把这个问题称为**多元分类**（multiclass classification）问题。
- 寻找层次结构，层次 结构假定在许多类之间存在某种关系。**层次分类**（如对动物进行层次分类）



##### 标记问题

学习预测**不相互排斥的类别**的问题称为**多标签分类**



##### 搜索

模型要输出**有序**的**元素⼦集**。可以先为集合中的每个元素分配相应的**相关性分数**，然后**检索**评级最⾼的元素。



##### 推荐系统

向特定⽤⼾进⾏“个性化” 推荐。推荐系统会为“给定⽤⼾和物品”的匹配性打分，这个“分数”可能是估计 的评级或购买的概率，然后检索得分最⾼的对象集推给用户



### DAY 2

---

---

#####   

##### 《drive into deeplearning》学习部分



##### 序列学习

输⼊和输出都是可变⻓度的序列  

  

- ###### 标记和解析

⽤属性注释⽂本序列（如给一个句子标记实体）

- ###### ⾃动语⾳识别

音频和文本之间没有1：1的对应关系，可能多个样本对应一个单独单词

- ###### ⽂本到语⾳

输出音频多于输入

- ###### 机器翻译

⽽在机器翻译中，颠倒输⼊和输出的顺序⾮常 重要。  

  

  

   

#### 2.无监督学习

数据中不含有“⽬标”的机器学习问题为⽆监督学习（unsupervised learning）。  

  

- 聚类

  如无标签的情况下进行分类

- 主成分分析

  少量的参数来准确地捕捉数据的**线性相关属性**

- 因果关系和概率图模型

  描述观察到的 许多数据的根本原因

- 生成对抗性网络

  提供了⼀种合成数据的⽅法，潜在的统计机制是检查真实和虚假数据是否相同的测试  

    

    

  

#### 3.与环境互动

监督、无监督学习都是在算法与环境断开后进⾏的，被称为**离线学习**（offline learning）。 

  

  

#### 4.强化学习

（1）在强化学习问题中，agent在⼀系列的时间步骤上与环境交互。在每个特定时间点，agent从环境接收⼀些观 察（observation），并且必须选择⼀个动作（action），然后通过某种机制（有时称为执⾏器）将其传输回环境， 最后agent从环境中获得奖励（reward）。此后新⼀轮循环开始，agent接收后续观察，并选择后续操作，依此 类推。

强化学习的⽬标是产⽣⼀个好的**策略**（policy）。强 化学习agent选择的**“动作”受策略控制**，即⼀个从环境观察映射到⾏动的功能。  



（2）**可以将任何监督学习问题转化为强化学习问题**（设置的奖励的原始监督学习问题的损失函数一致）。强化学习还可以解决许多监督学习⽆法解决的问题（如并不告诉agent每个观测的最优动作，而只是给其相应的奖励）

  

（3）当环境可被完全观察到时，我们将强化学习问题称为⻢尔可夫决策过程（markov decision process）。当状 态不依赖于之前的操作时，我们称该问题为上下⽂赌博机（contextual bandit problem）。当没有状态，只有 ⼀组最初未知回报的可⽤动作时，这个问题就是经典的多臂赌博机（multi-armed bandit problem）。

  

​    

  

#### 深度学习起源  



#####   神经网络中的关键原则

• 线性和⾮线性处理单元的交替，通常称为层（layers）。 

• 使⽤链式规则（也称为反向传播（backpropagation））⼀次性调整⽹络中的全部参数。

​    

机器学习和统计的关注点从（⼴义的）线性模型和核⽅法转 移到了深度神经⽹络的**原因**

（1）算⼒的增⻓速度已经超过了现有数据的增⻓速度。

（2）计算预算的增 加，能够花费更多时间来优化这些参数

  

##### 深度学习特点

（1）深度学习是“深度”的，模型 学习了许多“层”的转换，每⼀层提供⼀个层次的表⽰。靠近输⼊的层可以表⽰数据的低级细节，⽽ 接近分类输出的层可以表⽰⽤于区分的更抽象的概念。深度学习可以称为“**多级表⽰学习**”

（2）深度学习⽅法中最显著的共同点是使⽤**端到端训练**。

```
Canny边缘检测器 [Canny,1987] 和SIFT特征提取器 [Lowe, 2004] 作为将图像映射到特征向量的算法
```

（3）不仅取代了传统学习管道末端的浅层模型，⽽且还取代了劳动密集型的 特征⼯程过程

（4）从参数统计描述到完全⾮参数模型的转变（借助于相关 偏微分⽅程的数值模拟，⽽不是⽤⼿来求解电⼦⾏为的参数近似）

  

  

  

### pytorch

  

####   数据操作

（1）具有⼀个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）；具有两个轴以上的张量没有特殊的数学名称。

（2）torch.cat()将张量连结

（3）sum()将产生**单元素张量**

（4）广播机制

（5）索引和切片

（6）节省内存

可以使⽤切⽚表⽰法将操作的结果分配给先前分配的数组

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))


>>>id(Z): 140316199714544
>>>id(Z): 140316199714544

或者类似于X+=Y
```

（7）转换为其他Python对象

```python
A = X.numpy()   #由tensor转换为ndarray
B = torch.tensor(A)  #由ndarray转换为tensor
```

将**⼤⼩为1的张量**转换为Python标量可以调⽤item函数或Python的内置函数。

```python
a=torch.tensor([3.5])
a.item(),float(a),int(a)
```

  

  

#### 数据预处理

  

##### os.path.join()

用于路径拼接文件路径，可以传入多个路径

```
"./"：代表目前所在的目录

"../"：代表上一层目录

以"/"开头：代表根目录
```

```python
import os

#不存在以‘’/’开始的参数，则函数会自动加上
>>> print(os.path.join('path','abc','yyy'))
path\abc\yyy

# 存在以‘’/’’开始的参数，从最后一个以”/”开头的参数开始拼接，之前的参数全部丢弃。
>>> print('1',os.path.join('/aaa','bbb','/ccc.txt'))
1 /ccc.txt

# 同时存在以‘./’与’/’开始的参数，以‘’/’为主，从最后一个以”/”开头的参数开始拼接，之前的参数全部丢弃。
>>> print('2',os.path.join('/aaa','./bbb','ccc.txt'))
2 /aaa\./bbb\ccc.txt

#只存在以‘’./’开始的参数,会从”./”开头的参数的上一个参数开始拼接。
>>> print('2',os.path.join('aaa','./bbb','ccc.txt'))
2 aaa\./bbb\ccc.txt
```



实际运行

```python
>>>os.path.join('1','data','2')
'1\\data\\2'

原因：在程序中，字符串中的“\\”主要是为了转义，“\\”转义后被理解为“\”,“\”才能够被操作系统文件系统所理解，比如用字符串表示上述路径：“F:\\Office\\Trunk\\__Out\\Pro Debug\\Bin\\OfficeIn可以fo.dll”，同理，如果想要表示“\\”，可以写作“\\\\”。
```



（1）数据中的**NA**表示缺失，**NaN**：不是有效数字



###### 在上一级目录创建.csv文件并使用pandas进行读取后输出

```python
import os
os.makedirs(os.path.join('..','data'),exist_ok=True)
data_file=os.path.join('..','data','house_tiny.csv')
with open(data_file,'w') as f:
    f.write("NumRooms,Alley,Price\n")   #列名（表头）
    f.write("NA,PAve,127500\n")
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')
```

```python
import pandas as pd
data=pd.read_csv(data_file)
print(data)
```



###### 处理缺失值

典型的⽅法包**括插值法**和**删除法**，其中插值法⽤⼀个替 代值弥补缺失值，⽽删除法则直接忽略缺失值。



插值法：通过位置索引iloc，我们将data分成inputs和outputs，其中前者为data的前两列，⽽后者为data的最 后⼀列。对于inputs中缺少的数值，我们⽤同⼀列的**均值**替换“NaN”项。

```
inputs,outputs=data.iloc[:,0:2],data.iloc[:,2]
inputs=inputs.fillna(inputs.mean())
print(inputs)
```





对于inputs中的类别值或离散值，我们将“NaN”视为⼀个类别。pandas可以⾃动将此列转换为两列，将***_nan的值设置为0，另一列为1**

```python
inputs=pd.get_dummies(inputs,dummy_na=True)
print(inputs)
```



###### 转换为张量

```
X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
X, y
```

  

  

####   线性代数

（1）⼤量⽂献认为**列向量**是向量的默认⽅向,在表⽰表格数据集的矩阵中，将每个**数据样本**作为矩阵中的**⾏向量**更为常⻅

（2）可以通过调⽤Python的内置**len()**函数来访问张量的⻓度。

（3）区分张量和向量的维度数。张量的维度⽤来表⽰张量具有的轴数

（4）矩阵的转置，⽤a ⊤来表⽰矩阵的转置

```python
A.T
```

```python
B = A.clone() # 通过分配新内存，将A的⼀个副本分配给B
```

##### （5）两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号⊙）

```
A*B
```

##### （6）点积，又称数量积。对应元素相乘再相加(给定两个向量x, y ∈ R d，它 们的点积（dot product）x ⊤y )

```python
#4*1 和4*1进行点积  =>1*4 和4*1   =>1*1
x, y, torch.dot(x, y)

#等价于torch.sum(x*y)
```

（11）矩阵-矩阵乘法

这⾥的A是⼀个5⾏4列的矩阵，B是⼀个4⾏3列的矩阵。 两者相乘后，我们得到了⼀个5⾏3列的矩阵。

```
torch.mm(A,B)
```

（7）张量**乘以或加上**⼀个标量不会改变张量的形状，其中张量的**每个元素**都将与标量相加或相乘。

##### （8）降维

默认情况下，调⽤求和函数会沿所有的轴降低张量的维度，使它变为⼀个标量。还可以指定张量**沿哪⼀ 个轴**来通过求和降低维度。

```python
A=torch.tensor([[1,2,3],
               [4,5,6],
               [7,8,9]])
```

```python
#按行对每列进行求和,输⼊轴0的维数在输出形状中消失。
A_sum_axis0 = A.sum(axis=0)
tensor([12, 15, 18])

A_sum_axis0.shape
torch.Size([3])
```

```python
#按列对每行进行求和,输⼊轴1的维数在输出形状中消失。
A_sum_axis1 = A.sum(axis=1)
tensor([ 6, 15, 24])

A_sum_axis1.shape
torch.Size([3])
```

```python
#沿着⾏和列对矩阵求和，等价于对矩阵的所有元素进⾏求和。
A.sum(axis=[0, 1]) # SameasA.sum()
tensor(45)
```

（9）均值

```python
A.mean(), A.sum() / A.numel()
```

均值计算也可降维

```python
A.mean(axis=0)

tensor([4., 5., 6.])
```

（10）非降维求和

eg:计算总和或均值时保持轴数

```
sum_A = A.sum(axis=1, keepdims=True)
tensor([[ 6.],
        [15.],
        [24.]])
        
sum_A.shape
torch.Size([3, 1])
```

可通过广播

```
A/sum_A

tensor([[0.1667, 0.3333, 0.5000],
        [0.2667, 0.3333, 0.4000],
        [0.2917, 0.3333, 0.3750]])
```

  

cumsum不会沿任何轴降低输入张量的维度

```
A.cumsum(axis=0)

tensor([[ 1.,  2.,  3.],
        [ 5.,  7.,  9.],
        [12., 15., 18.]])
```

（12）范数

⼀个向量的范数告诉我们⼀个**向量有多⼤**（size）。

- 性质一：如果我们按常数因⼦α缩放向量的所有元素，其范数也会按相同常数因⼦的绝对值缩放：f(αx) = |α|f(x).

- 性质二：的三⻆不等式：f(x + y) ≤ f(x) + f(y).

- 性质三：范数必须是⾮负的（范数最⼩为0，当且仅 当向量全由0组成。）：f(x) ≥ 0

- L2范数是向量元素平⽅和的平⽅根（在L2范数中常常省略下标2，也就是说∥x∥等同于∥x∥2。）:

  ![image-20221021210556030](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221021210556030.png?raw=true)

```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)

tensor(5.)
```

- L1范数，将绝对值函数和按元素求和组合起来

```
torch.abs(u).sum()

tensor(7.)
```

- ，矩阵X ∈ R m×n的Frobenius范数（Frobenius norm）是矩阵元素平⽅和的平⽅根：

  ![image-20221021211031105](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221021211031105.png?raw=true)

```
torch.norm(torch.ones((4, 9))

tensor(6.)
```



##### 范数和目标

如⽤向量表⽰物品（如单词、产品或新闻⽂章），以便最⼩化相似项⽬之间的距离,最⼤化不同项⽬之间 的距离。⽬标通常被表达为范数。

  





### DAY 3



  

##### 微积分



拟合模型的任务分解为两个关键问题：

- 优化（optimization）：⽤模型拟合**观测数据**的过程； 

- 泛化（generalization）：数学原理和实践者的智慧，能够指导我们⽣成出有效性超出⽤于训练的数据集 本⾝的模型



（1）通常选择对于 模型参数**可微**的损失函数。简⽽⾔之，对于每个参数，如果我们把这个参数增加或减少⼀个⽆穷⼩的量，我 们可以知道损失会以多快的**速度**增加或减少。导数f ′ (x)解释为f(x)相对于x的瞬时（instantaneous）变化率。所谓的瞬时变化率是基于x中的变化h，**且h接近0。**



###### 导数和微分

- ##### 可微必可导，可导必可微（互为充分必要条件），取微分相当于画了一个线性函数，在某点附近能较好的逼近函数。取导数是给出该线性函数的斜率。

- ##### 对于多元函数，可微可以推出可导（即在各个方向都可导），**但任何方向偏导数存在，函数也不一定可微**（因为其他方向的偏导未知）。



###### 导数



```python
%matplotlib inline
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l


def f(x):
    return 3*x**2-4*x
    
def numerical_lim(f,x,h):
    return (f(x+h)-f(x))/h
    
    
    
h=0.1
for i in range(10):
    print(f'h={h:.5f},numerical limit={numerical_lim(f,1,h):.5f}')
    h*=0.1
```

```
h=0.10000,numerical limit=2.30000
h=0.01000,numerical limit=2.03000
h=0.00100,numerical limit=2.00300
h=0.00010,numerical limit=2.00030
h=0.00001,numerical limit=2.00003
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
```

  

##### 可视化



```
plot()
fmt = '#color#linestyle#marker'即代表各类参数。

eg:
fmts=('-', 'm--', 'g-.', 'r:')
分别是：实线,品色实线'-'点、绿色实线点、红色点虚线
```



```
#返回维度数目，如二维数组都是2，一维数组都是1
.ndim
```





##### 偏导数

推广到多元函数，对各个方向上的导数



##### 梯度

连结⼀个多元函数对其所有变量的偏导数，以得到该函数的梯度（gradient）**向量**



##### 矩阵求导



##### （1）标量对列向量的导数是行向量,行向量的每个元素就是标量的对每个元素的导数，得到的行向量表示的方向就是梯度（标量为常数的时候得到元素全为0的向量）。

##### （2）列向量对标量的导数是列向量

##### （3）向量对向量求导，对应元素对应求导





![image-20221022192651448](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192651448.png)



1.

![image-20221022192311613](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192311613.png)



2.![image-20221022192331544](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192331544.png)

3.

![image-20221022192430282](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192430282.png)



4.

![image-20221022192451673](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192451673.png)





![image-20221022192708865](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192708865.png)



1.

![image-20221022192723804](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192723804.png)

2.

![image-20221022192746187](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192746187.png)

3.

![image-20221022192800367](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192800367.png)

4.

![image-20221022192909460](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192909460.png)

5.

![image-20221022192925612](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192925612.png)

##### 

##### 线性回归



（1）**仿射变换**的特点是通过 加权和对特征进⾏线性变换（linear transformation），并通过偏置项来进⾏平移（translation）。

（2）机器学习通常使⽤的是**⾼维数据集**，建模时采⽤**线性代数**表⽰法会⽐较⽅便。





##### 损失函数

损失函数（loss function） 能够量化⽬标的**实际值**与**预测值**之间的差距。通常我们会选择**⾮负数**作为损失，且数值越⼩表⽰损失越⼩， 完美预测时的损失为0（常用平方损失函数）

![image-20221023211455159](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023211455159.png)

eg预测值的时候：

```
预测试和真实标签值相减然后平方外面再乘1/2  #1/2是为了求导后系数化为1
```

使用平方损失函数的时候：由于平⽅误差函数中的**⼆次⽅项**，估计值yˆ (i)和观测值y (i)之间较⼤的**差异将导致更⼤的损失**。为了**度量模型 在整个数据集上的质**量，我们需计算在训练集**n个样本上的损失均值**（也等价于求和）。

![image-20221023211506146](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023211506146.png)

在**训练模型**的时候，，我们希望寻找⼀组参数（w∗ , b∗），这组参数能**最⼩化在所有训练样本上的总损失**。如下式： w ∗ , b∗ = argmin w,b L(w, b).





### DAY 4



##### 反向传播

![image-20221023142503416](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023142503416.png)



![image-20221023142843935](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023142843935.png)



计算复杂度O(n),内存复杂度O(n)需要存储正向累积中的中间结果





##### 自动求导

深度学习大部分时候是对**标量**求导



```
x=torch.arange(4.0)
x.requires_grad_(True)

x.grad.zero_() #清0梯度
```

对于

```
y=x*x   #x是一个向量

#通常把y转换成一个标量再来求导
y.sum().backward()  #等价y.backward(torch.ones(len(x)))
x.grad
```

而对于

```
y=2*torch.dot(x,x)  #本身就是标量
y.backward()
```



```
y=x*x
u=y.detach()  #转换为常数
z=u*x   

z.sum().backward()
x.grad==u
```





##### 解析解

可 以⽤⼀个公式简单地表达出来，这类解叫作解析解。

如线性回归就是||y-Xw||的平方,此时损失平⾯上只有 ⼀个临界点，这个临界点对应于整个区域的损失极⼩点。将损失关于**w的导数设为0**，得到解析解：

![image-20221023212858426](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023212858426.png)



##### 随机梯度下降

（1）梯度下降：不断地在**损失函数递减的⽅向上更新**参数来降低误差。

（2）可以计算损失函数（数据集中**所有样本的损失均值**）关于模型参数的导数（在这⾥也可 以称为梯度）

（3）每次迭代的时候，随级抽取小批量的训练样本，计算**该批量样本的损失均值**关于**模型参数的导数**（梯度）；最后将**梯度乘以⼀个预先确定的正数η**，并从 当前参数的值中**减掉**。

![image-20221023213719650](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023213719650.png)



##### 总结：

（1）初始化模型参数（如随机初始化）

（2）从数据集中**随机**抽取**⼩批量**样 本且**在负梯度的⽅向上更新参数**，并**不断迭代**这⼀步骤。

对于平⽅损失和仿射变换：

![image-20221023214032986](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023214032986.png)

（3）算法会使得损失向最⼩值**缓慢收敛**，但却不能在有限的步数内⾮常精确地达到最⼩值。

（4）深度神经⽹络这样复杂的模型来说， 损失平⾯上通常包含多个最⼩值。



##### 泛化：找到⼀组参数，这组参数能够在我们从未⻅过的数据上实现较低的 损失，这⼀挑战被称为泛化（generalization）。





##### 使用模型进行预测







##### 正态分布和平方损失

改变均值会产⽣沿x轴的偏移，**增加⽅差**将会分散分布、降低其峰值。

**均⽅误差损失函数**（简称均⽅损失），在⾼斯 噪声的假设下，最⼩化均⽅误差等价于对线性模型的极⼤似然估计。





##### 神经网络图

（1）输⼊为x1, . . . , xd，因此输⼊层中的输⼊数（或称为**特征维度**，feature dimensionality）为**d**。

（2）将线性回归模型视为仅由**单个⼈⼯神经元**组成的神经⽹络，或 称为**单层神经⽹络**。

（3）**每个输⼊都与每个输出相连**,则称称为**全连接层**（fully-connected layer）或称为**稠密层**（dense layer）。

  

  

  



### DAY 5





（1）在深度学习框架中实现的内置迭代器效率要⾼得多，它可以处理存储在⽂件中 的数据和数据流提供的数据。



###### 初始化模型参数

eg：

```
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

```



##### reshape((-1,1))

即：转化为1列

```
arr.shape    # (a,b)
arr.reshape(m,-1) #改变维度为m行、d列 （-1表示列数自动计算，d= a*b /m ）
arr.reshape(-1,m) #改变维度为d行、m列 （-1表示行数自动计算，d= a*b /m ）
```

##### 将向量转为1列，行数自行计算后维度仍然为1





##### Python中的with

with 语句**适用于对资源进行访问的场合**，确保不管使用过程中是否发生异常都会**执行必要的“清理”操作，释放资源**，比如文件使用后自动关闭／线程中锁的自动获取和释放等。





eg：

对于

```python
file = open("１.txt")
data = file.read()
file.close()
```

可改进

```python
try:
    f = open('xxx')
except:
    print('fail to open')
    exit(-1)
try:
    do something
except:
    do something
finally:
    f.close()
```



使用with

（１）紧跟with后面的语句被求值后，返回对象的“–enter–()”方法被调用，这个方法的返回值将被**赋值给as**后面的变量；
（２）当with后面的代码块全部被执行完之后，将调用前面返回对象的“–exit–()”方法。

```python
with open("１.txt") as file:
    data = file.read()
```



##### requires_grad参数，如果设置为True，则反向传播时，该tensor就会自动求导,默认False。

**with torch.no_grad的作用**
在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。即使中间有张量设置requires_grad为true





###### 定义损失函数

定义损失函数记得将真实值转换为预测值的形状。



##### 一般把损失均值的求取提出来，放到梯度下降那部分



##### 

### softmax回归



如果**类别间有⼀些⾃然顺序**，⽐如说我们试图预测{婴⼉, ⼉童, ⻘少年, ⻘年⼈, 中年⼈, ⽼年⼈}，那么将这个问题转变为**回归问题**，并且保留这种格式是 有意义的。



##### 独热编码（one-hot encoding）

（1）⼀种表⽰**分类数据**的简单⽅法

（2）独热编码是⼀个**向量**，它的**分量和类别**⼀样多。类别**对应的分量设置为1**，其他所有分量设置为0。





###### 网络架构

多个**输入**（多个**特征**）多个输出，每个输出对应一个类别，需要和输出一样多的仿射函数（affine function）

```
eg：
3个类别，每个样本有4个特征，则总的需要12个参数
```



softmax回归也是⼀个单层神经⽹络。 由于计算**每个**输出o1、o2和o3**取决于所有输⼊**x1、x2、x3和x4，所以softmax回归的**输出层也是全连接层**。



（1）设置阈值，如选择输出值中减去一个数>=阈值的输出值的类别作为预测类别（主要是关心**输出**对**正确类别**的**置信度**大不大）

（2）需要注意，不能将**未规范化**的预测o直接视作我们感兴趣的输出，必须保证输出都是非负且总和为1。设计⽬标函数来激励模型精准地估计**概率**。例如，在分类器输出**0.5**的所有样本中，我们希望这些样本是刚好有⼀ 半实际上属于预测的类别。这个属性叫做**校准**（calibration）。



##### softmax运算

将未规范化的预测变换为**⾮负数**并且**总和为1**，同时让模型保持**可导**的性质。

（1）对每个未规范化的**预测**求幂，这样可以确保输出⾮负。

（2）为了确保最终输出的概率值总和为1， 我们再让**每个**求幂后的结果除以它们的**总和**。（使用softmax操作子得到每个类的预测置信度）

![image-20221025004442747](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025004442747.png)

##### exp(n)

返回e的n次方



（3）对于所有的输出总有0 ≤ yˆj ≤ 1

（4）softmax运算**不会改变未规范化的预测o之间的⼤⼩次序**，只会确定分配给每个类别的概率

（5）尽管softmax是⼀个**⾮线性函数**，但softmax回归的**输出**仍然**由输⼊特征的仿射变换**决定。因此，softmax回 归是⼀个线性模型（linear model）。



###### 损失函数

##### 交叉熵损失（cross-entropy loss）

![image-20221025004048139](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025004048139.png)

##### • 交叉熵是⼀个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的⽐特数。这里使用交叉熵来衡量预测和标号的差异，并将这种差异作为损失





##### softmax的导数

###### 交叉熵带入softmax运算



![image-20221025004520668](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025004520668.png)



![image-20221025004555730](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025004555730.png)

导数是我们s**oftmax模型分配的概率**与**实际发⽣的情况**（由独热标签向量表⽰）之间的差异。





##### 熵

分布p的熵

![image-20221025005121138](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025005121138.png)





### 损失函数

##### L2 Loss 平方损失

主要特性：当预测值和真实值隔得比较远的时候，权重更新的比较多，**靠近**真实值的时候，**梯度的绝对值会变小**，这样对于权重的更新也比较小。，梯度的平滑性较好。



##### L1 Loss绝对值损失

预测值和真实值相减的绝对值

主要特性：当预测值和真实值隔的比较远的时候，梯度永远是常数，对权重的更新不会特别大，可以带来稳定性上的好处。

**缺点**，0点处不可导，平滑性不太好。优化到后期，真实值和预测值相接近的时候，权重可能不太稳定。



##### 当真实值和预测值相差的绝对值大于1的时候使用L1loss





![image-20221025010701928](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025010701928.png)

  

  

#### DAY 6

  

###### 二维索引

```
y = torch.tensor([0, 2])

y_hat = torch.tensor([[0.9, 0.3, 0.6], [0.7, 0.2, 8]])


#从y这个数组里拿出索引为0和1所代表的值来作为y_hat数组的索引
y_hat[[0,1],y]
```

  

### 分类精度

分类精度即正确预测数量与总预测数量之⽐，精度通常是我们最关⼼的**性能衡量标准**。

```
def accuracy(y_hat, y): #@save
#"""计算预测正确的数量"""
	if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:			#判断y_hat是否是一个矩阵
		y_hat = y_hat.argmax(axis=1)	 	#是矩阵的话则按列对每行求最大预测值 的下标
	cmp = y_hat.type(y.dtype) == y			
	return float(cmp.type(y.dtype).sum())	#把比较结果转为y的数据类型并求和

```

代码解析：

获取到**每个样本最大的预测值**的**下标**后和样本对应的**真实类别的下标**进行比较，将比较的结果由**bool**类型转换为和真实类别的下标所对应的数据类型（False->0,True->1），这样为**1的就表示该样本预测成功**，为**0的话就表示失败**，然后**求和**就能得到**预测成功的数目**。用该数目除以总的样本数就得到分类精度。

  

  

##### .step()实现权重更新

  

###### isinstance函数

该函数的第一个参数是一个对象，第二个参数是一个类型名或多个类型名组成的[元组](https://so.csdn.net/so/search?q=元组&spm=1001.2101.3001.7020)，只要该对象是其中一个类型（当然也只能是一种类型）便返回True,否则False

  

###### assert [条件] 

条件不满足则抛掷断点。

  

  

  

### 多层感知机

  

（1）在线性回归和softmax回归中模型都是凡是仿射变换（带有偏置项的线性变换），如果我们的标签通过仿射变换后确实与我们的输⼊数据相关也能接受。

（2）线性模型，较为单调。任何**特征的增⼤**都会导致模型**输出的增⼤**（如果对应的**权重为正**），或者导致模输出的减⼩（如果对应的权重为负）

（3）例如：任何像素的重要性都以复杂的⽅式取决于该像素的上下⽂（周围像素的值）这时难以通过简单预处理、线性模型来描述。

  

##### 隐藏层

###### 在⽹络中加⼊⼀个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。

（1）将多层**全连接层**堆叠在一起，每一层的输入都是上一层的输出。把前L-1层看作**表示**，把最后一层看作**线性预测器**。这种架构被称为**多层感知机**（multilayer perceptron），通常缩写为**MLP**。（开销可能过大）

需要注意：仿射函数的仿射函数本⾝就是仿射函数，堆叠多层都等价于一层（退化为线性模型）。

eg：

一个三层的MLP模型，4个输入，3个输出，中间隐藏层包含5个隐藏单元。这里**输入层不涉及计算**，因此网络输出只要实现隐藏层和输出层的计算。多层感知机中的**层数为2**。

  

（2）为了发挥多层架构的潜⼒，我们还需要⼀个额外的关键要素：在**仿射变换之后**对每个隐藏单元应⽤**⾮线性的 激活函数**（activation function）σ。激活函数的输出（例如，σ(·)）被称为活性值（activations）。

  

（3）多层感知机可以通过隐藏神经元，捕捉到输⼊之间复杂的相互作⽤。层感知机是**通⽤近似器**。即使是⽹络只有⼀个隐藏层，给定⾜够的神经元和正确的权重，我们可以对任意函数建模，尽管实际 中学习该函数是很困难的。事实上应该使用更深（而不是更广）的网络来逼近函数。

  

  

  

#### 激活函数

激活函数（activation function）**通过计算加权和并加上偏置**来**确定神经元是否应该被激活**，它们将输⼊信号转换为输出的可微运算。⼤多数激活函数都是**⾮线性**的。

  

###### Relu函数

修正线性单元（Rectified linear unit，ReLU）：给定元素x，ReLU函数被定义为**该元素与0的最⼤值**：

特性：

（1）实现简单，同时在各种预测任务中表现良好。

（2）ReLU函数通过将相应的活性值设为0，仅**保留正元素并丢弃所有负元素**（分段的）。

![image-20221025173239499](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025173239499.png)

（3）当**输⼊为负**时，ReLU函数的**导数为0**，⽽当**输⼊为正**时，ReLU函数的**导数为1**。注意，当输⼊值**精确等于0**时， ReLU函数**不可导**。在此时，我们**默认使⽤左侧的导数**，即当输⼊为0时导数为0。要么让参数消失，要么让参数通过。这使得优化表现得更好，并 且ReLU**减轻了**困扰以往神经⽹络的**梯度消失**问题

![image-20221025173246330](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025173246330.png)

  

###### sigmoid函数

sigmoid函数将输⼊变换为**区间(0, 1)上的输出**。因此，sigmoid通常称为挤压 函数（squashing function）

  

特性：

（1）基于梯度的学习时，sigmoid函数是⼀个⾃然的选择，因为它是⼀个**平滑**的、**可微**的阈值单元近似。

（2）将输出视作**⼆元分类**问题的概率时，sigmoid仍然被⼴泛⽤作输出单元上的激活函数（你可以将sigmoid视为softmax的特例）。

（3）输⼊接近0时，sigmoid函数接近线性变换

![image-20221025174007745](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025174007745.png)

（4）当输⼊为0时，sigmoid函数的导数**达到**最⼤值0.25；⽽输⼊在任⼀ ⽅向上越远离0点时，导数越**接近**0。

![image-20221025174030227](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025174030227.png)

  

  

​    



###### tanh函数

tanh(双曲正切)函数也能将其输⼊压缩转换到区间(-1, 1)上

  

特性：

（1）当输⼊在0附近时，tanh函数**接近线性变换**

（2）tanh函数关**于坐标系原点中⼼对称**

![image-20221025174308861](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025174308861.png)

（3）当输⼊接近0时，tanh函数的导数**接近**最⼤值1。输⼊在任⼀⽅向上越远离0点，导数越**接近**0。

![image-20221025174620454](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025174620454.png)

 

  

  

  

### DAY 7



对于多层感知机的层数：通常，我们选择**2的若⼲次幂**作为层的**宽度**。因为内存在硬件中的分配和寻址⽅式，这么做往往可以在计 算上更⾼效。



##### 过拟合、正则化

（1）将模型在训练数据上拟合的⽐在潜在分布中更接近的现象称为过拟合（overfitting），⽤于对抗过拟合的技术 称为正则化（regularization）。



（2）训练误差（training error）是指，模型在训 练数据集上计算得到的误差。泛化误差（generalization error）是指，模型应⽤在同样从原始样本的分布中 抽取的⽆限多数据样本时，模型误差的期望。



##### 独立同分布

在监督学习情景中，我们假设**训练数据**和**测试数据**都是从**相同的分布中独⽴提取**的。这通常被称为**独⽴同分布假设**（i.i.d. assumption），这意味着对数据进⾏采样的过程没有 进⾏“记忆”。换句话说，抽取的第2个样本和第3个样本的相关性，并不⽐抽取的第2个样本和第200万个样本 的相关性更强。



##### 模型复杂性

（1）总述：有**简单的模型**和**⼤量的数据**时，我们期望泛化误差与训练误差相近。当我们有**更复杂的模型**和**更少的 样本**时，我们预计训练误差会下降，但泛化误差会增⼤。



##### （2）⼏个倾向于影响模型泛化的因素：

- ###### 可调整参数的数量。当可调整参数的数量（有时称为⾃由度）很⼤时，模型往往更容易过拟合。 

- ###### 参数采⽤的值。当权重的取值范围较⼤时，模型可能更容易过拟合。 

- ###### 训练样本的数量。即使你的模型很简单，也很容易过拟合只包含⼀两个样本的数据集。⽽过拟合⼀个有 数百万个样本的数据集则需要⼀个极其灵活的模型。



（3）⾼阶多项式函数⽐低阶多项式函数复杂得多。⾼阶多项式的参数较多，模型函数的选择范围较⼴。因此在固 定训练数据集的情况下，⾼阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。





**验证集**

为了确定候选模型中的最佳模型。

- 原则上，在我们确定所有的超参数之前，我们不希望⽤到**测试集**（避免在**测试集**上过拟合的风险）。训练集上的过拟合可以通过在测试集上的评估判断。
- 通常将数据集分为三分：训练集、测试集、验证集



##### k折交叉验证

起因：构成验证集数据不够；

解决方法：将原始数据集**分成K个不重叠的子集**。执行**k次模型训练和验证**。**每次**在**k-1个子集上训练**，在剩下的**1个子集上验证**。最后通过**对k次实验结果取均值**来估计训练和验证误差。





##### 数据集大小

（1）训练数据集中的**样本越少**，我们就**越有可能（且更严重地）过拟合**。

（2）如果没有⾜够的数据，简单的模型可能更有⽤。对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。

（3）深度学习⽬前的⽣机要归 功于廉价存储、互联设备以及数字化经济带来的海量数据集。



##### 权重衰减（正则化技术之一）

（1）**限制特征的数量**是缓解过拟合的⼀种常⽤技术。

（2）在**训练参数化**机器学习模型时，权重衰减（weight decay）的使用较为广泛，它通常也被 称为**L2正则化**。

（3）通过**函数与零的距离**来衡量函数的**复杂度**

eg：对于线性回归，可以向通过的**权重向量**的某个范数来**度量其复杂性**。要保证权重向量⽐较⼩，最常⽤⽅法是**将其范数作为惩罚项**加到**最⼩化损失**的问题中。将原来的训练⽬标最⼩化训 练标签上的预测损失，调整为**最⼩化预测损失和惩罚项之和。**

（4）通过**正则化常数λ**（非负，超参数）来描述这种权衡**新加了惩罚项的损失**

（5）使⽤L2范数的⼀个原因是它对**权重向量的⼤分量施加了巨⼤的惩罚**。这使得我们的学习算法偏向于在⼤量特征上**均匀分布权重**的模型。在实践中，这 可能使它们对单个变量中的观测误差更为稳定。相⽐之下，L1惩罚会导致模型**将权重集中在⼀⼩部分特征**上， ⽽将**其他权重清除为零**，这称为**特征选择**（feature selection）。

​                 



加入1/2是为了方便计算梯度

![image-20221027011115949](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221027011115949.png)



​                         



![image-20221027011105588](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221027011105588.png)



总结：

- 根据**估计值与观测值之间的差异**来更新w。然⽽，我们同时也在试图将w的**⼤⼩缩⼩到零**。
- 较⼩的λ值对应较少 约束的w，⽽较⼤的λ值对w的约束更⼤。
- 通常，⽹络**输出层**的**偏置项**不会被正则化。

