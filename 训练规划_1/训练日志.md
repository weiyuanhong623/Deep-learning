### DAY1



##### 完成实验环境的搭建





---

---

网页学习部分





##### 什么是神经网络

人工神经网络是受到人类大脑结构的启发而创造出来的

训练深度神经网络的过程就叫做深度学习





##### 将数据输入神经网络

（1）对于图像，计算机要存储三个独立的矩阵（三个**通道**）分别对应红、绿、蓝，矩阵里面的**数值**就对应于图像的红绿蓝**强度值**

（2）一般将3个矩阵转化成1个**向量**x（向量可以理解成1 * n或n * 1的数组，前者为行向量，后者为列向量

![image-20221020183623969](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221020183623969.png)

（3）每一个输入到神经网络的数据都被叫做一个特征，由二维矩阵变成的**向量**就叫做**特征向量**，向量的长度就是该向量的**维度**。神经网络接收这个特征向量x作为输入，并进行预测，然后给出相应的结果。

```
eg:
对于一张20*20*20像素的图像 在计算机中有三个二维矩阵进行存储，转为向量后规格为：1*(20*20*3)或(20*20*3)*1，该向量维度就是1200，这张图像就有1200个特征
```





**总结**：不同应用场景所输入的数据在计算机中都有对应的数字表示形式，将其化成一个特征向量，然后将其输入到神经网络中





##### 神经网络如何进行预测



向量相乘（对应元素相乘再相加）

```
dot(w,x)
```



场景：对某一事件进行预测

输入：影响事件发展结果因素（特征），并对特征赋予不同的**权重**，设置不同输出对应的事件结果

输出：预测结果





##### 激活函数——sigmoid

![image-20221020185827218](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221020185827218.png)

z越大越接近1，越小越接近0







如用于分类： 若输出经过sigmoid的隐藏层后输出为0.8，说明有80%的概率是目标类别





---

---



##### 《drive into deeplearning》学习部分





任⼀调整参数后 的程序，我们称为**模型**（model）。

通过操作参数⽽⽣成的所有不同程序（输⼊-输出映射）的集合称为“**模 型族**”

使⽤数据集来选择参数的元程序被称为**学习算法**（learning algorithm）。





#### 机器学习的一些关键组件



1.数据

（1）数据由**样本**组成，每个样本由被称为**特征**的属性组成，机器学习**模型**会根据这些属性进⾏预测。如在**监督学习问题**中，要预测的是⼀个特殊的属 性，它被称为**标签**（label，或⽬标（target））。

（2）与传统机器学习⽅法相⽐，深度学习的⼀个主要优势是可以处理**不同⻓度的数据**。

（3）更多的数据意味着可以**减少对预先设想假设的依赖**（数据集的由⼩变⼤为现代深度学习的成功奠定基础）

（4）要**筛除无效数据**以及注意数据集的**均衡问题**



2.模型

深度学习与经典⽅法的区别主要在于：前者关注的功能强⼤的模型，这些模型由神经⽹络错综复杂的交织在⼀起，包含层层数据转换，因此被称为深度学习（deep learning）。



3.目标函数

（1）定义模型的**优劣程度的度量**，这个度量在⼤多 数情况是“**可优化**”的，我们称之为⽬标函数（objective function），有时被称为损失函数。

（2）通常，损失函数是**根据模型参数**定义的，并取决于数据集。最常⻅的损失函数是平⽅误差（squared error）

（3）总的流程是：通过**最⼩化总损失**来学习 模型**参数**的**最佳值**。这里是获取**损失**。



4.优化算法

（1）**搜索出最佳参数**，以最⼩化损失函数（⼤多流⾏的优化算法通常基于⼀种基本⽅法‒梯度下 降）。在每个步骤中，梯度下降法都会检查每个参数，看看如果你仅对该参数 进⾏少量变动，训练集损失会朝哪个⽅向移动。然后，它在可以**减少损失的⽅向**上优化参数。







#### 机器学习的类别

##### 1.监督学习

（1）在“给定输⼊特征”的情况下预测标签。每个“**特征-标签**”对都称为 ⼀个样本（example）。估计给定输⼊特征的标签。

（2）监督学习过程：从已知数据集中随级选取一个子集，给子集中的每个样本获取真实标签（有的样本已有标签；有的需要人工标记），输入的特征和标签构成训练集通过**学习算法**输出**模型**，然后使用测试集特征进行输入，模型的输出作为对输入特征的标签的预测。



###### 监督学习的细分：



##### 回归

本质由输出决定，输出的预测标签取任意数值。⽬标是⽣成⼀个模型，它的预测⾮常接近实际标签值。



##### 分类

在分类中，我们训练⼀个**分类器**，它的输出即为预测的类别。

- 模型预测样本属于哪个类别。最简单的分类问题是只有两类，我们称之为“**⼆元分类**”。

- 有两个以上的类别时，我们把这个问题称为**多元分类**（multiclass classification）问题。
- 寻找层次结构，层次 结构假定在许多类之间存在某种关系。**层次分类**（如对动物进行层次分类）



##### 标记问题

学习预测**不相互排斥的类别**的问题称为**多标签分类**



##### 搜索

模型要输出**有序**的**元素⼦集**。可以先为集合中的每个元素分配相应的**相关性分数**，然后**检索**评级最⾼的元素。



##### 推荐系统

向特定⽤⼾进⾏“个性化” 推荐。推荐系统会为“给定⽤⼾和物品”的匹配性打分，这个“分数”可能是估计 的评级或购买的概率，然后检索得分最⾼的对象集推给用户



### DAY 2

---

---

#####   

##### 《drive into deeplearning》学习部分



##### 序列学习

输⼊和输出都是可变⻓度的序列  

  

- ###### 标记和解析

⽤属性注释⽂本序列（如给一个句子标记实体）

- ###### ⾃动语⾳识别

音频和文本之间没有1：1的对应关系，可能多个样本对应一个单独单词

- ###### ⽂本到语⾳

输出音频多于输入

- ###### 机器翻译

⽽在机器翻译中，颠倒输⼊和输出的顺序⾮常 重要。  

  

  

   

#### 2.无监督学习

数据中不含有“⽬标”的机器学习问题为⽆监督学习（unsupervised learning）。  

  

- 聚类

  如无标签的情况下进行分类

- 主成分分析

  少量的参数来准确地捕捉数据的**线性相关属性**

- 因果关系和概率图模型

  描述观察到的 许多数据的根本原因

- 生成对抗性网络

  提供了⼀种合成数据的⽅法，潜在的统计机制是检查真实和虚假数据是否相同的测试  

    

    

  

#### 3.与环境互动

监督、无监督学习都是在算法与环境断开后进⾏的，被称为**离线学习**（offline learning）。 

  

  

#### 4.强化学习

（1）在强化学习问题中，agent在⼀系列的时间步骤上与环境交互。在每个特定时间点，agent从环境接收⼀些观 察（observation），并且必须选择⼀个动作（action），然后通过某种机制（有时称为执⾏器）将其传输回环境， 最后agent从环境中获得奖励（reward）。此后新⼀轮循环开始，agent接收后续观察，并选择后续操作，依此 类推。

强化学习的⽬标是产⽣⼀个好的**策略**（policy）。强 化学习agent选择的**“动作”受策略控制**，即⼀个从环境观察映射到⾏动的功能。  



（2）**可以将任何监督学习问题转化为强化学习问题**（设置的奖励的原始监督学习问题的损失函数一致）。强化学习还可以解决许多监督学习⽆法解决的问题（如并不告诉agent每个观测的最优动作，而只是给其相应的奖励）

  

（3）当环境可被完全观察到时，我们将强化学习问题称为⻢尔可夫决策过程（markov decision process）。当状 态不依赖于之前的操作时，我们称该问题为上下⽂赌博机（contextual bandit problem）。当没有状态，只有 ⼀组最初未知回报的可⽤动作时，这个问题就是经典的多臂赌博机（multi-armed bandit problem）。

  

​    

  

#### 深度学习起源  



#####   神经网络中的关键原则

• 线性和⾮线性处理单元的交替，通常称为层（layers）。 

• 使⽤链式规则（也称为反向传播（backpropagation））⼀次性调整⽹络中的全部参数。

​    

机器学习和统计的关注点从（⼴义的）线性模型和核⽅法转 移到了深度神经⽹络的**原因**

（1）算⼒的增⻓速度已经超过了现有数据的增⻓速度。

（2）计算预算的增 加，能够花费更多时间来优化这些参数

  

##### 深度学习特点

（1）深度学习是“深度”的，模型 学习了许多“层”的转换，每⼀层提供⼀个层次的表⽰。靠近输⼊的层可以表⽰数据的低级细节，⽽ 接近分类输出的层可以表⽰⽤于区分的更抽象的概念。深度学习可以称为“**多级表⽰学习**”

（2）深度学习⽅法中最显著的共同点是使⽤**端到端训练**。

```
Canny边缘检测器 [Canny,1987] 和SIFT特征提取器 [Lowe, 2004] 作为将图像映射到特征向量的算法
```

（3）不仅取代了传统学习管道末端的浅层模型，⽽且还取代了劳动密集型的 特征⼯程过程

（4）从参数统计描述到完全⾮参数模型的转变（借助于相关 偏微分⽅程的数值模拟，⽽不是⽤⼿来求解电⼦⾏为的参数近似）

  

  

  

### pytorch

  

####   数据操作

（1）具有⼀个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）；具有两个轴以上的张量没有特殊的数学名称。

（2）torch.cat()将张量连结

（3）sum()将产生**单元素张量**

（4）广播机制

（5）索引和切片

（6）节省内存

可以使⽤切⽚表⽰法将操作的结果分配给先前分配的数组

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))


>>>id(Z): 140316199714544
>>>id(Z): 140316199714544

或者类似于X+=Y
```

（7）转换为其他Python对象

```python
A = X.numpy()   #由tensor转换为ndarray
B = torch.tensor(A)  #由ndarray转换为tensor
```

将**⼤⼩为1的张量**转换为Python标量可以调⽤item函数或Python的内置函数。

```python
a=torch.tensor([3.5])
a.item(),float(a),int(a)
```

  

  

#### 数据预处理

  

##### os.path.join()

用于路径拼接文件路径，可以传入多个路径

```
"./"：代表目前所在的目录

"../"：代表上一层目录

以"/"开头：代表根目录
```

```python
import os

#不存在以‘’/’开始的参数，则函数会自动加上
>>> print(os.path.join('path','abc','yyy'))
path\abc\yyy

# 存在以‘’/’’开始的参数，从最后一个以”/”开头的参数开始拼接，之前的参数全部丢弃。
>>> print('1',os.path.join('/aaa','bbb','/ccc.txt'))
1 /ccc.txt

# 同时存在以‘./’与’/’开始的参数，以‘’/’为主，从最后一个以”/”开头的参数开始拼接，之前的参数全部丢弃。
>>> print('2',os.path.join('/aaa','./bbb','ccc.txt'))
2 /aaa\./bbb\ccc.txt

#只存在以‘’./’开始的参数,会从”./”开头的参数的上一个参数开始拼接。
>>> print('2',os.path.join('aaa','./bbb','ccc.txt'))
2 aaa\./bbb\ccc.txt
```



实际运行

```python
>>>os.path.join('1','data','2')
'1\\data\\2'

原因：在程序中，字符串中的“\\”主要是为了转义，“\\”转义后被理解为“\”,“\”才能够被操作系统文件系统所理解，比如用字符串表示上述路径：“F:\\Office\\Trunk\\__Out\\Pro Debug\\Bin\\OfficeIn可以fo.dll”，同理，如果想要表示“\\”，可以写作“\\\\”。
```



（1）数据中的**NA**表示缺失，**NaN**：不是有效数字



###### 在上一级目录创建.csv文件并使用pandas进行读取后输出

```python
import os
os.makedirs(os.path.join('..','data'),exist_ok=True)
data_file=os.path.join('..','data','house_tiny.csv')
with open(data_file,'w') as f:
    f.write("NumRooms,Alley,Price\n")   #列名（表头）
    f.write("NA,PAve,127500\n")
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')
```

```python
import pandas as pd
data=pd.read_csv(data_file)
print(data)
```



###### 处理缺失值

典型的⽅法包**括插值法**和**删除法**，其中插值法⽤⼀个替 代值弥补缺失值，⽽删除法则直接忽略缺失值。



插值法：通过位置索引iloc，我们将data分成inputs和outputs，其中前者为data的前两列，⽽后者为data的最 后⼀列。对于inputs中缺少的数值，我们⽤同⼀列的**均值**替换“NaN”项。

```
inputs,outputs=data.iloc[:,0:2],data.iloc[:,2]
inputs=inputs.fillna(inputs.mean())
print(inputs)
```





对于inputs中的类别值或离散值，我们将“NaN”视为⼀个类别。pandas可以⾃动将此列转换为两列，将***_nan的值设置为0，另一列为1**

```python
inputs=pd.get_dummies(inputs,dummy_na=True)
print(inputs)
```



###### 转换为张量

```
X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
X, y
```

  

  

####   线性代数

（1）⼤量⽂献认为**列向量**是向量的默认⽅向,在表⽰表格数据集的矩阵中，将每个**数据样本**作为矩阵中的**⾏向量**更为常⻅

（2）可以通过调⽤Python的内置**len()**函数来访问张量的⻓度。

（3）区分张量和向量的维度数。张量的维度⽤来表⽰张量具有的轴数

（4）矩阵的转置，⽤a ⊤来表⽰矩阵的转置

```python
A.T
```

```python
B = A.clone() # 通过分配新内存，将A的⼀个副本分配给B
```

##### （5）两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号⊙）

```
A*B
```

##### （6）点积，又称数量积。对应元素相乘再相加(给定两个向量x, y ∈ R d，它 们的点积（dot product）x ⊤y )

```python
#4*1 和4*1进行点积  =>1*4 和4*1   =>1*1
x, y, torch.dot(x, y)

#等价于torch.sum(x*y)
```

（11）矩阵-矩阵乘法

这⾥的A是⼀个5⾏4列的矩阵，B是⼀个4⾏3列的矩阵。 两者相乘后，我们得到了⼀个5⾏3列的矩阵。

```
torch.mm(A,B)
```

（7）张量**乘以或加上**⼀个标量不会改变张量的形状，其中张量的**每个元素**都将与标量相加或相乘。

##### （8）降维

默认情况下，调⽤求和函数会沿所有的轴降低张量的维度，使它变为⼀个标量。还可以指定张量**沿哪⼀ 个轴**来通过求和降低维度。

```python
A=torch.tensor([[1,2,3],
               [4,5,6],
               [7,8,9]])
```

```python
#按行对每列进行求和,输⼊轴0的维数在输出形状中消失。
A_sum_axis0 = A.sum(axis=0)
tensor([12, 15, 18])

A_sum_axis0.shape
torch.Size([3])
```

```python
#按列对每行进行求和,输⼊轴1的维数在输出形状中消失。
A_sum_axis1 = A.sum(axis=1)
tensor([ 6, 15, 24])

A_sum_axis1.shape
torch.Size([3])
```

```python
#沿着⾏和列对矩阵求和，等价于对矩阵的所有元素进⾏求和。
A.sum(axis=[0, 1]) # SameasA.sum()
tensor(45)
```

（9）均值

```python
A.mean(), A.sum() / A.numel()
```

均值计算也可降维

```python
A.mean(axis=0)

tensor([4., 5., 6.])
```

（10）非降维求和

eg:计算总和或均值时保持轴数

```
sum_A = A.sum(axis=1, keepdims=True)
tensor([[ 6.],
        [15.],
        [24.]])
        
sum_A.shape
torch.Size([3, 1])
```

可通过广播

```
A/sum_A

tensor([[0.1667, 0.3333, 0.5000],
        [0.2667, 0.3333, 0.4000],
        [0.2917, 0.3333, 0.3750]])
```

  

cumsum不会沿任何轴降低输入张量的维度

```
A.cumsum(axis=0)

tensor([[ 1.,  2.,  3.],
        [ 5.,  7.,  9.],
        [12., 15., 18.]])
```

（12）范数

⼀个向量的范数告诉我们⼀个**向量有多⼤**（size）。

- 性质一：如果我们按常数因⼦α缩放向量的所有元素，其范数也会按相同常数因⼦的绝对值缩放：f(αx) = |α|f(x).

- 性质二：的三⻆不等式：f(x + y) ≤ f(x) + f(y).

- 性质三：范数必须是⾮负的（范数最⼩为0，当且仅 当向量全由0组成。）：f(x) ≥ 0

- L2范数是向量元素平⽅和的平⽅根（在L2范数中常常省略下标2，也就是说∥x∥等同于∥x∥2。）:

  ![image-20221021210556030](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221021210556030.png?raw=true)

```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)

tensor(5.)
```

- L1范数，将绝对值函数和按元素求和组合起来

```
torch.abs(u).sum()

tensor(7.)
```

- ，矩阵X ∈ R m×n的Frobenius范数（Frobenius norm）是矩阵元素平⽅和的平⽅根：

  ![image-20221021211031105](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221021211031105.png?raw=true)

```
torch.norm(torch.ones((4, 9))

tensor(6.)
```



##### 范数和目标

如⽤向量表⽰物品（如单词、产品或新闻⽂章），以便最⼩化相似项⽬之间的距离,最⼤化不同项⽬之间 的距离。⽬标通常被表达为范数。

  





### DAY 3



  

##### 微积分



拟合模型的任务分解为两个关键问题：

- 优化（optimization）：⽤模型拟合**观测数据**的过程； 

- 泛化（generalization）：数学原理和实践者的智慧，能够指导我们⽣成出有效性超出⽤于训练的数据集 本⾝的模型



（1）通常选择对于 模型参数**可微**的损失函数。简⽽⾔之，对于每个参数，如果我们把这个参数增加或减少⼀个⽆穷⼩的量，我 们可以知道损失会以多快的**速度**增加或减少。导数f ′ (x)解释为f(x)相对于x的瞬时（instantaneous）变化率。所谓的瞬时变化率是基于x中的变化h，**且h接近0。**



###### 导数和微分

- ##### 可微必可导，可导必可微（互为充分必要条件），取微分相当于画了一个线性函数，在某点附近能较好的逼近函数。取导数是给出该线性函数的斜率。

- ##### 对于多元函数，可微可以推出可导（即在各个方向都可导），**但任何方向偏导数存在，函数也不一定可微**（因为其他方向的偏导未知）。



###### 导数



```python
%matplotlib inline
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l


def f(x):
    return 3*x**2-4*x
    
def numerical_lim(f,x,h):
    return (f(x+h)-f(x))/h
    
    
    
h=0.1
for i in range(10):
    print(f'h={h:.5f},numerical limit={numerical_lim(f,1,h):.5f}')
    h*=0.1
```

```
h=0.10000,numerical limit=2.30000
h=0.01000,numerical limit=2.03000
h=0.00100,numerical limit=2.00300
h=0.00010,numerical limit=2.00030
h=0.00001,numerical limit=2.00003
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
```

  

##### 可视化



```
plot()
fmt = '#color#linestyle#marker'即代表各类参数。

eg:
fmts=('-', 'm--', 'g-.', 'r:')
分别是：实线,品色实线'-'点、绿色实线点、红色点虚线
```



```
#返回维度数目，如二维数组都是2，一维数组都是1
.ndim
```





##### 偏导数

推广到多元函数，对各个方向上的导数



##### 梯度

连结⼀个多元函数对其所有变量的偏导数，以得到该函数的梯度（gradient）**向量**



##### 矩阵求导



##### （1）标量对列向量的导数是行向量,行向量的每个元素就是标量的对每个元素的导数，得到的行向量表示的方向就是梯度（标量为常数的时候得到元素全为0的向量）。

##### （2）列向量对标量的导数是列向量

##### （3）向量对向量求导，对应元素对应求导





![image-20221022192651448](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192651448.png)



1.

![image-20221022192311613](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192311613.png)



2.![image-20221022192331544](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192331544.png)

3.

![image-20221022192430282](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192430282.png)



4.

![image-20221022192451673](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192451673.png)





![image-20221022192708865](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192708865.png)



1.

![image-20221022192723804](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192723804.png)

2.

![image-20221022192746187](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192746187.png)

3.

![image-20221022192800367](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192800367.png)

4.

![image-20221022192909460](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192909460.png)

5.

![image-20221022192925612](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192925612.png)

##### 

##### 线性回归



（1）**仿射变换**的特点是通过 加权和对特征进⾏线性变换（linear transformation），并通过偏置项来进⾏平移（translation）。

（2）机器学习通常使⽤的是**⾼维数据集**，建模时采⽤**线性代数**表⽰法会⽐较⽅便。





##### 损失函数

损失函数（loss function） 能够量化⽬标的**实际值**与**预测值**之间的差距。通常我们会选择**⾮负数**作为损失，且数值越⼩表⽰损失越⼩， 完美预测时的损失为0（常用平方损失函数）

![image-20221023211455159](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023211455159.png)

eg预测值的时候：

```
预测试和真实标签值相减然后平方外面再乘1/2  #1/2是为了求导后系数化为1
```

使用平方损失函数的时候：由于平⽅误差函数中的**⼆次⽅项**，估计值yˆ (i)和观测值y (i)之间较⼤的**差异将导致更⼤的损失**。为了**度量模型 在整个数据集上的质**量，我们需计算在训练集**n个样本上的损失均值**（也等价于求和）。

![image-20221023211506146](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023211506146.png)

在**训练模型**的时候，，我们希望寻找⼀组参数（w∗ , b∗），这组参数能**最⼩化在所有训练样本上的总损失**。如下式： w ∗ , b∗ = argmin w,b L(w, b).





### DAY 4



##### 反向传播

![image-20221023142503416](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023142503416.png)



![image-20221023142843935](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023142843935.png)



计算复杂度O(n),内存复杂度O(n)需要存储正向累积中的中间结果





##### 自动求导

深度学习大部分时候是对**标量**求导



```
x=torch.arange(4.0)
x.requires_grad_(True)

x.grad.zero_() #清0梯度
```

对于

```
y=x*x   #x是一个向量

#通常把y转换成一个标量再来求导
y.sum().backward()  #等价y.backward(torch.ones(len(x)))
x.grad
```

而对于

```
y=2*torch.dot(x,x)  #本身就是标量
y.backward()
```



```
y=x*x
u=y.detach()  #转换为常数
z=u*x   

z.sum().backward()
x.grad==u
```





##### 解析解

可 以⽤⼀个公式简单地表达出来，这类解叫作解析解。

如线性回归就是||y-Xw||的平方,此时损失平⾯上只有 ⼀个临界点，这个临界点对应于整个区域的损失极⼩点。将损失关于**w的导数设为0**，得到解析解：

![image-20221023212858426](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023212858426.png)



##### 随机梯度下降

（1）梯度下降：不断地在**损失函数递减的⽅向上更新**参数来降低误差。

（2）可以计算损失函数（数据集中**所有样本的损失均值**）关于模型参数的导数（在这⾥也可 以称为梯度）

（3）每次迭代的时候，随级抽取小批量的训练样本，计算**该批量样本的损失均值**关于**模型参数的导数**（梯度）；最后将**梯度乘以⼀个预先确定的正数η**，并从 当前参数的值中**减掉**。

![image-20221023213719650](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023213719650.png)



##### 总结：

（1）初始化模型参数（如随机初始化）

（2）从数据集中**随机**抽取**⼩批量**样 本且**在负梯度的⽅向上更新参数**，并**不断迭代**这⼀步骤。

对于平⽅损失和仿射变换：

![image-20221023214032986](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023214032986.png)

（3）算法会使得损失向最⼩值**缓慢收敛**，但却不能在有限的步数内⾮常精确地达到最⼩值。

（4）深度神经⽹络这样复杂的模型来说， 损失平⾯上通常包含多个最⼩值。



##### 泛化：找到⼀组参数，这组参数能够在我们从未⻅过的数据上实现较低的 损失，这⼀挑战被称为泛化（generalization）。





##### 使用模型进行预测







##### 正态分布和平方损失

改变均值会产⽣沿x轴的偏移，**增加⽅差**将会分散分布、降低其峰值。

**均⽅误差损失函数**（简称均⽅损失），在⾼斯 噪声的假设下，最⼩化均⽅误差等价于对线性模型的极⼤似然估计。





##### 神经网络图

（1）输⼊为x1, . . . , xd，因此输⼊层中的输⼊数（或称为**特征维度**，feature dimensionality）为**d**。

（2）将线性回归模型视为仅由**单个⼈⼯神经元**组成的神经⽹络，或 称为**单层神经⽹络**。

（3）**每个输⼊都与每个输出相连**,则称称为**全连接层**（fully-connected layer）或称为**稠密层**（dense layer）。



