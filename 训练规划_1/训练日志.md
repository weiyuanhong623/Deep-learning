### DAY1

<br>

##### 完成实验环境的搭建

<br>

<br>

---

---

网页学习部分

<br>

##### 什么是神经网络

人工神经网络是受到人类大脑结构的启发而创造出来的

训练深度神经网络的过程就叫做深度学习

<br>

##### 将数据输入神经网络

（1）对于图像，计算机要存储三个独立的矩阵（三个**通道**）分别对应红、绿、蓝，矩阵里面的**数值**就对应于图像的红绿蓝**强度值**

（2）一般将3个矩阵转化成1个**向量**x（向量可以理解成1 * n或n * 1的数组，前者为行向量，后者为列向量

![image-20221020183623969](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221020183623969.png)

（3）每一个输入到神经网络的数据都被叫做一个特征，由二维矩阵变成的**向量**就叫做**特征向量**，向量的长度就是该向量的**维度**。神经网络接收这个特征向量x作为输入，并进行预测，然后给出相应的结果。

```
eg:
对于一张20*20*20像素的图像 在计算机中有三个二维矩阵进行存储，转为向量后规格为：1*(20*20*3)或(20*20*3)*1，该向量维度就是1200，这张图像就有1200个特征
```

<br>

<br>

**总结**：不同应用场景所输入的数据在计算机中都有对应的数字表示形式，将其化成一个特征向量，然后将其输入到神经网络中

<br>

<br>

##### 神经网络如何进行预测

<br>

向量相乘（对应元素相乘再相加）

```
dot(w,x)
```

<br>

场景：对某一事件进行预测

输入：影响事件发展结果因素（特征），并对特征赋予不同的**权重**，设置不同输出对应的事件结果

输出：预测结果

<br>

<br>

##### 激活函数——sigmoid

![image-20221020185827218](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221020185827218.png)

z越大越接近1，越小越接近0

<br>

<br>

如用于分类： 若输出经过sigmoid的隐藏层后输出为0.8，说明有80%的概率是目标类别

<br>

---

---

<br>

##### 《drive into deeplearning》学习部分

<br>

<br>

任⼀调整参数后 的程序，我们称为**模型**（model）。

通过操作参数⽽⽣成的所有不同程序（输⼊-输出映射）的集合称为“**模 型族**”

使⽤数据集来选择参数的元程序被称为**学习算法**（learning algorithm）。

<br>

<br>

#### 机器学习的一些关键组件

<br>

1.数据

（1）数据由**样本**组成，每个样本由被称为**特征**的属性组成，机器学习**模型**会根据这些属性进⾏预测。如在**监督学习问题**中，要预测的是⼀个特殊的属 性，它被称为**标签**（label，或⽬标（target））。

（2）与传统机器学习⽅法相⽐，深度学习的⼀个主要优势是可以处理**不同⻓度的数据**。

（3）更多的数据意味着可以**减少对预先设想假设的依赖**（数据集的由⼩变⼤为现代深度学习的成功奠定基础）

（4）要**筛除无效数据**以及注意数据集的**均衡问题**

<br>

2.模型

深度学习与经典⽅法的区别主要在于：前者关注的功能强⼤的模型，这些模型由神经⽹络错综复杂的交织在⼀起，包含层层数据转换，因此被称为深度学习（deep learning）。

<br>

3.目标函数

（1）定义模型的**优劣程度的度量**，这个度量在⼤多 数情况是“**可优化**”的，我们称之为⽬标函数（objective function），有时被称为损失函数。

（2）通常，损失函数是**根据模型参数**定义的，并取决于数据集。最常⻅的损失函数是平⽅误差（squared error）

（3）总的流程是：通过**最⼩化总损失**来学习 模型**参数**的**最佳值**。这里是获取**损失**。

<br>

4.优化算法

（1）**搜索出最佳参数**，以最⼩化损失函数（⼤多流⾏的优化算法通常基于⼀种基本⽅法‒梯度下 降）。在每个步骤中，梯度下降法都会检查每个参数，看看如果你仅对该参数 进⾏少量变动，训练集损失会朝哪个⽅向移动。然后，它在可以**减少损失的⽅向**上优化参数。

<br>

<br>

#### 机器学习的类别

##### 1.监督学习

（1）在“给定输⼊特征”的情况下预测标签。每个“**特征-标签**”对都称为 ⼀个样本（example）。估计给定输⼊特征的标签。

（2）监督学习过程：从已知数据集中随级选取一个子集，给子集中的每个样本获取真实标签（有的样本已有标签；有的需要人工标记），输入的特征和标签构成训练集通过**学习算法**输出**模型**，然后使用测试集特征进行输入，模型的输出作为对输入特征的标签的预测。

<br>

###### 监督学习的细分：

##### 回归

本质由输出决定，输出的预测标签取任意数值。⽬标是⽣成⼀个模型，它的预测⾮常接近实际标签值。

<br>

##### 分类

在分类中，我们训练⼀个**分类器**，它的输出即为预测的类别。

- 模型预测样本属于哪个类别。最简单的分类问题是只有两类，我们称之为“**⼆元分类**”。

- 有两个以上的类别时，我们把这个问题称为**多元分类**（multiclass classification）问题。
- 寻找层次结构，层次 结构假定在许多类之间存在某种关系。**层次分类**（如对动物进行层次分类）

<br>

##### 标记问题

学习预测**不相互排斥的类别**的问题称为**多标签分类**

<br>

##### 搜索

模型要输出**有序**的**元素⼦集**。可以先为集合中的每个元素分配相应的**相关性分数**，然后**检索**评级最⾼的元素。

<br>

##### 推荐系统

向特定⽤⼾进⾏“个性化” 推荐。推荐系统会为“给定⽤⼾和物品”的匹配性打分，这个“分数”可能是估计 的评级或购买的概率，然后检索得分最⾼的对象集推给用户

<br>

### DAY 2

---

---

#####   <br>

##### 《drive into deeplearning》学习部分

<br>

##### 序列学习

输⼊和输出都是可变⻓度的序列  

  <br>

- ###### 标记和解析

⽤属性注释⽂本序列（如给一个句子标记实体）

- ###### ⾃动语⾳识别

音频和文本之间没有1：1的对应关系，可能多个样本对应一个单独单词

- ###### ⽂本到语⾳

输出音频多于输入

- ###### 机器翻译

⽽在机器翻译中，颠倒输⼊和输出的顺序⾮常 重要。  

  <br>

  <br>

   <br>

#### 2.无监督学习

数据中不含有“⽬标”的机器学习问题为⽆监督学习（unsupervised learning）。  

  <br>

- 聚类

  如无标签的情况下进行分类

- 主成分分析

  少量的参数来准确地捕捉数据的**线性相关属性**

- 因果关系和概率图模型

  描述观察到的 许多数据的根本原因

- 生成对抗性网络

  提供了⼀种合成数据的⽅法，潜在的统计机制是检查真实和虚假数据是否相同的测试  

  <br>

  <br>

  <br>

#### 3.与环境互动

监督、无监督学习都是在算法与环境断开后进⾏的，被称为**离线学习**（offline learning）。 

  <br>

  <br>

#### 4.强化学习

（1）在强化学习问题中，agent在⼀系列的时间步骤上与环境交互。在每个特定时间点，agent从环境接收⼀些观 察（observation），并且必须选择⼀个动作（action），然后通过某种机制（有时称为执⾏器）将其传输回环境， 最后agent从环境中获得奖励（reward）。此后新⼀轮循环开始，agent接收后续观察，并选择后续操作，依此 类推。

强化学习的⽬标是产⽣⼀个好的**策略**（policy）。强 化学习agent选择的**“动作”受策略控制**，即⼀个从环境观察映射到⾏动的功能。  

<br>

（2）**可以将任何监督学习问题转化为强化学习问题**（设置的奖励的原始监督学习问题的损失函数一致）。强化学习还可以解决许多监督学习⽆法解决的问题（如并不告诉agent每个观测的最优动作，而只是给其相应的奖励）

  <br>

（3）当环境可被完全观察到时，我们将强化学习问题称为⻢尔可夫决策过程（markov decision process）。当状 态不依赖于之前的操作时，我们称该问题为上下⽂赌博机（contextual bandit problem）。当没有状态，只有 ⼀组最初未知回报的可⽤动作时，这个问题就是经典的多臂赌博机（multi-armed bandit problem）。

  <br>

 <br>

  <br>

#### 深度学习起源  

<br>

#####   神经网络中的关键原则

• 线性和⾮线性处理单元的交替，通常称为层（layers）。 

• 使⽤链式规则（也称为反向传播（backpropagation））⼀次性调整⽹络中的全部参数。

​    <br>

机器学习和统计的关注点从（⼴义的）线性模型和核⽅法转 移到了深度神经⽹络的**原因**

（1）算⼒的增⻓速度已经超过了现有数据的增⻓速度。

（2）计算预算的增 加，能够花费更多时间来优化这些参数

  <br>

##### 深度学习特点

（1）深度学习是“深度”的，模型 学习了许多“层”的转换，每⼀层提供⼀个层次的表⽰。靠近输⼊的层可以表⽰数据的低级细节，⽽ 接近分类输出的层可以表⽰⽤于区分的更抽象的概念。深度学习可以称为“**多级表⽰学习**”

（2）深度学习⽅法中最显著的共同点是使⽤**端到端训练**。

```
Canny边缘检测器 [Canny,1987] 和SIFT特征提取器 [Lowe, 2004] 作为将图像映射到特征向量的算法
```

（3）不仅取代了传统学习管道末端的浅层模型，⽽且还取代了劳动密集型的 特征⼯程过程

（4）从参数统计描述到完全⾮参数模型的转变（借助于相关 偏微分⽅程的数值模拟，⽽不是⽤⼿来求解电⼦⾏为的参数近似）

  <br>

  <br>

  <br>

### pytorch

  <br>

####   数据操作

（1）具有⼀个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）；具有两个轴以上的张量没有特殊的数学名称。

（2）torch.cat()将张量连结

（3）sum()将产生**单元素张量**

（4）广播机制

（5）索引和切片

（6）节省内存

可以使⽤切⽚表⽰法将操作的结果分配给先前分配的数组

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))


>>>id(Z): 140316199714544
>>>id(Z): 140316199714544

或者类似于X+=Y
```

（7）转换为其他Python对象

```python
A = X.numpy()   #由tensor转换为ndarray
B = torch.tensor(A)  #由ndarray转换为tensor
```

将**⼤⼩为1的张量**转换为Python标量可以调⽤item函数或Python的内置函数。

```python
a=torch.tensor([3.5])
a.item(),float(a),int(a)
```

  <br>

  <br>

#### 数据预处理

  <br>

##### os.path.join()

用于路径拼接文件路径，可以传入多个路径

```
"./"：代表目前所在的目录

"../"：代表上一层目录

以"/"开头：代表根目录
```

```python
import os

#不存在以‘’/’开始的参数，则函数会自动加上
>>> print(os.path.join('path','abc','yyy'))
path\abc\yyy

# 存在以‘’/’’开始的参数，从最后一个以”/”开头的参数开始拼接，之前的参数全部丢弃。
>>> print('1',os.path.join('/aaa','bbb','/ccc.txt'))
1 /ccc.txt

# 同时存在以‘./’与’/’开始的参数，以‘’/’为主，从最后一个以”/”开头的参数开始拼接，之前的参数全部丢弃。
>>> print('2',os.path.join('/aaa','./bbb','ccc.txt'))
2 /aaa\./bbb\ccc.txt

#只存在以‘’./’开始的参数,会从”./”开头的参数的上一个参数开始拼接。
>>> print('2',os.path.join('aaa','./bbb','ccc.txt'))
2 aaa\./bbb\ccc.txt
```

<br>

实际运行

```python
>>>os.path.join('1','data','2')
'1\\data\\2'

原因：在程序中，字符串中的“\\”主要是为了转义，“\\”转义后被理解为“\”,“\”才能够被操作系统文件系统所理解，比如用字符串表示上述路径：“F:\\Office\\Trunk\\__Out\\Pro Debug\\Bin\\OfficeIn可以fo.dll”，同理，如果想要表示“\\”，可以写作“\\\\”。
```

<br>

（1）数据中的**NA**表示缺失，**NaN**：不是有效数字

<br>

###### 在上一级目录创建.csv文件并使用pandas进行读取后输出

```python
import os
os.makedirs(os.path.join('..','data'),exist_ok=True)
data_file=os.path.join('..','data','house_tiny.csv')
with open(data_file,'w') as f:
    f.write("NumRooms,Alley,Price\n")   #列名（表头）
    f.write("NA,PAve,127500\n")
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')
```

```python
import pandas as pd
data=pd.read_csv(data_file)
print(data)
```

<br>

###### 处理缺失值

典型的⽅法包**括插值法**和**删除法**，其中插值法⽤⼀个替 代值弥补缺失值，⽽删除法则直接忽略缺失值。

<br>

插值法：通过位置索引iloc，我们将data分成inputs和outputs，其中前者为data的前两列，⽽后者为data的最 后⼀列。对于inputs中缺少的数值，我们⽤同⼀列的**均值**替换“NaN”项。

```
inputs,outputs=data.iloc[:,0:2],data.iloc[:,2]
inputs=inputs.fillna(inputs.mean())
print(inputs)
```

<br>

<br>

对于inputs中的类别值或离散值，我们将“NaN”视为⼀个类别。pandas可以⾃动将此列转换为两列，将***_nan的值设置为0，另一列为1**

```python
inputs=pd.get_dummies(inputs,dummy_na=True)
print(inputs)
```

<br>

###### 转换为张量

```
X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
X, y
```

  <br>

  <br>

####   线性代数

（1）⼤量⽂献认为**列向量**是向量的默认⽅向,在表⽰表格数据集的矩阵中，将每个**数据样本**作为矩阵中的**⾏向量**更为常⻅

（2）可以通过调⽤Python的内置**len()**函数来访问张量的⻓度。

（3）区分张量和向量的维度数。张量的维度⽤来表⽰张量具有的轴数

（4）矩阵的转置，⽤a ⊤来表⽰矩阵的转置

```python
A.T
```

```python
B = A.clone() # 通过分配新内存，将A的⼀个副本分配给B
```

<br>

##### （5）两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号⊙）

```
A*B
```

<br>

##### （6）点积，又称数量积。对应元素相乘再相加(给定两个向量x, y ∈ R d，它 们的点积（dot product）x ⊤y )

```python
#4*1 和4*1进行点积  =>1*4 和4*1   =>1*1
x, y, torch.dot(x, y)

#等价于torch.sum(x*y)
```

<br>

（11）矩阵-矩阵乘法

这⾥的A是⼀个5⾏4列的矩阵，B是⼀个4⾏3列的矩阵。 两者相乘后，我们得到了⼀个5⾏3列的矩阵。

```
torch.mm(A,B)
```

<br>

（7）张量**乘以或加上**⼀个标量不会改变张量的形状，其中张量的**每个元素**都将与标量相加或相乘。

<br>

##### （8）降维

默认情况下，调⽤求和函数会沿所有的轴降低张量的维度，使它变为⼀个标量。还可以指定张量**沿哪⼀ 个轴**来通过求和降低维度。

```python
A=torch.tensor([[1,2,3],
               [4,5,6],
               [7,8,9]])
```

```python
#按行对每列进行求和,输⼊轴0的维数在输出形状中消失。
A_sum_axis0 = A.sum(axis=0)
tensor([12, 15, 18])

A_sum_axis0.shape
torch.Size([3])
```

```python
#按列对每行进行求和,输⼊轴1的维数在输出形状中消失。
A_sum_axis1 = A.sum(axis=1)
tensor([ 6, 15, 24])

A_sum_axis1.shape
torch.Size([3])
```

```python
#沿着⾏和列对矩阵求和，等价于对矩阵的所有元素进⾏求和。
A.sum(axis=[0, 1]) # SameasA.sum()
tensor(45)
```

<br>

（9）均值

```python
A.mean(), A.sum() / A.numel()
```

均值计算也可降维

```python
A.mean(axis=0)

tensor([4., 5., 6.])
```

<br>

（10）非降维求和

eg:计算总和或均值时保持轴数

```
sum_A = A.sum(axis=1, keepdims=True)
tensor([[ 6.],
        [15.],
        [24.]])
        
sum_A.shape
torch.Size([3, 1])
```

可通过广播

```
A/sum_A

tensor([[0.1667, 0.3333, 0.5000],
        [0.2667, 0.3333, 0.4000],
        [0.2917, 0.3333, 0.3750]])
```

  <br>

cumsum不会沿任何轴降低输入张量的维度

```
A.cumsum(axis=0)

tensor([[ 1.,  2.,  3.],
        [ 5.,  7.,  9.],
        [12., 15., 18.]])
```

<br>

（12）范数

⼀个向量的范数告诉我们⼀个**向量有多⼤**（size）。

- 性质一：如果我们按常数因⼦α缩放向量的所有元素，其范数也会按相同常数因⼦的绝对值缩放：f(αx) = |α|f(x).

- 性质二：的三⻆不等式：f(x + y) ≤ f(x) + f(y).

- 性质三：范数必须是⾮负的（范数最⼩为0，当且仅 当向量全由0组成。）：f(x) ≥ 0

- L2范数是向量元素平⽅和的平⽅根（在L2范数中常常省略下标2，也就是说∥x∥等同于∥x∥2。）:

  ![image-20221021210556030](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221021210556030.png?raw=true)

```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)

tensor(5.)
```

- L1范数，将绝对值函数和按元素求和组合起来

```
torch.abs(u).sum()

tensor(7.)
```

- ，矩阵X ∈ R m×n的Frobenius范数（Frobenius norm）是矩阵元素平⽅和的平⽅根：

  ![image-20221021211031105](https://github.com/weiyuanhong623/Deep-learning/blob/main/images/image-20221021211031105.png?raw=true)

```
torch.norm(torch.ones((4, 9))

tensor(6.)
```

<br>

##### 范数和目标

如⽤向量表⽰物品（如单词、产品或新闻⽂章），以便最⼩化相似项⽬之间的距离,最⼤化不同项⽬之间 的距离。⽬标通常被表达为范数。

  <br>

<br>



### DAY 3

---

---

<br>

<br>

##### 微积分

<br>

拟合模型的任务分解为两个关键问题：

- 优化（optimization）：⽤模型拟合**观测数据**的过程； 

- 泛化（generalization）：数学原理和实践者的智慧，能够指导我们⽣成出有效性超出⽤于训练的数据集 本⾝的模型

<br>

（1）通常选择对于 模型参数**可微**的损失函数。简⽽⾔之，对于每个参数，如果我们把这个参数增加或减少⼀个⽆穷⼩的量，我 们可以知道损失会以多快的**速度**增加或减少。导数f ′ (x)解释为f(x)相对于x的瞬时（instantaneous）变化率。所谓的瞬时变化率是基于x中的变化h，**且h接近0。**

<br>

###### 导数和微分

- ##### 可微必可导，可导必可微（互为充分必要条件），取微分相当于画了一个线性函数，在某点附近能较好的逼近函数。取导数是给出该线性函数的斜率。

- ##### 对于多元函数，可微可以推出可导（即在各个方向都可导），**但任何方向偏导数存在，函数也不一定可微**（因为其他方向的偏导未知）。

<br>

###### 导数

<br>

```python
%matplotlib inline
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l


def f(x):
    return 3*x**2-4*x
    
def numerical_lim(f,x,h):
    return (f(x+h)-f(x))/h
    
    
    
h=0.1
for i in range(10):
    print(f'h={h:.5f},numerical limit={numerical_lim(f,1,h):.5f}')
    h*=0.1
```

```
h=0.10000,numerical limit=2.30000
h=0.01000,numerical limit=2.03000
h=0.00100,numerical limit=2.00300
h=0.00010,numerical limit=2.00030
h=0.00001,numerical limit=2.00003
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
h=0.00000,numerical limit=2.00000
```

  <br>

##### 可视化

<br>

```
plot()
fmt = '#color#linestyle#marker'即代表各类参数。

eg:
fmts=('-', 'm--', 'g-.', 'r:')
分别是：实线,品色实线'-'点、绿色实线点、红色点虚线
```

<br>

```
#返回维度数目，如二维数组都是2，一维数组都是1
.ndim
```

<br>

<br>

##### 偏导数

推广到多元函数，对各个方向上的导数

<br>

##### 梯度

连结⼀个多元函数对其所有变量的偏导数，以得到该函数的梯度（gradient）**向量**

<br>

##### 矩阵求导

<br>

##### （1）标量对列向量的导数是行向量,行向量的每个元素就是标量的对每个元素的导数，得到的行向量表示的方向就是梯度（标量为常数的时候得到元素全为0的向量）。

##### （2）列向量对标量的导数是列向量

##### （3）向量对向量求导，对应元素对应求导

<br>



![image-20221022192651448](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192651448.png)



1.

![image-20221022192311613](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192311613.png)



2.![image-20221022192331544](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192331544.png)

3.

![image-20221022192430282](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192430282.png)



4.

![image-20221022192451673](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192451673.png)





![image-20221022192708865](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192708865.png)



1.

![image-20221022192723804](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192723804.png)

2.

![image-20221022192746187](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192746187.png)

3.

![image-20221022192800367](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192800367.png)

4.

![image-20221022192909460](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192909460.png)

5.

![image-20221022192925612](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221022192925612.png)

##### 

##### 线性回归

<br>

（1）**仿射变换**的特点是通过 加权和对特征进⾏线性变换（linear transformation），并通过偏置项来进⾏平移（translation）。

（2）机器学习通常使⽤的是**⾼维数据集**，建模时采⽤**线性代数**表⽰法会⽐较⽅便。

<br>

<br>

##### 损失函数

损失函数（loss function） 能够量化⽬标的**实际值**与**预测值**之间的差距。通常我们会选择**⾮负数**作为损失，且数值越⼩表⽰损失越⼩， 完美预测时的损失为0（常用平方损失函数）

![image-20221023211455159](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023211455159.png)

eg预测值的时候：

```
预测试和真实标签值相减然后平方外面再乘1/2  #1/2是为了求导后系数化为1
```

使用平方损失函数的时候：由于平⽅误差函数中的**⼆次⽅项**，估计值yˆ (i)和观测值y (i)之间较⼤的**差异将导致更⼤的损失**。为了**度量模型 在整个数据集上的质**量，我们需计算在训练集**n个样本上的损失均值**（也等价于求和）。

![image-20221023211506146](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023211506146.png)

在**训练模型**的时候，，我们希望寻找⼀组参数（w∗ , b∗），这组参数能**最⼩化在所有训练样本上的总损失**。如下式： w ∗ , b∗ = argmin w,b L(w, b).

<br>

<br>

### DAY 4

---

---

<br>

<br>

##### 反向传播

![image-20221023142503416](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023142503416.png)

<br>

![image-20221023142843935](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023142843935.png)

<br>

计算复杂度O(n),内存复杂度O(n)需要存储正向累积中的中间结果

<br>

<br>

##### 自动求导

深度学习大部分时候是对**标量**求导

<br>

```
x=torch.arange(4.0)
x.requires_grad_(True)

x.grad.zero_() #清0梯度
```

对于

```
y=x*x   #x是一个向量

#通常把y转换成一个标量再来求导
y.sum().backward()  #等价y.backward(torch.ones(len(x)))
x.grad
```

而对于

```
y=2*torch.dot(x,x)  #本身就是标量
y.backward()
```

<br>

```
y=x*x
u=y.detach()  #转换为常数
z=u*x   

z.sum().backward()
x.grad==u
```

<br>

<br>

##### 解析解

可 以⽤⼀个公式简单地表达出来，这类解叫作解析解。

如线性回归就是||y-Xw||的平方,此时损失平⾯上只有 ⼀个临界点，这个临界点对应于整个区域的损失极⼩点。将损失关于**w的导数设为0**，得到解析解：

![image-20221023212858426](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023212858426.png)

<br>

##### 随机梯度下降

（1）梯度下降：不断地在**损失函数递减的⽅向上更新**参数来降低误差。

（2）可以计算损失函数（数据集中**所有样本的损失均值**）关于模型参数的导数（在这⾥也可 以称为梯度）

（3）每次迭代的时候，随级抽取小批量的训练样本，计算**该批量样本的损失均值**关于**模型参数的导数**（梯度）；最后将**梯度乘以⼀个预先确定的正数η**，并从 当前参数的值中**减掉**。

![image-20221023213719650](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023213719650.png)

<br>

##### 总结：

（1）初始化模型参数（如随机初始化）

（2）从数据集中**随机**抽取**⼩批量**样 本且**在负梯度的⽅向上更新参数**，并**不断迭代**这⼀步骤。

对于平⽅损失和仿射变换：

![image-20221023214032986](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221023214032986.png)

（3）算法会使得损失向最⼩值**缓慢收敛**，但却不能在有限的步数内⾮常精确地达到最⼩值。

（4）深度神经⽹络这样复杂的模型来说， 损失平⾯上通常包含多个最⼩值。

<br>

##### 泛化：找到⼀组参数，这组参数能够在我们从未⻅过的数据上实现较低的 损失，这⼀挑战被称为泛化（generalization）。

<br>

<br>

##### 使用模型进行预测

<br>

<br>

<br>

##### 正态分布和平方损失

改变均值会产⽣沿x轴的偏移，**增加⽅差**将会分散分布、降低其峰值。

**均⽅误差损失函数**（简称均⽅损失），在⾼斯 噪声的假设下，最⼩化均⽅误差等价于对线性模型的极⼤似然估计。

<br>

<br>

##### 神经网络图

（1）输⼊为x1, . . . , xd，因此输⼊层中的输⼊数（或称为**特征维度**，feature dimensionality）为**d**。

（2）将线性回归模型视为仅由**单个⼈⼯神经元**组成的神经⽹络，或 称为**单层神经⽹络**。

（3）**每个输⼊都与每个输出相连**,则称称为**全连接层**（fully-connected layer）或称为**稠密层**（dense layer）。

  <br>

  <br>

  <br>

### DAY 5

---

---

<br>

<br>

<br>

（1）在深度学习框架中实现的内置迭代器效率要⾼得多，它可以处理存储在⽂件中 的数据和数据流提供的数据。

<br>

###### 初始化模型参数

eg：

```
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

```

<br>

##### reshape((-1,1))

即：转化为1列

```
arr.shape    # (a,b)
arr.reshape(m,-1) #改变维度为m行、d列 （-1表示列数自动计算，d= a*b /m ）
arr.reshape(-1,m) #改变维度为d行、m列 （-1表示行数自动计算，d= a*b /m ）
```

##### 将向量转为1列，行数自行计算后维度仍然为1

<br>

<br>

##### Python中的with

with 语句**适用于对资源进行访问的场合**，确保不管使用过程中是否发生异常都会**执行必要的“清理”操作，释放资源**，比如文件使用后自动关闭／线程中锁的自动获取和释放等。

<br>

eg：

对于

```python
file = open("１.txt")
data = file.read()
file.close()
```

可改进

```python
try:
    f = open('xxx')
except:
    print('fail to open')
    exit(-1)
try:
    do something
except:
    do something
finally:
    f.close()
```

<br>

使用with

（１）紧跟with后面的语句被求值后，返回对象的“–enter–()”方法被调用，这个方法的返回值将被**赋值给as**后面的变量；
（２）当with后面的代码块全部被执行完之后，将调用前面返回对象的“–exit–()”方法。

```python
with open("１.txt") as file:
    data = file.read()
```

<br>

##### requires_grad参数，如果设置为True，则反向传播时，该tensor就会自动求导,默认False。

**with torch.no_grad的作用**
在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。即使中间有张量设置requires_grad为true

<br>

<br>

###### 定义损失函数

定义损失函数记得将真实值转换为预测值的形状。

<br>

##### 一般把损失均值的求取提出来，放到梯度下降那部分

<br>

##### 

### softmax回归

<br>

如果**类别间有⼀些⾃然顺序**，⽐如说我们试图预测{婴⼉, ⼉童, ⻘少年, ⻘年⼈, 中年⼈, ⽼年⼈}，那么将这个问题转变为**回归问题**，并且保留这种格式是 有意义的。

<br>

##### 独热编码（one-hot encoding）

（1）⼀种表⽰**分类数据**的简单⽅法

（2）独热编码是⼀个**向量**，它的**分量和类别**⼀样多。类别**对应的分量设置为1**，其他所有分量设置为0。

<br>

<br>

###### 网络架构

多个**输入**（多个**特征**）多个输出，每个输出对应一个类别，需要和输出一样多的仿射函数（affine function）

```
eg：
3个类别，每个样本有4个特征，则总的需要12个参数
```

<br>

softmax回归也是⼀个单层神经⽹络。 由于计算**每个**输出o1、o2和o3**取决于所有输⼊**x1、x2、x3和x4，所以softmax回归的**输出层也是全连接层**。

<br>

（1）设置阈值，如选择输出值中减去一个数>=阈值的输出值的类别作为预测类别（主要是关心**输出**对**正确类别**的**置信度**大不大）

（2）需要注意，不能将**未规范化**的预测o直接视作我们感兴趣的输出，必须保证输出都是非负且总和为1。设计⽬标函数来激励模型精准地估计**概率**。例如，在分类器输出**0.5**的所有样本中，我们希望这些样本是刚好有⼀ 半实际上属于预测的类别。这个属性叫做**校准**（calibration）。

<br>

##### softmax运算

将未规范化的预测变换为**⾮负数**并且**总和为1**，同时让模型保持**可导**的性质。

（1）对每个未规范化的**预测**求幂，这样可以确保输出⾮负。

（2）为了确保最终输出的概率值总和为1， 我们再让**每个**求幂后的结果除以它们的**总和**。（使用softmax操作子得到每个类的预测置信度）

![image-20221025004442747](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025004442747.png)

##### exp(n)

返回e的n次方

<br>

（3）对于所有的输出总有0 ≤ yˆj ≤ 1

（4）softmax运算**不会改变未规范化的预测o之间的⼤⼩次序**，只会确定分配给每个类别的概率

（5）尽管softmax是⼀个**⾮线性函数**，但softmax回归的**输出**仍然**由输⼊特征的仿射变换**决定。因此，softmax回 归是⼀个线性模型（linear model）。

<br>

###### 损失函数

##### 交叉熵损失（cross-entropy loss）

![image-20221025004048139](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025004048139.png)

##### • 交叉熵是⼀个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的⽐特数。这里使用交叉熵来衡量预测和标号的差异，并将这种差异作为损失

<br>

<br>

##### softmax的导数

###### 交叉熵带入softmax运算



![image-20221025004520668](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025004520668.png)



![image-20221025004555730](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025004555730.png)

导数是我们s**oftmax模型分配的概率**与**实际发⽣的情况**（由独热标签向量表⽰）之间的差异。

<br>

<br>

##### 熵

分布p的熵

![image-20221025005121138](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025005121138.png)

<br>

<br>

### 损失函数

##### L2 Loss 平方损失

主要特性：当预测值和真实值隔得比较远的时候，权重更新的比较多，**靠近**真实值的时候，**梯度的绝对值会变小**，这样对于权重的更新也比较小。，梯度的平滑性较好。

<br>

##### L1 Loss绝对值损失

（1）预测值和真实值相减的绝对值

（2）主要特性：当预测值和真实值隔的比较远的时候，梯度永远是常数，对权重的更新不会特别大，可以带来稳定性上的好处。

**缺点**，0点处不可导，平滑性不太好。优化到后期，真实值和预测值相接近的时候，权重可能不太稳定。

<br>

##### 当真实值和预测值相差的绝对值大于1的时候使用L1loss





![image-20221025010701928](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025010701928.png)

  

 <br> <br>

<br>

#### DAY 6

---

---

<br>

###### 二维索引

```
y = torch.tensor([0, 2])

y_hat = torch.tensor([[0.9, 0.3, 0.6], [0.7, 0.2, 8]])


#从y这个数组里拿出索引为0和1所代表的值来作为y_hat数组的索引
y_hat[[0,1],y]
```

  <br>

### 分类精度

分类精度即正确预测数量与总预测数量之⽐，精度通常是我们最关⼼的**性能衡量标准**。

```
def accuracy(y_hat, y): #@save
#"""计算预测正确的数量"""
	if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:			#判断y_hat是否是一个矩阵
		y_hat = y_hat.argmax(axis=1)	 	#是矩阵的话则按列对每行求最大预测值 的下标
	cmp = y_hat.type(y.dtype) == y			
	return float(cmp.type(y.dtype).sum())	#把比较结果转为y的数据类型并求和

```

代码解析：

获取到**每个样本最大的预测值**的**下标**后和样本对应的**真实类别的下标**进行比较，将比较的结果由**bool**类型转换为和真实类别的下标所对应的数据类型（False->0,True->1），这样为**1的就表示该样本预测成功**，为**0的话就表示失败**，然后**求和**就能得到**预测成功的数目**。用该数目除以总的样本数就得到分类精度。

  <br>

  <br>

##### .step()实现权重更新

  <br>

###### isinstance函数

该函数的第一个参数是一个对象，第二个参数是一个类型名或多个类型名组成的[元组](https://so.csdn.net/so/search?q=元组&spm=1001.2101.3001.7020)，只要该对象是其中一个类型（当然也只能是一种类型）便返回True,否则False

  <br>

###### assert [条件] 

条件不满足则抛掷断点。

 <br>

 <br>

<br>

### 多层感知机

  <br>

（1）在线性回归和softmax回归中模型都是凡是仿射变换（带有偏置项的线性变换），如果我们的标签通过仿射变换后确实与我们的输⼊数据相关也能接受。

（2）线性模型，较为单调。任何**特征的增⼤**都会导致模型**输出的增⼤**（如果对应的**权重为正**），或者导致模输出的减⼩（如果对应的权重为负）

（3）例如：任何像素的重要性都以复杂的⽅式取决于该像素的上下⽂（周围像素的值）这时难以通过简单预处理、线性模型来描述。

  <br>

##### 隐藏层

###### 在⽹络中加⼊⼀个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。

（1）将多层**全连接层**堆叠在一起，每一层的输入都是上一层的输出。把前L-1层看作**表示**，把最后一层看作**线性预测器**。这种架构被称为**多层感知机**（multilayer perceptron），通常缩写为**MLP**。（开销可能过大）

需要注意：仿射函数的仿射函数本⾝就是仿射函数，堆叠多层都等价于一层（退化为线性模型）。

eg：

一个三层的MLP模型，4个输入，3个输出，中间隐藏层包含5个隐藏单元。这里**输入层不涉及计算**，因此网络输出只要实现隐藏层和输出层的计算。多层感知机中的**层数为2**。

  <br>

（2）为了发挥多层架构的潜⼒，我们还需要⼀个额外的关键要素：在**仿射变换之后**对每个隐藏单元应⽤**⾮线性的 激活函数**（activation function）σ。激活函数的输出（例如，σ(·)）被称为活性值（activations）。

  <br>

（3）多层感知机可以通过隐藏神经元，捕捉到输⼊之间复杂的相互作⽤。层感知机是**通⽤近似器**。即使是⽹络只有⼀个隐藏层，给定⾜够的神经元和正确的权重，我们可以对任意函数建模，尽管实际 中学习该函数是很困难的。事实上应该使用更深（而不是更广）的网络来逼近函数。

  <br>

  <br>

 <br>

#### 激活函数

激活函数（activation function）**通过计算加权和并加上偏置**来**确定神经元是否应该被激活**，它们将输⼊信号转换为输出的可微运算。⼤多数激活函数都是**⾮线性**的。

  <br>

###### Relu函数

修正线性单元（Rectified linear unit，ReLU）：给定元素x，ReLU函数被定义为**该元素与0的最⼤值**：

特性：

（1）实现简单，同时在各种预测任务中表现良好。

（2）ReLU函数通过将相应的活性值设为0，仅**保留正元素并丢弃所有负元素**（分段的）。

![image-20221025173239499](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025173239499.png)

（3）当**输⼊为负**时，ReLU函数的**导数为0**，⽽当**输⼊为正**时，ReLU函数的**导数为1**。注意，当输⼊值**精确等于0**时， ReLU函数**不可导**。在此时，我们**默认使⽤左侧的导数**，即当输⼊为0时导数为0。要么让参数消失，要么让参数通过。这使得优化表现得更好，并 且ReLU**减轻了**困扰以往神经⽹络的**梯度消失**问题

![image-20221025173246330](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025173246330.png)

  <br>

###### sigmoid函数

sigmoid函数将输⼊变换为**区间(0, 1)上的输出**。因此，sigmoid通常称为挤压 函数（squashing function）

  <br>

特性：

（1）基于梯度的学习时，sigmoid函数是⼀个⾃然的选择，因为它是⼀个**平滑**的、**可微**的阈值单元近似。

（2）将输出视作**⼆元分类**问题的概率时，sigmoid仍然被⼴泛⽤作输出单元上的激活函数（你可以将sigmoid视为softmax的特例）。

（3）输⼊接近0时，sigmoid函数接近线性变换

![image-20221025174007745](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025174007745.png)

（4）当输⼊为0时，sigmoid函数的导数**达到**最⼤值0.25；⽽输⼊在任⼀ ⽅向上越远离0点时，导数越**接近**0。

![image-20221025174030227](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025174030227.png)

  

 <br>

 <br>

<br>

###### tanh函数

tanh(双曲正切)函数也能将其输⼊压缩转换到区间(-1, 1)上

  <br>

特性：

（1）当输⼊在0附近时，tanh函数**接近线性变换**

（2）tanh函数关**于坐标系原点中⼼对称**

![image-20221025174308861](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025174308861.png)

（3）当输⼊接近0时，tanh函数的导数**接近**最⼤值1。输⼊在任⼀⽅向上越远离0点，导数越**接近**0。

![image-20221025174620454](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221025174620454.png)

 <br>

  <br>

 <br> <br>

### DAY 7

---

---

<br>

<br>

对于多层感知机的层数：通常，我们选择**2的若⼲次幂**作为层的**宽度**。因为内存在硬件中的分配和寻址⽅式，这么做往往可以在计 算上更⾼效。

<br>

##### 过拟合、正则化

（1）将模型在训练数据上拟合的⽐在潜在分布中更接近的现象称为过拟合（overfitting），⽤于对抗过拟合的技术 称为正则化（regularization）。

<br>

（2）训练误差（training error）是指，模型在训 练数据集上计算得到的误差。泛化误差（generalization error）是指，模型应⽤在同样从原始样本的分布中 抽取的⽆限多数据样本时，模型误差的期望。

<br>

##### 独立同分布

在监督学习情景中，我们假设**训练数据**和**测试数据**都是从**相同的分布中独⽴提取**的。这通常被称为**独⽴同分布假设**（i.i.d. assumption），这意味着对数据进⾏采样的过程没有 进⾏“记忆”。换句话说，抽取的第2个样本和第3个样本的相关性，并不⽐抽取的第2个样本和第200万个样本 的相关性更强。

<br>

##### 模型复杂性

（1）总述：有**简单的模型**和**⼤量的数据**时，我们期望泛化误差与训练误差相近。当我们有**更复杂的模型**和**更少的 样本**时，我们预计训练误差会下降，但泛化误差会增⼤。

<br>

##### （2）⼏个倾向于影响模型泛化的因素：

- ###### 可调整参数的数量。当可调整参数的数量（有时称为⾃由度）很⼤时，模型往往更容易过拟合。 

- ###### 参数采⽤的值。当权重的取值范围较⼤时，模型可能更容易过拟合。 

- ###### 训练样本的数量。即使你的模型很简单，也很容易过拟合只包含⼀两个样本的数据集。⽽过拟合⼀个有 数百万个样本的数据集则需要⼀个极其灵活的模型。

<br>

（3）⾼阶多项式函数⽐低阶多项式函数复杂得多。⾼阶多项式的参数较多，模型函数的选择范围较⼴。因此在固 定训练数据集的情况下，⾼阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。

<br>

<br>

**验证集**

为了确定候选模型中的最佳模型。

- 原则上，在我们确定所有的超参数之前，我们不希望⽤到**测试集**（避免在**测试集**上过拟合的风险）。训练集上的过拟合可以通过在测试集上的评估判断。
- 通常将数据集分为三分：训练集、测试集、验证集

<br>

##### k折交叉验证

起因：构成验证集数据不够；

解决方法：将原始数据集**分成K个不重叠的子集**。执行**k次模型训练和验证**。**每次**在**k-1个子集上训练**，在剩下的**1个子集上验证**。最后通过**对k次实验结果取均值**来估计训练和验证误差。

<br>

<br>

##### 数据集大小

（1）训练数据集中的**样本越少**，我们就**越有可能（且更严重地）过拟合**。

（2）如果没有⾜够的数据，简单的模型可能更有⽤。对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。

（3）深度学习⽬前的⽣机要归 功于廉价存储、互联设备以及数字化经济带来的海量数据集。

<br>

##### 权重衰减（正则化技术之一）

（1）**限制特征的数量**是缓解过拟合的⼀种常⽤技术。

（2）在**训练参数化**机器学习模型时，权重衰减（weight decay）的使用较为广泛，它通常也被 称为**L2正则化**。

（3）通过**函数与零的距离**来衡量函数的**复杂度**

eg：对于线性回归，可以向通过的**权重向量**的某个范数来**度量其复杂性**。要保证权重向量⽐较⼩，最常⽤⽅法是**将其范数作为惩罚项**加到**最⼩化损失**的问题中。将原来的训练⽬标最⼩化训 练标签上的预测损失，调整为**最⼩化预测损失和惩罚项之和。**

（4）通过**正则化常数λ**（非负，超参数）来描述这种权衡**新加了惩罚项的损失**

（5）使⽤L2范数的⼀个原因是它对**权重向量的⼤分量施加了巨⼤的惩罚**。这使得我们的学习算法偏向于在⼤量特征上**均匀分布权重**的模型。在实践中，这 可能使它们对单个变量中的观测误差更为稳定。相⽐之下，L1惩罚会导致模型**将权重集中在⼀⼩部分特征**上， ⽽将**其他权重清除为零**，这称为**特征选择**（feature selection）。

 <br> <br>

加入1/2是为了方便计算梯度

![image-20221027011115949](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221027011115949.png)



​                         



![image-20221027011105588](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221027011105588.png)



总结：

- 根据**估计值与观测值之间的差异**来更新w。然⽽，我们同时也在试图将w的**⼤⼩缩⼩到零**。
- 较⼩的λ值对应较少 约束的w，⽽较⼤的λ值对w的约束更⼤。
- 通常，⽹络**输出层**的**偏置项**不会被正则化。

![image-20221027192121211](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221027192121211.png)

<br>

<br><br>

#### DAY 8

---

---

<br>

<br>

#### Dropout

<br>

背景：

（1）模型应该深度挖掘特征，即**将其权重分散到许多特征中**，⽽不是过于依赖少数潜在的虚假关联。

（2）当⾯对**更多的特征⽽样本不⾜**时，**线性模型往往会过拟合**。相反，当给出**更多样本⽽不是特征**，通常线性模 型不会过拟合。

##### （3）线性模型没有考虑到特征之间的 交互作⽤。

（4）泛化性和灵活性之间的这种基本权衡被描述为**偏差-⽅差权衡**（bias-variance tradeoff）。**线性模型有很⾼的 偏差**：它们只能表⽰⼀⼩类函数。然⽽，这些模型的**⽅差很低**：它们**在不同的随机数据样本上可以得出相似 的结果**。

##### （5）深度神经⽹络位于偏差-⽅差谱的另⼀端。与线性模型不同，神经⽹络并不局限于单独查看每个特征，⽽是学 习特征之间的交互。

<br>

<br>

**经典泛化理论**认为，为了缩⼩训练和测试性能之间的差距，应该以**简单的模型**为⽬ 标。简单性以较⼩维度的形式展现：

- 参数的范数代表了⼀种有⽤的**简单性度量**

- 平滑性，即函数不应该对其**输⼊的微⼩变化**敏感

<br>

<br>

##### dropout概念：

（1）在训练过程中，他们建议在**计算后续层之前**向⽹络的**每⼀层注⼊噪声**。因为当训练⼀个有**多层** 的深层⽹络时，**注⼊噪声只会在输⼊-输出映射上增强平滑性**。

（2）神经⽹络过拟合与每⼀层都依赖于前⼀ 层激活值相关，称这种情况为“共适应性”。作者认为，暂退法会破坏共适应性，就像有性⽣殖会破坏共适应 的基因⼀样。

（3）是以⼀种**⽆偏向（unbiased）**的⽅式注⼊噪声。这样在固 定住其他层时，每⼀层的**期望值等于没有噪⾳时的值。**

![image-20221027193905465](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221027193905465.png)

<br>

<br>



具体实践:

在隐藏层中以p的概率将隐藏单元**置为0**。通常在训练的时候不用dropout

<br>

<br>

###### 实战总结：

• 真实数据通常混合了不同的数据类型，需要进⾏预处理。 

• 常⽤的预处理⽅法：将实值数据重新缩放为零均值和单位⽅法；⽤均值替换缺失值。 

• 将类别特征转化为指标特征，可以使我们把这个特征当作⼀个独热向量来对待。

 • 我们可以使⽤K折交叉验证来选择模型并调整超参数。 • 对数对于相对误差很有⽤。

<br>

<br>

#### 自定义块

<br>

每个块的**基本功能**:

1.输入数据作为向前传播的参数。

2.将向前传播函数生成输出，

3.计算输出关于输入的梯度

4.存储和访问前向传播需要的参数

5.根据需要初始化模型参数。

<br>

<br>

实例：

实现一个多层感知机：20维的输入，256个隐藏单元的隐藏层和10维的输出层

```python
class MLP(nn.Module):
    def __init__(self):
        #调用MLP的父类Module的构造函数来进行初始化
        super().__init__()
        self.hidden=nn.Linear(20,256)
        self.out=nn.Linear(256,10)
	
    # 定义模型的前向传播，即如何根据输⼊X返回所需的模型输出
    def forward(self,X):
        return self.out(nn.functional.relu(self.hidden(X)))
```

<br>

<br>

<br>

<br>

### DAY 9

---

---

<br>

<br>

##### 顺序块

Sequential：把其他模块串起来。

<br>

Sequential**基本功能**：

1.⼀种将块逐个追加到列表中的函数。 

2.⼀种前向传播函数，⽤于将输⼊按追加块的顺序传递给块组成的“链条”。

<br>

Sequential的实现：

```
#__init__函数将每个模块逐个添加到有序字典_modules中
#优点是：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的⼦块。
```

```python
class MySequential(nn.Module):
    def __init__(self,*args):
        super().__init__()
        for idx,module in enumerate(args):
            #module是Module子类的一个实例。保存在Module类的成员变量的_modules中。类型是OrderedDict
            self._modules[str(idx)]=module

    #定义向前传播
    def forward(self,X):
        ## OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X=block(X)
        return X
```

<br>

<br>

<br>

<br>

**向前传播中**合并既不是上⼀层的结果也不是可更新参数的项，我们称之为常数参数（constant parameter）。例如，我们需要 ⼀个计算函数 f(x, w) = c · w⊤x的层，其中x是输⼊，w是参数，c是某个在优化过程中**没有更新的指定常量**。

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()

        #向前传播不需要计算梯度
        self.rand_weight=torch.rand((20,20),requires_grad=False)
        self.linear=nn.Linear(20,20)

    #前向传播
    def forward(self,X):
        #全连接层
        X=self.linear(X)

        #隐藏层 relu使用创建的常量参数（这里是随机权重+1）
        X=F.relu(torch.mm(X,self.rand_weight)+1)

        #全连接层
        X=self.linear(X)

        while X.abs().sum()>1:
        #在L1范数⼤于1的条件下，将输出向量除以2，直到它满⾜条件为⽌。
            X/=2
        return X.sum()
```

<br>

<br>

<br>

混合使用模块

```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net=nn.Sequential(nn.Linear(20,64),nn.ReLU(),nn.Linear(64,32),nn.ReLU())
        self.linear=nn.Linear(32,16)

    #定义向前出传播
    def forward(self,X):
        #先按顺序将数据前向传播进入net的每一层，接着传播到全连接层输出。
        return self.linear(self.net(X))
```

<br>

<br>

<br>

### DAY 10

---

---

<br>

##### 参数管理

对参数的训练目标是：找到使得损失函数最小的参数。

<br>

###### 参数访问

- 通过Sequential类定义模型时，我们可以通过索引来访问模型的任意层。

eg：一个全连接层，Relu作为激活函数的隐藏层，一个全连接输出层，共**3层**。然后查看第一层的参数

```python
#下标为0,1,2
print(net[1].state_dict())
```

<br>

- 目标参数

（1）每个**参数**都表⽰为**参数类**的⼀个**实例**。要对参数执⾏任何操作，⾸先我们需要访问底层的数值。

（2）参数是复合的对象，包含值、梯度和额外信息，因此需要显示访问。

<br>

eg:从第2个全连接层提取偏置，提取后**返回一个参数实例**，并进一步访问该参数的值。

```python
print(type(net[2].bias))    #输出bias类型
#<class 'torch.nn.parameter.Parameter'>

print(net[2].bias)			#输出该实例
print(net[2].bias.data)		#输出该实例的值
```

<br>

进一步访问梯度（未调用反向传播无梯度）

```python
net[2].weight.grad == None

True
```

<br>

- 一次性访问所有参数

eg:

```python
#访问第一层的所有参数,并打印权重参数
print(*[(name,param.shape,param.data) for name,param in net[0].named_parameters()])


#访问所有层的参数
print(*[(name,param.shape) for name,param in net.named_parameters()])
```

<br>

```python
#另一种方法
print(net.state_dict()['2.bias'].data)
```

<br>

- 从嵌套模块收集参数

```python
#嵌套模块
def block1():
    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,4),nn.ReLU())


def block2():
    net=nn.Sequential()
    for i in range(4):

        #每个模块的名字为block+i,block2模块一共添加了4个模块；每个模块具体内容为block1，
        net.add_module(f'block{i}',block1())
    return net


rgnet=nn.Sequential(block2(),nn.Linear(4,1))
```

<br>

```python
#查看整个网络结构
print(rgnet)

Sequential(
  (0): Sequential(
    (block0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)

```

<br>

查看rgnet的第一个模块中的第一个模块block0块的第一层（全连接层）

```python
#查看具体层的结构
print(rgnet[0][0][0].weight.data)
```

<br><br><br><br>

##### 参数初始化

###### 内置初始化

eg:三层的感知机，一个(4,8)全连接层、一层隐藏层、一层(8,1)全连接层，将全连接层的**权重参数**初始化为**均值**为0,**方差**为0.1的**高斯分布**。**偏置参数**初始化为0

```python
def init_normal(m):
    if type(m)==nn.Linear:
        nn.init.normal_(m.weight,mean=0,std=0.1)
        nn.init.zeros_(m.bias)
        
        
net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))
net.apply(init_normal)   #去掉()
print(net[0].weight.shape,net[0].bias.shape)
print(net[0].weight.data[0],net[0].bias.data)
```

<br>

将参数初始化为**指定常数**

eg:将权重参数初始化为1

```python
def init_constant(m):
    if type(m)==nn.Linear:
        nn.init.constant_(m.weight,1)
        nn.init.zeros_(m.bias)
        
net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))
net.apply(init_constant)
print(net[0].weight.data[0],net[0].bias.data)
```

<br>

##### 对不同的层进行不同的初始化

eg:对第一层全连接层的权重参数使用Xavier初始化，第二个全连接层的权重初始化为42

```
def init_xavier(m):
    if type(m)==nn.Linear:
        nn.init.xavier_normal_(m.weight)

def inin_42(m):
    if type(m)==nn.Linear:
        nn.init.constant_(m.weight,42)

net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))
net[0].apply(init_xavier)
print(net[0].weight.shape,net[0].bias.shape)
net[2].apply(inin_42)
print(net[2].weight.shape,net[2].weight.data,net[2].bias.shape)
```

<br>

##### 自定义初始化

![image-20221029143144426](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221029143144426.png)

<br>

权重参数符合-5到5之间的均匀分布,并且

```python
def my_init(m):
    if type(m)==nn.Linear:
        print("Init",*[(name,param.shape) for name,param in m.named_parameters()][0])
        nn.init.uniform_(m.weight,-10,10)
        m.weight.data*=m.weight.data.abs()>=5

net.apply(my_init)
print(net[0].weight[:2])
```

<br>

###### 直接设置参数

```python
net[0].weight.data[:]+=1
net[0].weight.data[0,0]=22
```

<br>

<br>

##### 参数绑定

多个层之间共享参数：比如用一个全连接层的参数来设置其他层的参数

```python
shared=nn.Linear(8,8)
net=nn.Sequential(nn.Linear(4,8),
				nn.ReLU(),
				shared,
				nn.ReLU(),
				shared,
				nn.ReLU(),
				nn.Linear(8,1))

# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同⼀个对象，⽽不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])
```

<br>

<br>

<br>

##### 延后初始化

（1）定义了网络框架但是**没有指定输入维度**，或者添加层时**没有前一层的输出维度**，这刚初始化的时候没有足够的信息来确定模型参数的规格。

（2）框架的延后初始化（defers initialization）:直到数据**第⼀次通过模型传递时**，框架才会动态地推断出每个层的⼤⼩。等到**知道了所有的参数形状**，框架就可以初始化参数。

```python
net = nn.Sequential(
    nn.Linear(20, 256), nn.ReLU(),
    nn.LazyLinear(128), nn.ReLU(),
    nn.LazyLinear(10)
)
```

注：指定了第一层的输入尺寸，但没有指定后续层的尺寸，那么第一层将会立刻初始化，后面的层等第一次通过模型传递后再进行初始化。

<br>

<br>

<br>

#### 自定义层

<br>

###### 不带参数的层

eg:CenteredLayer类要从输⼊中减去均值。**继承基础层类**并实现向前传播

```python
class CenteredLayer(nn.Module):
    def __init__(self):
        #调用父类的构造函数
        super().__init__()

    def forward(self,X):
        return X-X.mean()

#检验
layer=CenteredLayer()
result=layer(torch.FloatTensor([1,2,3,4,5]))
print(result)
```

```python
#加入到其他层中构成更为复杂的模型
net=nn.Sequential(nn.Linear(4,8),CenteredLayer())
print(net(X))
```

<br>

###### 带参数的层

可以使用内置函数来创建参数，以便实现管理访问、初始化、共享、保存和加载模型参数等功能。

eg：自定义实现全连接层（参数一：表示权重；参数二：表示偏置项），并使用Relu激活函数

```python
class MyLinear(nn.Module):
    #输入数、输出数
    def __init__(self,in_units,units):
        super().__init__()
        self.weight=nn.Parameter(torch.randn(in_units,units))
        self.bias=nn.Parameter(torch.randn(units,))

    #定义向前传播
    def forward(self,X):
        linear=torch.matmul(X,self.weight.data)+self.bias
        return F.relu(linear)


linear=MyLinear(4,8)
print(linear.weight)
```

```python
#加入其他层构成更复杂的模型，并使用自定义的Mysequential类作为顺序块
net=MySequential(MyLinear(4,8),nn.Linear(8,2))
print(net(X))
```

<br>

<br>

<br>

#### 保存文件

###### 加载和保存张量

1.保存张量

```python
x=torch.arange(4)
y = torch.zeros(4)

torch.save(x,'x-file')
```

<br>

2.读取数据到内存

```python
if __name__=='__main__':
    x2=torch.load('x-file')
    print(x2)
```

<br>

3.存储、读取**张量列表**

```python
if __name__=='__main__':
    torch.save([x,y],'x-file')
    x2,y2=torch.load('x-file')
    print(x2,y2)
```

<br>

4.写⼊或读取**从字符串映射到张量**的**字典**

```python
if __name__=='__main__':
    mydict={'x':x,'y':y}
    torch.save(mydict,'mydict')
    mydict2=torch.load('mydict')
    print(mydict2)
```

<br>

###### 加载和保存模型参数

这将**保存模型的参数**⽽不是保存整个模型。因为模型本身的复杂性难以序列化。为了恢复模型，需要**先生成对应架构**，然后从磁盘**加载模型参数**。

eg：使用三层MLP模型20输入 256个隐藏单元 10个输出

```python
if __name__=='__main__':    
    net=MLP()
    X=torch.randn(size=(2,20))
    Y=net(X)

    #保存模型参数
    torch.save(net.state_dict(),'mlp_params')
```

```python
clone=MLP()
#加载模型参数
clone.load_state_dict(torch.load('mlp_params'))
#打印加载后的参数规格
print(clone.eval())
```

```python
Y_clone=clone(X)
Y_clone==Y
```

不同实例具有相同的模型参数，在输⼊相同的X时，两个实例的计算结果相同

<br>

##### 保存网络架构和参数

```python
model = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
torch.save(model, 'model.pt')
m = torch.load('model.pt')
print(m)
```

<br>

保存和加载某层模型的参数

```
# 如保存和加载隐藏层
torch.save(net.hidden.state_dict(), 'mlp.hidden.params')
clone = MLP()
clone.hidden.load_state_dict(torch.load('mlp.hidden.params'))
print(clone.hidden.weight == net.hidden.weight)
```

<br>

<br>

<br>

#### GPU

（1）在PyTorch中，每个数组都有⼀个**设备**（device），我们通常将其称为**上下⽂**（context）。

（2）在PyTorch中，CPU和GPU可以⽤**torch.device('cpu')** 和**torch.device('cuda')**表⽰。

（3）cpu设备意味着**所有物理CPU和内存**，这意味着PyTorch的计算将尝试使⽤**所有CPU核⼼**。然⽽，gpu设备 **只代表⼀个卡和相应的显存**。

（4）对多个GPU，使⽤**torch.device(f'cuda:{i}')** 来**表⽰**第i块GPU （i从0开始）。另外，cuda:0和cuda是等价的。

<br>

```
#查看gpu信息（命令行中）
nvidia-smi
```

<br>

```python
#查询可用gpu数量
print(torch.cuda.device_count())
```

<br>

##### 对于gpu和cpu的使用

```python
# 不存在gpu的时候运行代码
def try_gpu(i=0):  #@save

    # 如果存在gpu则返回gpu(i)，否则返回cpu
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus(): #@save

    #返回所有可用的gpu，如果没有gpu则返回cpu()
    devices=[torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]
    return devices if devices else  [torch.device('cpu')]
```

```python
print(try_gpu())
print(try_gpu(1))
print(try_all_gpus())
```

<br>

##### 张量和gpu

（1）查询张量所在的设备。**默认**情况下，张量是在**CPU**上创建的。

##### （2）对多个项进⾏操作时，它们都必须在同⼀个设备上。

（3）存储在gpu

```python
a=torch.tensor([1,2,3],device=try_gpu())
print(a)
print(a.device)
```

```python
tensor([1, 2, 3], device='cuda:0')
cuda:0
```

（4）**不同设备**上的张量进行运算需要进行复制

eg:运算X+Y，可用先将X复制到Y所在的设备再进行运算

```
Z = X.cuda(1)
Y+Z
```

（5）当**打印张量**或**将张量转换为NumPy**格式时，如果**数据不在内存**中，框架会⾸**先将其复制到内存中， 这会导致额外的传输开销。**

<br>

##### 神经网络与GPU

（1）神经网络可以指定设备

```python
net=Sequential(nn.Linear(3,1))
net=net.to(device=try_gpu())
```

确认参数位于哪个设备

```python
net[0].weight.data.device
```

<br>

（6）移动数据导致**显著性能下降**：计算GPU上**每个⼩批量的损失**，并在 命令⾏中将其报告给⽤⼾（或将其记录在NumPy ndarray中）时，将触发全局解释器锁，从⽽使所 有GPU阻塞。最好是为**GPU内部的⽇志分配内存**，并且**只移动较⼤的⽇志**。

<br>

<br>

<br>

<br>

### DAY 11

---

---

<br>

#### 卷积神经网络（convolutional neural network，CNN）

（1）背景：

**之前**对于图像数据的处理，是将三通道的二维像素值（彩色图像）**展平成一维向量**，**忽略了每个图像的空间结构信息。**再将数据送入**全连接**的多层感知机。由于**⽹络特征元素的顺序是不变**的，因此最优的结果是利⽤先验知识，即利⽤**相近像素之间的相互关联性**，从图像数据中学习得到有效的模型。

<br>

###### （2）多层感知机的应用和局限性：

多层感知机⼗分适合处理**表格数据**，其中⾏对应样本，列对应特征。**可能涉及特征之间的交互**，但是我们**不能预先假设任何与特征交互相关的先验结构**。但对于**⾼维感知数据**，这种缺少结构的⽹络可能会变得不实⽤。

<br>

（3）**空间不变性**：

基于卷积神经网络实现的模型能将**对位置信息不敏感**更加系统化，并用**较少的参数**来学习有用的表示。

- **平移不变性**：不管检测对象出现在图像中的哪个位置，神经⽹络的**前⾯⼏层** 应该**对相同的图像区域**具有**相似的反应**，即为“平移不变性”。**总结**：更换位置后，分类器依然能使用。
- **局部性**：神经⽹络的**前⾯⼏层**应该**只探索输⼊图像中的局部区域**，⽽**不过度在意图像中相隔 较远区域的关系**，这就是“局部性”原则。最终，可以**聚合这些局部特征**，以在整个图像级别进⾏预测。**总结**：只需要局部的信息来用于分类。

<br>

<br>

#### 由全连接层到卷积层

<br>

### 整体认知（#验证数据流动）:

- 对于以下结构的MLP

```
X=torch.randn(size=(1,4))

net=Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,2))		#每一层两个参数（输入数，输出数）
```

输入层单元数：4

全连接层单元数（使用ReLU激活函数）：8		输入层到全连接层之间的权重数目：4*8=32

输出层单元数（全连接层）：2		全连接层到输出层之间的权重数目：8*2=16

<br>

###### 神经网络图：

![image-20221030181920802](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221030181920802.png)

注：图中对应的每根线就是一个权重参数（为了方便，省去了Relu的实现）

<br>

拓展1：

**X是一个向量**,形状是[4]，维度是1。要进入神经网络需要**转换成二维**

```python
X=torch.tensor([1,2,3,4])
print(X.shape)
print(X)
print(X.ndim)

X=X.reshape((1,4))
print(X.shape)
print(X)
print(X.ndim)
```

```python
torch.Size([4])
tensor([1, 2, 3, 4])
1
torch.Size([1, 4])
tensor([[1, 2, 3, 4]])
2
```

<br>

##### 实验一：输入数据为向量

```python
#验证数据流动：
X=torch.tensor([1.,2.,3.,4.])
# Y=torch.randn(size=(1,4))
# print(Y)
X=X.reshape((1,4))


net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,2))
#权重初始化为1,偏置初始化为0
net.apply(init_constant)
# net(X)
print(net)
print(net(X))
print(net[0].weight.data, net[0].bias.data)
print(net[2].weight.data, net[2].bias.data)
```

```python
Sequential(
  (0): Linear(in_features=4, out_features=8, bias=True)
  (1): ReLU()
  (2): Linear(in_features=8, out_features=2, bias=True)
)
tensor([[80., 80.]], grad_fn=<AddmmBackward0>)
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]]) tensor([0., 0., 0., 0., 0., 0., 0., 0.])
tensor([[1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1.]]) tensor([0., 0.])
```

**解析**：

输入层：4个输入单元分别为1.、2.、3.、4.

隐藏层：8个单元

##### 由输入层到隐藏层的权重参数形状是(4,8)，由于nn.Linear中创建权重是按照（输出数，输入数）创建的，实际是如上面的(8,4)但运算的时候会进行转置，所以之间当作（输入，输出）来计算就行。元素都是1，偏差为0

```python
1*4的矩阵和4*8的矩阵进行矩阵乘法（内积）得到1*8的矩阵每个元素都是8
```

<br>

输出层：2个单元

隐藏层到输出层的权重参数形状是(8*2)，同上。

```python
1*8的矩阵和8*2的矩阵进行内积，得到1*2的矩阵，元素都是80
```

---

##### 实验二：输入数据为二维张量

##### 解析：

![IMG_20221030_205246](C:\Users\china\AppData\Roaming\Typora\typora-user-images\IMG_20221030_205246.jpg)

<br>

（1）对于全连接层：将输入输出变换为**矩阵**（高度、宽度）

（2）对于**四维权重张量**的理解：w_{i,j,k,l}中，i,j代表输出的点在输出矩阵中的位置，k,l代表输入点在输入的图（或者矩阵）中的位置。那么这个权重矩阵应该记录**输入中的每一个点对于输出中的每一个点的影响**（也就是权重）。举例来说，比如输入图是4x4的，输出图是2x2的。我需要记录输入图中(1,1), (1,2), ..., (2,1), ..., (4,4)这些所有的点对输出图(1,1)的影响，同理也需要记录这些所有点对输出图中(1,2), ...., (2,2)的影响。那么这时候对于每一组点就有4个参数：**输入图的横坐标、纵坐标**，**输出图的横坐标、纵坐标**。所以要想完全**记录所有的权重**，需要一个4维张量。

（3）**对多层感知机的限制**：

- 原则一：平移不变性

输入X的平移导致V权重的变化进而引起输出h的平移，**v（权重）的规格（大小）不应该依赖于输入**。解决方案：对于权重V将前两个维度（输入的维度）抹掉。

![image-20221031223538542](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221031223538542.png)

这就是2维卷积（2维交叉相关）

##### 对模型参数的取值范围做出限制，从而降低模型复杂度

<br>

- 原则二：局部性

要得出输出h，只需要看输入X附近的参数就行了

![image-20221031223843905](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221031223843905.png)



##### 对模型参数的取值范围做出限制，从而降低模型复杂度

<br>

##### 对全连接层使用平移不变性和局部性得到卷积层（卷积层是特殊的全连接层）

<br>

<br>

<br>

<br>

### DAY 12

---

---

##### 互相关运算（cross-correlation）

（1）在卷积层中，输入**张量**和**核张量**通过**互相关运算**产生**输出张量**。

（2）具体过程：

在⼆维互相关运算中，卷积窗⼝从输⼊张量的**左上⻆开始，从左到右、从上到下滑动**。当卷积窗⼝滑动到新 ⼀个位置时，包含在该窗⼝中的部分张量与卷积核张量进⾏**按元素相乘**，得到的张量**再求和得到⼀个单⼀的 标量值**，由此我们得出了这⼀位置的输出张量值。

（3）**运算过程中卷积核不变即平移不变性，而输出只取决于由卷积核所框定的区域，即局部性。**

#### （4）互相关运算和卷积层的实现

```python
#互相关运算
def corr2d(X,K): #@save
    #获取卷积核高、宽
    h,w=K.shape
    Y=torch.zeros((X.shape[0]-h+1,X.shape[1]-w+1))
    for i in range(Y.shape[0]):             #根据输出的规格来按照元素进行互相关运算。就能够实现对于输入的部分元素的舍弃（不进行卷积运算）如3*3和2*2的核进行互相关运算，最后一列无法进行
        for j in range(Y.shape[1]):
            Y[i,j]=(X[i:i+h,j:j+w]*K).sum()
    return Y

#卷积层
class Conv2D(nn.Module):
    def __init__(self,kernel_size):
        super.__init__()
        self.weight=nn.Parameter(torch.rand(kernel_size))
        self.bias=nn.Parameter(torch.zeros(1))

    def forward(self,X):
        return corr2d(X,self.weight)+self.bias
```

（5）由卷积运算得到的输出被称为：**特征映射**（feature map），被视为⼀个输 ⼊映射到下⼀层的空间维度的转换器。对于某⼀层的任意元素x，其**感受野**（receptive field）是指在前向传播期间可能影响x计算的所有元素（来⾃**所有先前层**）。如(3,3)输入,(2,2)核得到(2,2)输出，输出的其中一个元素的感受野就是2*2=4

##### （6）卷积的本质是有效提取相邻像素间的相关特 征

<br>

###### 卷积层的应用

###### 1.图像中目标边缘检测

使用卷积核[[1.,-1.]]

<br>

##### 学习卷积核

在**端对端**的学习过程中**学习生成卷积核**

eg：

基本流程：将卷积核初始化为随机张量，每次迭代比较输出和卷积层输出（目标函数的输出，这里设置的卷积核为[[1.,-1.]]）的平方损失，然后计算梯度更新卷积核

```python
X=torch.ones((6,8))
X[:,2:6]=0
K=torch.tensor([[1.0,-1.0]])
Y=corr2d(X,K)

#批量大小，通道数、高度、宽度
myconv2d=nn.Conv2d(1,1,kernel_size=(1,2),bias=False)

X=X.reshape((1,1,6,8))
Y=Y.reshape((1,1,6,7))
#学习率
lr=3e-2

#迭代学习
for i in range(10):
    Y_hat=myconv2d(X)
    #平方损失
    l=(Y_hat-Y)**2

    #梯度清零
    myconv2d.zero_grad()
    l.sum().backward()

    myconv2d.weight.data[:]-=lr*myconv2d.weight.grad
    if (i+1)%2==0:
        print(f'epoch {i+1},loss {l.sum():.3f}')


    print(myconv2d.weight.data)

```

<br>

输入X:nh*nW

核：kh*kw

偏差：b∈R

输出Y：（nh-kh+1）*（nw-kw+1）

输出是<=输入的，因为有的时候核不能完全覆盖输入会丢弃一些参数，如输入(3,3)，核(2,2)，这时**最后一列参数**不能进行互相关运算。（详细看**卷积运算的实现**）

Y=X*W+b

W,b都是可学习的参数。

<br>

###### 交叉相关和卷积

![image-20221031230709005](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221031230709005.png)

卷积在索引权重W的时候是反着走的。

<br>

一维交叉相关

![image-20221031230948463](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221031230948463.png)

<br>

三维交叉相关

![image-20221031230943304](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221031230943304.png)

<br>

<br>

<br>

<br>

#### DAY 13

---

---

##### 填充和步幅

1.**填充（padding）**：主要是应对原始图像在进行卷积运算（多数是连续卷积）的时候会丢失边界的信息。而随着模型深度的加深，损失的信息会越来越多，到后面就无法进行传递。

- 解决方法：在图像边界填充元素（通常是0）

- **填充过程**：如果填充ph行（大约一半在上半部分，一半在底部），pw列（一半在左边，一半在右边），此时输出形状为（nh-kh+ph+1）*（nw-kw+pw+1）。输出将增加ph行，pw列。

因此通常设置ph=kh-1，pw=kw-1以使输出和输入形状一样。

- 当kh是奇数（-1，ph为偶数），将在上、下部分分别填充ph/2；kh是偶数（-1，ph为奇数），在输⼊顶部填充⌈ph/2⌉⾏（**等于大于**自己的最小整数），在底部填充⌊ph/2⌋（**等于小于**自己的最大整数）

- 通常卷积核的kh、kw设置为奇数。并且对于任何⼆维张量X，当满⾜：1. 卷积核的⼤⼩ 是奇数；2. 所有边的填充⾏数和列数相同；3. 输出与输⼊具有相同⾼度和宽度则可以得出：**输出Y[i, j]是 通过以输⼊X[i, j]为中⼼，与卷积核进⾏互相关计算得到的**。

eg1：

对于输入为(8,8)，核（3,3），要使输出和输入形状一样，总填充为2，各边分别填充1

```python
def comp_conv2d(conv2d,X):
    X=X.reshape((1,1)+X.shape)
    print(X.shape)
    Y=conv2d(X)
    print(Y.shape)

    #忽略前两个维度
    return Y.reshape(Y.shape[2:])
    
myconv2d=nn.Conv2d(1,1,kernel_size=3,padding=1)
X=torch.rand(size=(8,8))
comp_conv2d(myconv2d,X).shape
```

<br>

eg2:

卷积核高、宽不同，（5，3），这时ph=5-1=4，上下各填充2，pw=3-1=2，上下各填充1，使得输出和输入形状一样

```python
myconv2d=nn.Conv2d(1,1,kernel_size=(5,3),padding=(2,1))
X=torch.rand(size=(8,8))
comp_conv2d(myconv2d,X).shape
```

<br>

补充：

```
torch.rand. 返回服从均匀分布的初始化后的tenosr，外形是其参数size
```

<br>

##### 维度拓展

```python
#加上样本数和通道数两个维度,原本是（8，8）变成（1，1，8，8），卷积运算后仍然是（1，1，8，8）
X=X.reshape((1,1)+X.shape)
print(X.shape)
Y=conv2d(X)
print(Y.shape)
```

<br>

2.**步幅（stride）**：默认步长为1。有时候为了**⾼效计算**或是**缩减采样次数**，卷积窗⼝可以跳过中间位置，每次滑动多个元素。

（1）当垂直步幅为sh、⽔平步幅为sw时，输出形状为 **⌊(nh − kh + ph + sh)/sh⌋ × ⌊(nw − kw + pw + sw)/sw⌋.** 

（2）设置了ph = kh − 1和pw = kw − 1后，为**⌊(nh + sh-1)/sh⌋ × ⌊(nw + sw-1)/sw⌋.** 

（3）同时输⼊的⾼度和宽度可以被垂直和⽔平步幅**整除**，则输出形状将为**(nh/sh) × (nw/sw)。**

eg1：

输入(8,8)，核（5,3），填充(2,1)，步幅2

```python
myconv2d=nn.Conv2d(1,1,kernel_size=(5,3),padding=(2,1),stride=2)
X=torch.rand(size=(8,8))
comp_conv2d(myconv2d,X).shape
```

```python
torch.Size([1, 1, 8, 8])
torch.Size([1, 1, 4, 4])
```

<br>

eg2:

输入(8,8),核（3，5），填充（0，1），步幅（3，4）

```python
myconv2d=nn.Conv2d(1,1,kernel_size=(3,5),padding=(0,1),stride=(3,4))
X=torch.rand(size=(8,8))
comp_conv2d(myconv2d,X).shape
```

```python
torch.Size([1, 1, 8, 8])
torch.Size([1, 1, 2, 2])
```

<br>

###### 3.填充和步幅的总结

- 当输⼊⾼度和宽度两侧的填充数量分别为ph和pw时，我们称之为填充(ph, pw)。当ph = pw = p时，**填充是p**。同理，当⾼度和宽度上的步幅分别为sh和sw时，我们称之为步幅(sh, sw)。特别地，当sh = sw = s时， 我们称**步幅为s**。默认情况下，填充为0，步幅为1。在实践中，我们很少使⽤不⼀致的步幅或填充，也就是说， 我们通常有ph = pw和sh = sw。

- 填充和步幅可⽤于有效地调整数据的维度。

<br>

<br>

##### 多输入多输出通道

- ##### 多输⼊多输出通道可以⽤来扩展卷积层的模型。

（1）**多通道输入**：输入为多通道，对应的卷积核也需要是多通道（和输入的通道数相同）的

```python
#单通道输出，输入：通道数、高、宽（2，3，3）  核：通道数、高、宽（2，2，2） 每个通道得到的输出再求和

def corr2d_multi_in(X,K):
    #按照通道维度进行卷积运算（互相关运算），再相加
    return sum(corr2d(X,K) for X,K in zip(X,K))

X=torch.arange(18)
#通道数2
X=X.reshape((2,3,3))
K=torch.arange(8)
K=K.reshape((2,2,2))

result=corr2d_multi_in(X,K)
print(result)
```

```python
tensor([[268., 296.],
        [352., 380.]])
```

<br>

（2）**多通道输出**：随着神经⽹络层数的加深，我们常会**增加输出通道的维数**，通过**减少空间分辨率**以获得更⼤的通道深度。

具体表示：**⽤ci和co分别表⽰输⼊和输出通道的数⽬，并让kh和kw为卷积核的⾼度和宽度。为了获得多个通道的输出，我 们可以为每个输出通道创建⼀个形状为ci × kh × kw的卷积核张量，这样卷积核的形状是co × ci × kh × kw。**

 ![image-20221101165944939](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221101165944939.png)

<br>

##### torch.stack

- stack函数可以**保留–[序列(先后)信息] 和 [张量的矩阵信息]** 
- 对**序列数据**内部的张量进行**扩维拼接**

- 沿着一个新维度对输入张量**序列**进行连接。 序列中所有的张量都应该为**相同形状**。
  假如数据都是二维矩阵(平面)，它可以把这些一个个平面按第三维(例如：时间序列)压成一个三维的立方体，而立方体的长度就是时间序列长度。

```python
torch.stack

outputs = torch.stack(inputs, dim=?) → Tensor

inputs : 待连接的张量序列。
注：python的序列数据只有list和tuple。

dim : 新的维度， 必须在0到len(outputs)之间。
注：len(outputs)是生成数据的维度大小，也就是outputs的维度值。
```

<br>

##### torch.cat

在**给定维度**上对输入的张量序列seq 进行连接操作

```python
outputs = torch.cat(inputs, dim=?) → Tensor

inputs : 待连接的张量序列，可以是任意相同Tensor类型的python 序列
dim : 选择的扩维, 必须在0到len(inputs[0])之间，沿着此维连接张量序列。
```

<br>

```python
def corr2d_multi_in_out(X,K):
    #按照K的通道维度对输入X进行互相关运算，最后进行叠加
    for k in K:
        print(k)
    #K的0维度是输出通道数1维度是输入通道数
    return torch.stack([corr2d_multi_in(X,k) for k in K],0)

X=torch.arange(18)
#通道数2
X=X.reshape((2,3,3))
K=torch.arange(8)
K=K.reshape((2,2,2))
# print(K)
#⭐输出通道数为3
K=torch.stack((K,K+1,K+2),0)
# print(K.shape)   （3，2，2，2）
# print(K)

result=corr2d_multi_in_out(X,K)
print(result)
```

```python
#第一个输出和上面的单通道输出一样
tensor([[[268., 296.],
         [352., 380.]],

        [[320., 356.],
         [428., 464.]],

        [[372., 416.],
         [504., 548.]]])
```

<br>

##### 1*1卷积层

- ##### 1 × 1卷积层通常⽤于调整⽹络层的通道数量和控制模型复杂性。

- 不识别空间模型，只是**融合通道**

输出中的每个元素都是从输⼊图像中同⼀位置的元素的线性组合。1 × 1卷积层可以看作是在 **每个像素位置应⽤的全连接层**，以ci个输⼊值转换为co个输出值。

![image-20221101153206758](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221101153206758.png)

注：上图可帮助理解多通道输入输出

```python
def corr2d_multi_in_out_1x1(X,K):
    #输入通道，高、宽
    c_i,h,w=X.shape
    #输出通道
    c_o=K.shape[0]
    #⭐先将每个通道展平成一个向量，再将每个通道所代表的向量组合起来形成矩阵
    X=X.reshape((c_i,h*w))
    K=K.reshape((c_o,c_i))
    
    #矩阵乘法
    Y=torch.matmul(K,X)
    return Y.reshape((c_o,h,w))

X=torch.normal(0,1,(3,3,3))
#输出通道数是2,输入通道数是3
K=torch.normal(0,1,(2,3,1,1))
Y1=corr2d_multi_in_out(X,K)
Y2=corr2d_multi_in_out_1x1(X,K)
print(Y1)
print(Y2)
assert float(torch.abs(Y1-Y2).sum())< 1e-6
```

```python
tensor([[[ 1.5829,  2.4119, -0.7918],
         [ 3.1293,  3.2417, -0.8101],
         [ 1.5471, -2.8363,  0.3602]],

        [[ 0.2554,  1.2584,  0.8679],
         [ 0.4446, -1.3118,  2.8172],
         [ 3.4283, -0.0272,  2.4191]]])
tensor([[[ 1.5829,  2.4119, -0.7918],
         [ 3.1293,  3.2417, -0.8101],
         [ 1.5471, -2.8363,  0.3602]],

        [[ 0.2554,  1.2584,  0.8679],
         [ 0.4446, -1.3118,  2.8172],
         [ 3.4283, -0.0272,  2.4191]]])
```

<br>

#### ⭐对于多输入多输出通道的理解：

**每个输出通道**相当于**一种特定的识别模式**，比如垂直边缘，水平边缘，颜色，纹理，锐化等等，这种识别模式就对应**一种卷积核**，由这个卷积核所提取的就是对应的识别模式的**一种特征**。**多输出通道**就意味着该卷积层能提取一张图片的**多个特征**。接着对于这些特征，分别赋予不同的权重再进行组合，得到组合的模式识别。

<br>

对于一个深度神经网络来说，**较上面**的会识别整只猫，**较中间**的一些层会识别图像的部分（如猫的头），**较底层**的会识别一些纹理（如猫的胡须的纹理）

<br>

##### 汇聚层(pooling)

###### （1）存在的问题：

- 在处理图像的时候需要**逐渐降低**隐藏表⽰的**空间分辨率**、**聚集信息**，这样随着我们在神经⽹络中 层叠的上升，每个神经元对其敏感的**感受野（输⼊）就越⼤**。

- 机器学习任务通常会跟**全局图像**的问题有关（例如，“图像是否包含⼀只猫呢？”），所以我们**最后⼀ 层**的神经元应该**对整个输⼊的全局敏感**。通过**逐渐聚合信息**，⽣成越来越粗糙的映射，最终实现学习全局表 ⽰的⽬标，同时将卷积图层的所有优势保留在中间层。

- 当检测**较底层**的特征时（例如 6.2节中所讨论的**边缘**），我们通常希望这些特征保持某种程度上的**平移 不变性**。

（2）汇聚层的**双重目的**：：**降低卷积层对位置的敏感性，同时降低对空间降采样表⽰ 的敏感性。**

<br>

##### 最大汇聚层和平均汇聚层

- 池化过程：汇聚层运算符由⼀个固定形状的窗⼝组成，该窗⼝根据其步幅⼤⼩在输⼊的所有区域上滑动， 为固定形状窗⼝（有时称为汇聚窗⼝）遍历的每个位置计算⼀个输出。

- 计算**汇聚窗⼝中所有元素**的 **最⼤值**或**平均值**。这些操作分别称为最⼤汇聚层（maximum pooling）和平均汇聚层（average pooling）。

- ##### 使⽤最⼤汇聚层以及⼤于1的步幅，可减少空间维度（如⾼度和宽度）。

```python
#汇聚层
def pool2d(X,pool_size,mode='max'):
    p_h,p_w=pool_size
    Y=torch.zeros((X.shape[0]-p_h+1,X.shape[1]-p_w+1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode=='max':
                Y[i,j]=X[i:i+p_h,j:j+p_w].max()
            elif mode=='avg':
                Y[i,j]=X[i:i+p_h,j:j+p_w].mean()
    return Y


X=torch.arange(9.)
X=X.reshape((3,3))
result=pool2d(X,(2,2))
print(result)

result2=pool2d(X,(2,2),'avg')
print(result2)

X2=torch.ones((6,8))
X2[:,2:6]=0
K=torch.tensor([[1.0,-1.0]])
result3=corr2d(X2,K)
print(result3)
result4=pool2d(result3,(2,2))
print(result4)
```

```python
tensor([[4., 5.],
        [7., 8.]])
tensor([[2., 3.],
        [5., 6.]])
tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])
tensor([[1., 1., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.]])
```

<br>

##### ⭐小技巧：

```python
X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))

tensor([[[[ 0., 1., 2., 3.],
		[ 4., 5., 6., 7.],
		[ 8., 9., 10., 11.],
		[12., 13., 14., 15.]]]])
```

<br>

##### 在输入nn的模型时需要将输入参数转换成4维的，即（样本数、通道数、高、宽）

<br>

##### 汇聚层的填充和步幅

默认步幅是(3,3)；手动设定

```python
pool2d = nn.MaxPool2d(3)

pool2d = nn.MaxPool2d(3, padding=1, stride=2)

pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))
```

<br>

##### 多通道与汇聚层

处理多通道输⼊数据时，汇聚层在**每个输⼊通道上单独运算**，⽽不是像卷积层⼀样在通道上对输⼊进⾏汇 总。

### 即输出通道数和输入通道数一样。

```python
X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))
X=torch.cat((X,X+1),1)
print(X.shape)
print(X)
mypool2d=nn.MaxPool2d(3,padding=1,stride=2)
result=mypool2d(X)
print(result)
```

```python
torch.Size([1, 2, 4, 4])
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],

         [[ 1.,  2.,  3.,  4.],
          [ 5.,  6.,  7.,  8.],
          [ 9., 10., 11., 12.],
          [13., 14., 15., 16.]]]])
tensor([[[[ 5.,  7.],
          [13., 15.]],

         [[ 6.,  8.],
          [14., 16.]]]])
```

<br>

<br>

<br>

<br>

#### DAY 14

##### 卷积神经网络（LeNet）

1.用卷积层代替全连接层，**好处**：能够保存图像中的空间结构；能够减少参数

2.应用：ATM机器处理识别钞票数字

3、LeNet（**LeNet-5**）结构主要由两部分组成：

- 卷积编码器：由**两个**卷积层组成; 

（1）每个卷积块中的基本单元是⼀个**卷积层**、⼀个**sigmoid激活函数**和**平均汇聚层**。

（2）每个卷积层使⽤**5 × 5**卷积核和⼀个sigmoid激活函数。这 些层将输⼊映射到多个⼆维特征输出，通常同时增加通道的数量。第⼀卷积层有**6个输出通道**，⽽第⼆个卷 积层有**16个输出通道**。每个**2 × 2池操作（步幅2）**通过空间下采样将维数减少4倍（高减少2倍，宽减少两倍，整个减少2*2=4倍）。卷积的输出形状由批量⼤ ⼩、通道数、⾼度、宽度决定。

<br>

- 全连接层密集块：由**三个**全连接层组成。

（1）LeNet的稠密块有三个全连接层，分别有120、84和10个输出（10是在应用于分类的时候对于的类别数）。

（2）**⼩批量中展平每个样本**，以将卷积块的输出传递给全连接层（⭐**4维-->2维**）。

<br>

<br>

```python
loss 2.316,train acc 0.103,test acc 0.166
loss 1.327,train acc 0.482,test acc 0.568
loss 0.843,train acc 0.668,test acc 0.674
loss 0.700,train acc 0.727,test acc 0.698
loss 0.635,train acc 0.753,test acc 0.762
loss 0.590,train acc 0.771,test acc 0.753
loss 0.549,train acc 0.789,test acc 0.793
loss 0.515,train acc 0.803,test acc 0.778
loss 0.492,train acc 0.813,test acc 0.754
loss 0.469,train acc 0.823,test acc 0.822
```

![image-20221105210210965](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221105210210965.png)

<br>





#### DAY 15

##### 对LeNet-5的模型进行改变：

将激活函数全化为Relu()

![image-20221105210814750](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221105210814750.png)

将sigmoid全部换成relu后：学习率为0.9，基本不收敛

换为0.1后收敛

```python
loss 0.332,train acc 0.877,test acc 0.863
```

![image-20221105222703292](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221105222703292.png)

<br>

<br>

将卷积块的激活函数换为**Relu（）**:

```python
loss 1.152,train acc 0.557,test acc 0.694
loss 0.619,train acc 0.763,test acc 0.694
loss 0.492,train acc 0.816,test acc 0.778
loss 0.434,train acc 0.838,test acc 0.833
loss 0.392,train acc 0.854,test acc 0.851
loss 0.367,train acc 0.864,test acc 0.833
loss 0.346,train acc 0.871,test acc 0.788
loss 0.333,train acc 0.875,test acc 0.857
loss 0.317,train acc 0.882,test acc 0.863
loss 0.308,train acc 0.885,test acc 0.870
```

##### 训练损失大幅降低：

- ##### 减少计算量、模型复杂度降低

- ##### 当sigmoid激活函数的输出⾮常接近于0或1时，这些区域的梯度⼏乎为0，因此反向 传播⽆法继续更新⼀些模型参数。相反，ReLU激活函数在正区间的梯度总是1。因此，如果模型参数没有正 确初始化，sigmoid函数可能在正区间内得到⼏乎为0的梯度，从⽽使模型⽆法得到有效的训练。

![image-20221105211349244](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221105211349244.png)

<br>

#### DAY 16







#### DAY 17

Conv3d以及MaxPool3d一般用于处理三维数据：视频（每一帧图片再加上时间轴）、医学图像（3D的，平面俯视来看就是一张张图片，由高度堆叠而成、气象图片）

<br>

##### 学习表征

种预测这个领域发展的⽅法————观察图像特征的提取⽅法。

###### 对应的发展历程：

**设计**⼀套新的特征函数、改进结果——>特征本⾝应该**被学习**,在合理地复杂性前提下， 特征应该由多个共同学习的神经⽹络层组成，每个层都有可学习的参数（在计算机视觉中，最底层的特征识别模式可能是：检测边缘、颜色、纹理）。

<br>

##### 深度卷积神经⽹络突破的两个关键因素：

- 数据
- 硬件

<br>

#### AlexNet（2012）

1.它**⾸次**证明了学习到的特征可以超越⼿⼯设计的特征。

2.在AlexNet⽹络的最底层，模型**学习**到了⼀些类似于**传统滤波器**的**特征抽取器**。

（1）**总述**：AlexNet的**更⾼层**建⽴在这些**底层表⽰的基础上**，以表⽰**更⼤的特征**，如眼睛、⿐⼦、草叶等等。⽽**更⾼的层 可以检测整个物体**，如⼈、⻜机、狗或⻜盘。**最终**的隐藏神经元**可以学习图像的综合表⽰**，从⽽使属于不同 类别的数据易于区分。

<br>

**AlexNet**模型由8层网络组成：

- 5个卷积层

第1层卷积层卷积核形状**（10，10）**，第2层卷积层**（5，5）**，第3、4、5层都是**（3，3）**。然后在第**1、2、5**层卷积层后加上**（3，3），步幅为2的最大池化层**。

- 2个全连接隐藏层
- 1个全连接层

- 全部使用Relu()作为**激活函数**。

- ###### 容量控制和预处理

（1）使用**Dropout**控制**全连接层的模型复杂度**（LeNet只有权重衰减）

（2）为了进⼀步扩充数 据，AlexNet在训练时**增加了⼤量的图像增强数据**，如**翻转**、**裁切**和变⾊。这使得**模型更健壮**，更⼤的样本量 有效地**减少了过拟合**。

<br>

###### 自己写的模型和训练函数：

训练结果：

```python
loss 0.660,train acc 0.756,test acc 0.789
```

![image-20221106210859630](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221106210859630.png)

<br>

哦，原来是把训练集和测试集搞反了

```python
loss 0.328,train acc 0.880,test acc 0.882
```

![image-20221106235917091](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221106235917091.png)

<br>

<br>

<br>

#### DAY 18

#### VGG（使用块的网络）

<br>

单个**VGG块**主要由个部分组成：

- 卷积层、池化层
- 全连接层
- 超参数变量**conv_arch**。该 变量指定了**每个VGG块**⾥**卷积层个数**和**输出通道数**。

<br>

##### VGG-11

（1）第⼀个**模块**有64个 输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。

（2）该⽹络络有5个卷积块,使⽤8个卷积层和3个全连接 层，因此它通常被称为VGG-11。

```python
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
```

```
运算：//，商向下取整
```

<br>

运行结果：

```python
loss 0.195,train acc 0.928,test acc 0.918
loss 0.181,train acc 0.932,test acc 0.916		#最后一次有所下降
```

![image-20221108000000682](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221108000000682.png)

<br>

总结：

**深层且窄**的卷积（即3 × 3）⽐较浅层且宽的卷积更有效。

<br>

**对于LeNet、AlexNet、VGG网络的小结：**

通过⼀系列的**卷积层与汇聚层**来**提取空间结构特征**；然后 通过**全连接层**对**特征的表征**进⾏**处理**。AlexNet和VGG对LeNet的改进主要在于如何**扩⼤和加深这两个模块**。

<br>

<br>

<br>

#### DAY 19

##### NiN

1.使⽤全连接层，可能会完全放弃表征的空间结构。⽹络中的⽹络（NiN）提供了⼀个⾮常简单的解决⽅案：**在每个像素的通道上分别使⽤多层感知机**

<br>

2.（1）**总述**：在**每个像素位置**（针对每个⾼度和宽度）**应⽤⼀个全连接层**。如果我们**将权重连接到每个空间位置**，我们可以将其视为**1 × 1卷积层**，即将空间维度中的**每个像素**视为**单个样本**，将**通道维度**视为**不同特征**（feature）。

（2）**之前的结构**：卷积层的输⼊和输出由四维张量组成，张量的每个轴分别对应**样本、通道、⾼度和宽度**。另外， 全连接层的输⼊和输出通常是分别对应于**样本和特征的⼆维张量**。

- ##### 卷积层后的第一个全连接层参数过多，容易过拟合。

<br>

##### NiN块结构：

- 卷积层
- 两个卷积核为1*1的卷积层（带有RelU()激活函数，充当逐像素的全连接层），**后一个卷积层为每个像素增加了非线性性（因为包含了RelU激活函数）**

##### NiN网络结构：

- NiN块后面接**最大池化层**，最后接**全局平均池化层**
- NiN使⽤包含窗⼝形状为**11×11、5×5和3× 3**的卷积层的NiN块，输出通道数量与AlexNet中的相同。**每个NiN块后有⼀个最⼤汇聚层**，汇聚窗⼝形状为**3 × 3，步 幅为2**。最后放⼀个**全局平均汇聚层**（global average pooling layer），⽣成⼀个**对数⼏率**（logits）。 NiN设计的⼀个**优点**是，它**显著减少了模型所需参数的数量**。然⽽，在实践中，这种设计有时会**增加训练模 型的时间。**

<br>

特点：

- NiN去除了容易造成过拟合的全连接层，将它们替换为**全局平均池化层**（即在所有位置上进⾏求和）

- **移除全连接层可减少过拟合**，同时**显著减少NiN的参数。**

- ##### 使用全局平均池化的缺点：收敛的速度变慢。

<br>

结果：

```python
loss 0.338,train acc 0.875,test acc 0.845
total time: [654.5977079868317] s
```

###### 出现过拟合

![image-20221109191506486](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221109191506486.png)

<br>

<br>

### 不定周期所积累的问题的和解决

**问题：\1. 参数的数量是多少？\2. 计算量是多少？ 3. 训练期间需要多少显存？ 4. 预测期间需要多少显存？**

<br>

##### enumerate

将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时**列出数据和数据下标**

```python
>>> seq = ['one', 'two', 'three']
>>> for i, element in enumerate(seq):
		print i, element
		
0 one
1 two
2 three
```

<br>

##### 查看每次读取的数据的规格

```python
for i,(X,y) in enumerate(train_iter):
    print("X_shape：",X.shape)
```

<br>

##### 分类问题的softmax运算在损失函数中已经实现了

```python
#CrossEntropyLoss的具体过程
    #x为三个样本经过神经网络最后的输出
    x = torch.tensor([[1.435,2.223,3.353],
                    [4.345,5.2231,6.389],
                    [7.33,8.21,9.36]])

    #三个样本的真实标签
    y = torch.tensor([0, 1, 1])
    print(len(x))
    print(x)
    #先进行softmax运算变换到0-1之间
    x_softmax=softmax(x)
    print(x_softmax)
    #取对数
    x_log=torch.log(x_softmax)
    print(x_log)

    #按照真实标签：y所；取得每个样本对应的真实标签的概率
    loss=x_log[range(len(x)),y]
    # print(range(len(x)))
    # print(loss)
    loss=abs(sum(loss)/len(x))
    print(loss)

    cros=nn.CrossEntropyLoss()
    print(cros(x,y))

```

<br>

<br>

<br>

#### DAY 20

#### 含并⾏连结的⽹络（GoogLeNet）

- ⼀个重点是解决了什么样⼤ ⼩的卷积核最合适的问题。有时使⽤不同⼤⼩的卷积核组合是有利的

<br>

##### 1.Iception块

（1）

- 块由四条并⾏路径组成。前三条路径使⽤窗⼝⼤⼩为1 × 1、3 × 3和5 × 5的卷积层， 从不同空间⼤⼩中提取信息。中间的两条路径在输⼊上**先执⾏1 × 1卷积**，以减少通道数，从⽽降低模型的复杂 性。第四条路径使⽤3 × 3最⼤汇聚层，然后使⽤1 × 1卷积层来改变通道数。
- 四条路径都使⽤合适的填充 来使**输⼊与输出的⾼和宽⼀致**，最后我们将每条线路的输出在**通道维度上连结**，并构成Inception块的输出。

（2）GoogLeNet有效的原因：滤波器（filter）的组合，它们可以⽤**各种滤 波器尺⼨**探索图像，这意味着**不同⼤⼩的滤波器**可以有效地识别不同范围的图像细节。同时对不同filter**分配不同的参数**。

<br>

##### 2.GoogLeNet模型：

- 9个**Inception块**和**全局平均池化层**的堆叠来⽣成其估计值。

- Inception块 之间的**最⼤汇聚层**可**降低维度**。

- 全局平均汇聚层避免了在最后使⽤全连接层（减少参数，降低模型复杂度）。

<br>

```python
loss 0.246,train acc 0.906,test acc 0.874
total time:  7.955962697664897  min
```

![image-20221110213201061](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221110213201061.png)

<br>

<br>

<br>

#### DAY 21

### 批量归一化

##### 问题提出：神经网络层越来越深，浅层的梯度较大，越深的梯度越小（对权重的更新就越少），浅层的部分收敛的较快，深层收敛较慢。深层的参数进行变换的时候会导致浅层不断的跟随重新进行学习，导致整个过程的学习变慢。（线性变换）

批量归一化就是在解决这个问题。

<br>

##### 批量归一化主要思路：

使每一层的参数分布固定在一定范围，使得每层的**输入或者输出**都符合某一个分布，

<br>

#### ⭐批量归一化所作的工作：

- ##### 本质上是在每个批量里面加入噪音来控制模型复杂度。

![image-20221110215412594](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221110215412594.png)

- ##### 这里的批量均值和批量标准差是在每次的随机批量上计算得来的，是噪音项，减和除分别做了 随机偏移 和 随机缩放。

- 没必要跟丢弃法混合使用（**都是控制模型复杂度**）

<br>

##### ⭐总结：

- 批量归一化**固定小批量中的均值和方差，**然后学习出适合的偏移和缩放。

- 可以加速收敛，但一般不改变模型精度（可以调大学习率）。

<br>

<br>

1.特点：

（1）**可持续加速深层⽹络 的收敛速度**。**结合残差块**，批量规范化使得研究⼈员能够训练100层以上的⽹络。

（2）可以将参数的**量级进⾏统⼀**（如有的权重过大，需要进行归一化使得特征的权重相同）

（3）更深层的**⽹络很复杂**，**容易过拟合**。这意味着**正则化**变得更加重要。

（4）大致过程：在每次训练迭代中，我们⾸先**规范化输 ⼊**，即通过**减去其均值并除以其标准差**，其中两者均**基于当前⼩批量处理**。接下来，我们应⽤**⽐例系数和⽐ 例偏移**。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。**注**：（使用**批量⼤⼩为1**的⼩批量应⽤批量规范化，**将⽆法学到任何东西**，减去均 值之后，每个隐藏单元将为0）。只有使⽤**⾜够⼤的⼩批量**，批量规范化这种⽅法才是**有效且稳定**的。

<br>

**归一化过程：**

- ⽤x ∈ B表⽰⼀个来⾃⼩批量B的输⼊，批量规范化BN根据以下表达式转换x（，µˆB是⼩批量B的**样本均值**，σˆ B是⼩批量B的**样本标准差**）：

![image-20221110215412594](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221110215412594.png)

- 应⽤标准化后，⽣成的⼩批量的平均 值为0和单位⽅差为1。**拉伸参数 （scale）γ和偏移参数（shift）β**，它们的**形状与x相同**。请注意，γ和β是需要与其他模型参数⼀起**学习的参数**。
- 在训练过程中，**中间层的变化幅度不能过于剧烈**，⽽批量规范化将每⼀层**主动居中**，并将它们重新调整 为给定的**平均值和方差⼤⼩**（通过µˆB和σˆ B）。

![image-20221110220032096](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221110220032096.png)

注：**⽅差估计值**中**添加⼀个⼩的常量**ϵ > 0，以确保不会**除以零**

<br>

- 估计值µˆB和σˆ B通过使⽤平均值和⽅差的**噪声**（noise）估计来**抵消缩放问 题。**你可能会认为这种噪声是⼀个问题，⽽事实上它是有益的。
- **优化中的各种噪声源**通常 会导致**更快的训练和较少的过拟合**：这种变化**似乎是正则化的⼀种形式**。
- 批量规范化 最适应**50 ∼ 100范围**中的**中等批量⼤⼩**的难题
- 批量规范化在**训练模式**中，我们**⽆法得知**使⽤整个数据集来估计平均值和⽅差，所以只能**根据**每个⼩批 次的**平均值和⽅差不断训练**模型。⽽在**预测模式**下，可以根据**整个数据集精确计算批量规范化所需的平均值 和⽅差**。

<br>

#### 批量归一化层

##### 1.对于全连接层

- 将批量规范化层置于全连接层中的**仿射变换和激活函数之间**。设全连接层的输⼊为x，权重参数 和偏置参数分别为W和b，激活函数为ϕ，批量规范化的运算符为BN。（**特征维度**）

![image-20221110221907967](C:\Users\china\AppData\Roaming\Typora\typora-user-images\image-20221110221907967.png)

- ##### 作用在输入上

  ##### <br>

##### 2.对于卷积层

- ##### 作用在输入上

- 在**卷积层之后和⾮线性激活函数之前**应⽤批量规范化
- 当卷积有**多个输出通道**时，我们需要对这些通道的“**每个”输出执⾏批量规范化**，每个通道都有⾃⼰的**拉伸（scale）和偏移**（shift） 参数，这两个参数都是标量。设小批量m个，卷积的输出具有⾼度p和 宽度q。那么对于卷积层，我们在**每个输出通道的m · p · q个元素**上**同时执⾏每个批量规范化**

##### 对作用在卷积层上通道维度的理解：

卷积层输入为（批量、通道数、高、宽），作用在通道相当于把批量和通道合并作为”样本数“（“拉成维度矩阵”），作为“特征维度”，进而对每个样本进行归一化。

<br>

##### 3.预测过程中

**移动平均**估算**整个训练数据集的样本均值和⽅差**，并在预测时使⽤它们得到确定的输出。

